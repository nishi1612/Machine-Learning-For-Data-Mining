{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML4DM_Lab_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishi1612/Machine-Learning-For-Data-Mining/blob/master/ML4DM_Lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVx4M-h9hqnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThExCsAihtUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igXAcITIiTzG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "53b2da59-0929-4fc8-85b4-751c8e70e08a"
      },
      "source": [
        "id = \"18Ml33QE8vQiOPhUCbqxGRpeSlzcOt7FA\"\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('auto-mpg.csv')  \n",
        "df = pd.read_csv('auto-mpg.csv')\n",
        "df.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>origin</th>\n",
              "      <th>car name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>chevrolet chevelle malibu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>buick skylark 320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>plymouth satellite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>amc rebel sst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>ford torino</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  ... model year  origin                   car name\n",
              "0  18.0          8         307.0  ...         70       1  chevrolet chevelle malibu\n",
              "1  15.0          8         350.0  ...         70       1          buick skylark 320\n",
              "2  18.0          8         318.0  ...         70       1         plymouth satellite\n",
              "3  16.0          8         304.0  ...         70       1              amc rebel sst\n",
              "4  17.0          8         302.0  ...         70       1                ford torino\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IRXmhXNjV83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1dbea428-b59b-47a7-d8b0-4b7a670444a5"
      },
      "source": [
        "df.index"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RangeIndex(start=0, stop=398, step=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02nmGGKbjX8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(columns = df.columns[len(df.columns)-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUW23DNJjpKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8fb149d6-c125-4e5b-d91f-21626075951c"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  ... acceleration  model year  origin\n",
              "0  18.0          8         307.0  ...         12.0          70       1\n",
              "1  15.0          8         350.0  ...         11.5          70       1\n",
              "2  18.0          8         318.0  ...         11.0          70       1\n",
              "3  16.0          8         304.0  ...         12.0          70       1\n",
              "4  17.0          8         302.0  ...         10.5          70       1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr3I9R4xjvUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwAssswZkghz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2ae3b94b-af4b-47a8-bace-c36103ff28e9"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  ... acceleration  model year  origin\n",
              "0  18.0          8         307.0  ...         12.0          70       1\n",
              "1  15.0          8         350.0  ...         11.5          70       1\n",
              "2  18.0          8         318.0  ...         11.0          70       1\n",
              "3  16.0          8         304.0  ...         12.0          70       1\n",
              "4  17.0          8         302.0  ...         10.5          70       1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJA9RFF3j7Sz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6b71c8b4-ba11-42a8-d3b3-721cb416ae0d"
      },
      "source": [
        "for cols in df.columns:\n",
        "  df = df[~df[cols].isin(['?'])]\n",
        "df.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  ... acceleration  model year  origin\n",
              "0  18.0          8         307.0  ...         12.0          70       1\n",
              "1  15.0          8         350.0  ...         11.5          70       1\n",
              "2  18.0          8         318.0  ...         11.0          70       1\n",
              "3  16.0          8         304.0  ...         12.0          70       1\n",
              "4  17.0          8         302.0  ...         10.5          70       1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg9CShG5lf3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "49587f24-ea30-43af-fc1e-fdf65c2b7e8f"
      },
      "source": [
        "for col in df.columns:\n",
        "  vals = df[col].values\n",
        "  print(type(vals[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.float64'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.float64'>\n",
            "<class 'str'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.float64'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.int64'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWr61AWsn9Rk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa30379d-def5-4634-98b8-cee42d5177d2"
      },
      "source": [
        "df[df.columns[len(df.columns)-1]].unique()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvLqRAeLlw95",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "3a5b4fd1-5f46-4252-8c85-1a84df69d5bf"
      },
      "source": [
        "one_hot = pd.get_dummies(df[df.columns[len(df.columns)-1]])\n",
        "df = df.drop(df.columns[len(df.columns)-1],axis = 1)\n",
        "df = df.join(one_hot)\n",
        "df.head(5)  "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement horsepower  ...  model year  1  2  3\n",
              "0  18.0          8         307.0        130  ...          70  1  0  0\n",
              "1  15.0          8         350.0        165  ...          70  1  0  0\n",
              "2  18.0          8         318.0        150  ...          70  1  0  0\n",
              "3  16.0          8         304.0        150  ...          70  1  0  0\n",
              "4  17.0          8         302.0        140  ...          70  1  0  0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Q6WGdBrTQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "07841f8e-e306-41d1-cbe3-6af6e28c7ca0"
      },
      "source": [
        "for col in df.columns:\n",
        "  vals = df[col].values\n",
        "  print(type(vals[0]))\n",
        "x = df[df.columns[3]]\n",
        "x = np.asfarray(x,float)\n",
        "df[df.columns[3]] = x"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.float64'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.float64'>\n",
            "<class 'str'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.float64'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.uint8'>\n",
            "<class 'numpy.uint8'>\n",
            "<class 'numpy.uint8'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69rIKe50tYy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df.drop(columns=[df.columns[0],df.columns[len(df.columns)-1],df.columns[len(df.columns)-2],df.columns[len(df.columns)-3]])\n",
        "normalized_df = (df2-df2.mean())/df2.std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRX-jtO2tb7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8ddb94b-abdc-4a63-9bd0-304ccb0504dc"
      },
      "source": [
        "for cols in normalized_df.columns:\n",
        "  df[cols] = normalized_df[cols]\n",
        "df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.075915</td>\n",
              "      <td>0.663285</td>\n",
              "      <td>0.619748</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.572585</td>\n",
              "      <td>0.842258</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.539692</td>\n",
              "      <td>-1.646086</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.536160</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.028134</td>\n",
              "      <td>0.923085</td>\n",
              "      <td>0.554997</td>\n",
              "      <td>-1.827320</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.241772</td>\n",
              "      <td>2.429924</td>\n",
              "      <td>1.605147</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.480677</td>\n",
              "      <td>3.001484</td>\n",
              "      <td>1.620452</td>\n",
              "      <td>-2.371022</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.346890</td>\n",
              "      <td>2.871584</td>\n",
              "      <td>1.571005</td>\n",
              "      <td>-2.552256</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.490234</td>\n",
              "      <td>3.131384</td>\n",
              "      <td>1.704040</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.869080</td>\n",
              "      <td>2.222085</td>\n",
              "      <td>1.027093</td>\n",
              "      <td>-2.552256</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.802186</td>\n",
              "      <td>1.702485</td>\n",
              "      <td>0.689209</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.391269</td>\n",
              "      <td>1.442685</td>\n",
              "      <td>0.743365</td>\n",
              "      <td>-2.733490</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.964642</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.922314</td>\n",
              "      <td>-2.189788</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.490234</td>\n",
              "      <td>3.131384</td>\n",
              "      <td>0.127638</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.777990</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.712953</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.034288</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.170219</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>-0.194055</td>\n",
              "      <td>-0.239679</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>-0.459834</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-1.519034</td>\n",
              "      <td>-1.345162</td>\n",
              "      <td>1.797361</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.806659</td>\n",
              "      <td>-0.453855</td>\n",
              "      <td>-0.359764</td>\n",
              "      <td>0.709956</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.835327</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.644670</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.863996</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.709421</td>\n",
              "      <td>0.709956</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.701540</td>\n",
              "      <td>0.221625</td>\n",
              "      <td>-0.875420</td>\n",
              "      <td>-1.102384</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.388019</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.582394</td>\n",
              "      <td>2.871584</td>\n",
              "      <td>1.927726</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.075915</td>\n",
              "      <td>2.481884</td>\n",
              "      <td>1.646352</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>11.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>2.741684</td>\n",
              "      <td>1.653416</td>\n",
              "      <td>-0.739916</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>9.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>2.300025</td>\n",
              "      <td>2.065470</td>\n",
              "      <td>1.072424</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.438643</td>\n",
              "      <td>1.471139</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.397437</td>\n",
              "      <td>1.108671</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>34.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.685875</td>\n",
              "      <td>0.891190</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>-0.473962</td>\n",
              "      <td>0.238748</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>29.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.532826</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.285594</td>\n",
              "      <td>0.891190</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.132545</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.854440</td>\n",
              "      <td>-0.791594</td>\n",
              "      <td>-1.174454</td>\n",
              "      <td>-0.087473</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>37.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.121476</td>\n",
              "      <td>0.963684</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.186227</td>\n",
              "      <td>0.746203</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.854440</td>\n",
              "      <td>-1.077374</td>\n",
              "      <td>-1.003746</td>\n",
              "      <td>-0.304954</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.921333</td>\n",
              "      <td>-0.895514</td>\n",
              "      <td>-1.003746</td>\n",
              "      <td>0.637463</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.962540</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.835327</td>\n",
              "      <td>-0.765614</td>\n",
              "      <td>-0.909562</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>34.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.825771</td>\n",
              "      <td>-0.895514</td>\n",
              "      <td>-0.862470</td>\n",
              "      <td>0.492476</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.192113</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.192113</td>\n",
              "      <td>0.057514</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.156794</td>\n",
              "      <td>0.238748</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>25.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.128168</td>\n",
              "      <td>0.143685</td>\n",
              "      <td>-0.038361</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>38.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.645885</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>0.044050</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.367073</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.462189</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.359199</td>\n",
              "      <td>0.195645</td>\n",
              "      <td>-0.167864</td>\n",
              "      <td>-0.304954</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.481748</td>\n",
              "      <td>-0.220035</td>\n",
              "      <td>-0.368005</td>\n",
              "      <td>-0.594928</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.715308</td>\n",
              "      <td>-0.921150</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.032475</td>\n",
              "      <td>0.637463</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.479835</td>\n",
              "      <td>-0.220842</td>\n",
              "      <td>0.021267</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>44.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-1.363154</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>3.283479</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.803605</td>\n",
              "      <td>-1.428605</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.661694</td>\n",
              "      <td>-0.415097</td>\n",
              "      <td>1.108671</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.720653</td>\n",
              "      <td>-0.583754</td>\n",
              "      <td>-0.303253</td>\n",
              "      <td>1.398646</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>392 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      mpg  cylinders  displacement  horsepower  ...  model year  1  2  3\n",
              "0    18.0   1.482053      1.075915    0.663285  ...   -1.623241  1  0  0\n",
              "1    15.0   1.482053      1.486832    1.572585  ...   -1.623241  1  0  0\n",
              "2    18.0   1.482053      1.181033    1.182885  ...   -1.623241  1  0  0\n",
              "3    16.0   1.482053      1.047246    1.182885  ...   -1.623241  1  0  0\n",
              "4    17.0   1.482053      1.028134    0.923085  ...   -1.623241  1  0  0\n",
              "5    15.0   1.482053      2.241772    2.429924  ...   -1.623241  1  0  0\n",
              "6    14.0   1.482053      2.480677    3.001484  ...   -1.623241  1  0  0\n",
              "7    14.0   1.482053      2.346890    2.871584  ...   -1.623241  1  0  0\n",
              "8    14.0   1.482053      2.490234    3.131384  ...   -1.623241  1  0  0\n",
              "9    15.0   1.482053      1.869080    2.222085  ...   -1.623241  1  0  0\n",
              "10   15.0   1.482053      1.802186    1.702485  ...   -1.623241  1  0  0\n",
              "11   14.0   1.482053      1.391269    1.442685  ...   -1.623241  1  0  0\n",
              "12   15.0   1.482053      1.964642    1.182885  ...   -1.623241  1  0  0\n",
              "13   14.0   1.482053      2.490234    3.131384  ...   -1.623241  1  0  0\n",
              "14   24.0  -0.862911     -0.777990   -0.246015  ...   -1.623241  0  0  1\n",
              "15   22.0   0.309571      0.034288   -0.246015  ...   -1.623241  1  0  0\n",
              "16   18.0   0.309571      0.043844   -0.194055  ...   -1.623241  1  0  0\n",
              "17   21.0   0.309571      0.053400   -0.505815  ...   -1.623241  1  0  0\n",
              "18   27.0  -0.862911     -0.930889   -0.427875  ...   -1.623241  0  0  1\n",
              "19   26.0  -0.862911     -0.930889   -1.519034  ...   -1.623241  0  1  0\n",
              "20   25.0  -0.862911     -0.806659   -0.453855  ...   -1.623241  0  1  0\n",
              "21   24.0  -0.862911     -0.835327   -0.375915  ...   -1.623241  0  1  0\n",
              "22   25.0  -0.862911     -0.863996   -0.246015  ...   -1.623241  0  1  0\n",
              "23   26.0  -0.862911     -0.701540    0.221625  ...   -1.623241  0  1  0\n",
              "24   21.0   0.309571      0.043844   -0.375915  ...   -1.623241  1  0  0\n",
              "25   10.0   1.482053      1.582394    2.871584  ...   -1.623241  1  0  0\n",
              "26   10.0   1.482053      1.075915    2.481884  ...   -1.623241  1  0  0\n",
              "27   11.0   1.482053      1.181033    2.741684  ...   -1.623241  1  0  0\n",
              "28    9.0   1.482053      1.047246    2.300025  ...   -1.623241  1  0  0\n",
              "29   27.0  -0.862911     -0.930889   -0.427875  ...   -1.351777  0  0  1\n",
              "..    ...        ...           ...         ...  ...         ... .. .. ..\n",
              "367  28.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "368  27.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "369  34.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "370  31.0  -0.862911     -0.787546   -0.505815  ...    1.634321  1  0  0\n",
              "371  29.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "372  27.0  -0.862911     -0.414854   -0.375915  ...    1.634321  1  0  0\n",
              "373  24.0  -0.862911     -0.519972   -0.323955  ...    1.634321  1  0  0\n",
              "375  36.0  -0.862911     -0.854440   -0.791594  ...    1.634321  0  1  0\n",
              "376  37.0  -0.862911     -0.988227   -0.947474  ...    1.634321  0  0  1\n",
              "377  31.0  -0.862911     -0.988227   -0.947474  ...    1.634321  0  0  1\n",
              "378  38.0  -0.862911     -0.854440   -1.077374  ...    1.634321  1  0  0\n",
              "379  36.0  -0.862911     -0.921333   -0.895514  ...    1.634321  1  0  0\n",
              "380  36.0  -0.862911     -0.711097   -0.427875  ...    1.634321  0  0  1\n",
              "381  36.0  -0.862911     -0.835327   -0.765614  ...    1.634321  0  0  1\n",
              "382  34.0  -0.862911     -0.825771   -0.895514  ...    1.634321  0  0  1\n",
              "383  38.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "384  32.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "385  38.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "386  25.0   0.309571     -0.128168    0.143685  ...    1.634321  1  0  0\n",
              "387  38.0   0.309571      0.645885   -0.505815  ...    1.634321  1  0  0\n",
              "388  26.0  -0.862911     -0.367073   -0.323955  ...    1.634321  1  0  0\n",
              "389  22.0   0.309571      0.359199    0.195645  ...    1.634321  1  0  0\n",
              "390  32.0  -0.862911     -0.481748   -0.220035  ...    1.634321  0  0  1\n",
              "391  36.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "392  27.0  -0.862911     -0.414854   -0.375915  ...    1.634321  1  0  0\n",
              "393  27.0  -0.862911     -0.519972   -0.479835  ...    1.634321  1  0  0\n",
              "394  44.0  -0.862911     -0.930889   -1.363154  ...    1.634321  0  1  0\n",
              "395  32.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "396  28.0  -0.862911     -0.711097   -0.661694  ...    1.634321  1  0  0\n",
              "397  31.0  -0.862911     -0.720653   -0.583754  ...    1.634321  1  0  0\n",
              "\n",
              "[392 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9HrCzDRpTRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msk = np.random.rand(len(df)) < 0.8\n",
        "train_df = df[msk]\n",
        "test_df = df[~msk]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vL38AoGq921",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc97c511-cfa9-41e7-9d4d-22fe6ec3aa3d"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.075915</td>\n",
              "      <td>0.663285</td>\n",
              "      <td>0.619748</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.572585</td>\n",
              "      <td>0.842258</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.539692</td>\n",
              "      <td>-1.646086</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.536160</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.028134</td>\n",
              "      <td>0.923085</td>\n",
              "      <td>0.554997</td>\n",
              "      <td>-1.827320</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.241772</td>\n",
              "      <td>2.429924</td>\n",
              "      <td>1.605147</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.480677</td>\n",
              "      <td>3.001484</td>\n",
              "      <td>1.620452</td>\n",
              "      <td>-2.371022</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.346890</td>\n",
              "      <td>2.871584</td>\n",
              "      <td>1.571005</td>\n",
              "      <td>-2.552256</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.490234</td>\n",
              "      <td>3.131384</td>\n",
              "      <td>1.704040</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.802186</td>\n",
              "      <td>1.702485</td>\n",
              "      <td>0.689209</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.391269</td>\n",
              "      <td>1.442685</td>\n",
              "      <td>0.743365</td>\n",
              "      <td>-2.733490</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.964642</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.922314</td>\n",
              "      <td>-2.189788</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.490234</td>\n",
              "      <td>3.131384</td>\n",
              "      <td>0.127638</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.777990</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.712953</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.034288</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.170219</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>-0.194055</td>\n",
              "      <td>-0.239679</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>-0.459834</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.806659</td>\n",
              "      <td>-0.453855</td>\n",
              "      <td>-0.359764</td>\n",
              "      <td>0.709956</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.835327</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.644670</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.863996</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.709421</td>\n",
              "      <td>0.709956</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.701540</td>\n",
              "      <td>0.221625</td>\n",
              "      <td>-0.875420</td>\n",
              "      <td>-1.102384</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.388019</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.582394</td>\n",
              "      <td>2.871584</td>\n",
              "      <td>1.927726</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.075915</td>\n",
              "      <td>2.481884</td>\n",
              "      <td>1.646352</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>11.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>2.741684</td>\n",
              "      <td>1.653416</td>\n",
              "      <td>-0.739916</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.840101</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.777990</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.882484</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>19.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.359199</td>\n",
              "      <td>-0.116115</td>\n",
              "      <td>-0.404501</td>\n",
              "      <td>-0.921150</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>16.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.292305</td>\n",
              "      <td>0.013785</td>\n",
              "      <td>0.543224</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>25.4</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.252399</td>\n",
              "      <td>0.299565</td>\n",
              "      <td>-0.091340</td>\n",
              "      <td>-1.066137</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>24.2</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.462635</td>\n",
              "      <td>0.403485</td>\n",
              "      <td>-0.056021</td>\n",
              "      <td>-0.631175</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>22.4</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.349643</td>\n",
              "      <td>0.143685</td>\n",
              "      <td>0.514969</td>\n",
              "      <td>0.093761</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>26.6</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>0.013785</td>\n",
              "      <td>0.879931</td>\n",
              "      <td>1.253659</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>20.2</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>0.097028</td>\n",
              "      <td>0.564969</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>17.6</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.292305</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>0.573834</td>\n",
              "      <td>0.383735</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.397437</td>\n",
              "      <td>1.108671</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>34.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.685875</td>\n",
              "      <td>0.891190</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>-0.473962</td>\n",
              "      <td>0.238748</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>29.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.532826</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.285594</td>\n",
              "      <td>0.891190</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.132545</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.854440</td>\n",
              "      <td>-0.791594</td>\n",
              "      <td>-1.174454</td>\n",
              "      <td>-0.087473</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>37.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.121476</td>\n",
              "      <td>0.963684</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.186227</td>\n",
              "      <td>0.746203</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.921333</td>\n",
              "      <td>-0.895514</td>\n",
              "      <td>-1.003746</td>\n",
              "      <td>0.637463</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.835327</td>\n",
              "      <td>-0.765614</td>\n",
              "      <td>-0.909562</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.192113</td>\n",
              "      <td>0.057514</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>25.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.128168</td>\n",
              "      <td>0.143685</td>\n",
              "      <td>-0.038361</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>38.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.645885</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>0.044050</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.367073</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.462189</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.359199</td>\n",
              "      <td>0.195645</td>\n",
              "      <td>-0.167864</td>\n",
              "      <td>-0.304954</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.481748</td>\n",
              "      <td>-0.220035</td>\n",
              "      <td>-0.368005</td>\n",
              "      <td>-0.594928</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.715308</td>\n",
              "      <td>-0.921150</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.032475</td>\n",
              "      <td>0.637463</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.479835</td>\n",
              "      <td>-0.220842</td>\n",
              "      <td>0.021267</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>44.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-1.363154</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>3.283479</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.803605</td>\n",
              "      <td>-1.428605</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.661694</td>\n",
              "      <td>-0.415097</td>\n",
              "      <td>1.108671</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.720653</td>\n",
              "      <td>-0.583754</td>\n",
              "      <td>-0.303253</td>\n",
              "      <td>1.398646</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>310 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      mpg  cylinders  displacement  horsepower  ...  model year  1  2  3\n",
              "0    18.0   1.482053      1.075915    0.663285  ...   -1.623241  1  0  0\n",
              "1    15.0   1.482053      1.486832    1.572585  ...   -1.623241  1  0  0\n",
              "2    18.0   1.482053      1.181033    1.182885  ...   -1.623241  1  0  0\n",
              "3    16.0   1.482053      1.047246    1.182885  ...   -1.623241  1  0  0\n",
              "4    17.0   1.482053      1.028134    0.923085  ...   -1.623241  1  0  0\n",
              "5    15.0   1.482053      2.241772    2.429924  ...   -1.623241  1  0  0\n",
              "6    14.0   1.482053      2.480677    3.001484  ...   -1.623241  1  0  0\n",
              "7    14.0   1.482053      2.346890    2.871584  ...   -1.623241  1  0  0\n",
              "8    14.0   1.482053      2.490234    3.131384  ...   -1.623241  1  0  0\n",
              "10   15.0   1.482053      1.802186    1.702485  ...   -1.623241  1  0  0\n",
              "11   14.0   1.482053      1.391269    1.442685  ...   -1.623241  1  0  0\n",
              "12   15.0   1.482053      1.964642    1.182885  ...   -1.623241  1  0  0\n",
              "13   14.0   1.482053      2.490234    3.131384  ...   -1.623241  1  0  0\n",
              "14   24.0  -0.862911     -0.777990   -0.246015  ...   -1.623241  0  0  1\n",
              "15   22.0   0.309571      0.034288   -0.246015  ...   -1.623241  1  0  0\n",
              "16   18.0   0.309571      0.043844   -0.194055  ...   -1.623241  1  0  0\n",
              "17   21.0   0.309571      0.053400   -0.505815  ...   -1.623241  1  0  0\n",
              "18   27.0  -0.862911     -0.930889   -0.427875  ...   -1.623241  0  0  1\n",
              "20   25.0  -0.862911     -0.806659   -0.453855  ...   -1.623241  0  1  0\n",
              "21   24.0  -0.862911     -0.835327   -0.375915  ...   -1.623241  0  1  0\n",
              "22   25.0  -0.862911     -0.863996   -0.246015  ...   -1.623241  0  1  0\n",
              "23   26.0  -0.862911     -0.701540    0.221625  ...   -1.623241  0  1  0\n",
              "24   21.0   0.309571      0.043844   -0.375915  ...   -1.623241  1  0  0\n",
              "25   10.0   1.482053      1.582394    2.871584  ...   -1.623241  1  0  0\n",
              "26   10.0   1.482053      1.075915    2.481884  ...   -1.623241  1  0  0\n",
              "27   11.0   1.482053      1.181033    2.741684  ...   -1.623241  1  0  0\n",
              "30   28.0  -0.862911     -0.519972   -0.375915  ...   -1.351777  1  0  0\n",
              "31   25.0  -0.862911     -0.777990   -0.246015  ...   -1.351777  0  0  1\n",
              "33   19.0   0.309571      0.359199   -0.116115  ...   -1.351777  1  0  0\n",
              "34   16.0   0.309571      0.292305    0.013785  ...   -1.351777  1  0  0\n",
              "..    ...        ...           ...         ...  ...         ... .. .. ..\n",
              "361  25.4   0.309571     -0.252399    0.299565  ...    1.362858  0  0  1\n",
              "362  24.2   0.309571     -0.462635    0.403485  ...    1.362858  0  0  1\n",
              "363  22.4   0.309571      0.349643    0.143685  ...    1.362858  1  0  0\n",
              "364  26.6   1.482053      1.486832    0.013785  ...    1.362858  1  0  0\n",
              "365  20.2   0.309571      0.053400   -0.427875  ...    1.362858  1  0  0\n",
              "366  17.6   0.309571      0.292305   -0.505815  ...    1.362858  1  0  0\n",
              "368  27.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "369  34.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "370  31.0  -0.862911     -0.787546   -0.505815  ...    1.634321  1  0  0\n",
              "371  29.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "372  27.0  -0.862911     -0.414854   -0.375915  ...    1.634321  1  0  0\n",
              "373  24.0  -0.862911     -0.519972   -0.323955  ...    1.634321  1  0  0\n",
              "375  36.0  -0.862911     -0.854440   -0.791594  ...    1.634321  0  1  0\n",
              "376  37.0  -0.862911     -0.988227   -0.947474  ...    1.634321  0  0  1\n",
              "377  31.0  -0.862911     -0.988227   -0.947474  ...    1.634321  0  0  1\n",
              "379  36.0  -0.862911     -0.921333   -0.895514  ...    1.634321  1  0  0\n",
              "381  36.0  -0.862911     -0.835327   -0.765614  ...    1.634321  0  0  1\n",
              "384  32.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "386  25.0   0.309571     -0.128168    0.143685  ...    1.634321  1  0  0\n",
              "387  38.0   0.309571      0.645885   -0.505815  ...    1.634321  1  0  0\n",
              "388  26.0  -0.862911     -0.367073   -0.323955  ...    1.634321  1  0  0\n",
              "389  22.0   0.309571      0.359199    0.195645  ...    1.634321  1  0  0\n",
              "390  32.0  -0.862911     -0.481748   -0.220035  ...    1.634321  0  0  1\n",
              "391  36.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "392  27.0  -0.862911     -0.414854   -0.375915  ...    1.634321  1  0  0\n",
              "393  27.0  -0.862911     -0.519972   -0.479835  ...    1.634321  1  0  0\n",
              "394  44.0  -0.862911     -0.930889   -1.363154  ...    1.634321  0  1  0\n",
              "395  32.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "396  28.0  -0.862911     -0.711097   -0.661694  ...    1.634321  1  0  0\n",
              "397  31.0  -0.862911     -0.720653   -0.583754  ...    1.634321  1  0  0\n",
              "\n",
              "[310 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbmXLkJ4rKLB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89bdbcce-d879-4f01-91e7-11d99e56040c"
      },
      "source": [
        "test_df"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.869080</td>\n",
              "      <td>2.222085</td>\n",
              "      <td>1.027093</td>\n",
              "      <td>-2.552256</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-1.519034</td>\n",
              "      <td>-1.345162</td>\n",
              "      <td>1.797361</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>9.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>2.300025</td>\n",
              "      <td>2.065470</td>\n",
              "      <td>1.072424</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.964642</td>\n",
              "      <td>1.832385</td>\n",
              "      <td>1.749954</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>12.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.802186</td>\n",
              "      <td>1.962285</td>\n",
              "      <td>2.328008</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>19.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.531211</td>\n",
              "      <td>-0.116115</td>\n",
              "      <td>0.358388</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>30.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.016895</td>\n",
              "      <td>-0.739634</td>\n",
              "      <td>-1.074384</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.926111</td>\n",
              "      <td>-0.635714</td>\n",
              "      <td>-1.002568</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>21.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.691984</td>\n",
              "      <td>-0.479835</td>\n",
              "      <td>-0.884839</td>\n",
              "      <td>0.347488</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>13.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.572585</td>\n",
              "      <td>1.526268</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.496388</td>\n",
              "      <td>1.260825</td>\n",
              "      <td>1.355560</td>\n",
              "      <td>-0.921150</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>13.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.312785</td>\n",
              "      <td>1.794692</td>\n",
              "      <td>-0.739916</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>12.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.442685</td>\n",
              "      <td>1.740536</td>\n",
              "      <td>-0.739916</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.811846</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-1.033178</td>\n",
              "      <td>0.347488</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.817534</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.941151</td>\n",
              "      <td>-1.102384</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.359199</td>\n",
              "      <td>-0.116115</td>\n",
              "      <td>-0.038361</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.531211</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>0.051113</td>\n",
              "      <td>0.347488</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>20.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.822442</td>\n",
              "      <td>1.253659</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>16.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.964642</td>\n",
              "      <td>3.261284</td>\n",
              "      <td>1.530977</td>\n",
              "      <td>-2.189788</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.749321</td>\n",
              "      <td>-0.765614</td>\n",
              "      <td>-0.964895</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>16.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.531211</td>\n",
              "      <td>-0.116115</td>\n",
              "      <td>0.945860</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.292305</td>\n",
              "      <td>0.013785</td>\n",
              "      <td>0.748074</td>\n",
              "      <td>0.347488</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>16.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.028134</td>\n",
              "      <td>0.923085</td>\n",
              "      <td>1.369687</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.102901</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.194468</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.687674</td>\n",
              "      <td>-0.797719</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.194055</td>\n",
              "      <td>-0.575209</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.825771</td>\n",
              "      <td>-0.297975</td>\n",
              "      <td>-0.690584</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>25.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.303253</td>\n",
              "      <td>-0.051226</td>\n",
              "      <td>0.548467</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>20.3</td>\n",
              "      <td>-0.276670</td>\n",
              "      <td>-0.605978</td>\n",
              "      <td>-0.038175</td>\n",
              "      <td>-0.173751</td>\n",
              "      <td>0.130008</td>\n",
              "      <td>0.548467</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>31.5</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.007339</td>\n",
              "      <td>-0.869534</td>\n",
              "      <td>-1.162681</td>\n",
              "      <td>-0.232460</td>\n",
              "      <td>0.548467</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>17.6</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.028134</td>\n",
              "      <td>0.637305</td>\n",
              "      <td>0.879931</td>\n",
              "      <td>-0.776162</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>19.2</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>0.693666</td>\n",
              "      <td>0.533385</td>\n",
              "      <td>0.738655</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>31.9</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.007339</td>\n",
              "      <td>-0.869534</td>\n",
              "      <td>-1.239205</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>34.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.036008</td>\n",
              "      <td>-1.025414</td>\n",
              "      <td>-1.180340</td>\n",
              "      <td>-0.123720</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>35.7</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.921333</td>\n",
              "      <td>-0.635714</td>\n",
              "      <td>-1.250978</td>\n",
              "      <td>-0.413694</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>27.4</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.701540</td>\n",
              "      <td>-0.635714</td>\n",
              "      <td>-0.362118</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>23.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>0.533385</td>\n",
              "      <td>1.085958</td>\n",
              "      <td>0.673710</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>23.9</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>0.626773</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>0.520855</td>\n",
              "      <td>2.413556</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>28.4</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.362118</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>28.8</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.204617</td>\n",
              "      <td>0.273585</td>\n",
              "      <td>-0.450416</td>\n",
              "      <td>-1.537345</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>37.2</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.036008</td>\n",
              "      <td>-1.025414</td>\n",
              "      <td>-1.128539</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>34.3</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.687674</td>\n",
              "      <td>-0.929576</td>\n",
              "      <td>0.093761</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>40.8</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.045564</td>\n",
              "      <td>-1.025414</td>\n",
              "      <td>-1.021405</td>\n",
              "      <td>1.326152</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>36.4</td>\n",
              "      <td>-0.276670</td>\n",
              "      <td>-0.701540</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-0.032475</td>\n",
              "      <td>1.579880</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>30.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.462635</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>0.320715</td>\n",
              "      <td>2.268569</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329</th>\n",
              "      <td>44.6</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.327503</td>\n",
              "      <td>-0.631175</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>29.8</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.007339</td>\n",
              "      <td>-1.103354</td>\n",
              "      <td>-1.333389</td>\n",
              "      <td>-0.087473</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>23.5</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.204617</td>\n",
              "      <td>0.143685</td>\n",
              "      <td>-0.297367</td>\n",
              "      <td>-1.066137</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>39.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.102901</td>\n",
              "      <td>-1.207274</td>\n",
              "      <td>-1.439346</td>\n",
              "      <td>0.492476</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>34.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.168567</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>28.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.510416</td>\n",
              "      <td>-0.635714</td>\n",
              "      <td>0.297169</td>\n",
              "      <td>1.761114</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.438643</td>\n",
              "      <td>1.471139</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.854440</td>\n",
              "      <td>-1.077374</td>\n",
              "      <td>-1.003746</td>\n",
              "      <td>-0.304954</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.962540</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>34.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.825771</td>\n",
              "      <td>-0.895514</td>\n",
              "      <td>-0.862470</td>\n",
              "      <td>0.492476</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.192113</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.156794</td>\n",
              "      <td>0.238748</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      mpg  cylinders  displacement  horsepower  ...  model year  1  2  3\n",
              "9    15.0   1.482053      1.869080    2.222085  ...   -1.623241  1  0  0\n",
              "19   26.0  -0.862911     -0.930889   -1.519034  ...   -1.623241  0  1  0\n",
              "28    9.0   1.482053      1.047246    2.300025  ...   -1.623241  1  0  0\n",
              "29   27.0  -0.862911     -0.930889   -0.427875  ...   -1.351777  0  0  1\n",
              "39   14.0   1.482053      1.964642    1.832385  ...   -1.351777  1  0  0\n",
              "42   12.0   1.482053      1.802186    1.962285  ...   -1.351777  1  0  0\n",
              "47   19.0   0.309571      0.531211   -0.116115  ...   -1.351777  1  0  0\n",
              "52   30.0  -0.862911     -1.016895   -0.739634  ...   -1.351777  0  1  0\n",
              "58   25.0  -0.862911     -0.926111   -0.635714  ...   -1.080314  1  0  0\n",
              "61   21.0  -0.862911     -0.691984   -0.479835  ...   -1.080314  1  0  0\n",
              "62   13.0   1.482053      1.486832    1.572585  ...   -1.080314  1  0  0\n",
              "65   14.0   1.482053      1.496388    1.260825  ...   -1.080314  1  0  0\n",
              "68   13.0   1.482053      1.486832    1.312785  ...   -1.080314  1  0  0\n",
              "69   12.0   1.482053      1.486832    1.442685  ...   -1.080314  1  0  0\n",
              "81   28.0  -0.862911     -0.930889   -0.323955  ...   -1.080314  0  0  1\n",
              "84   27.0  -0.862911     -0.930889   -0.427875  ...   -1.080314  0  0  1\n",
              "86   14.0   1.482053      1.047246    1.182885  ...   -0.808850  1  0  0\n",
              "89   15.0   1.482053      1.181033    1.182885  ...   -0.808850  1  0  0\n",
              "99   18.0   0.309571      0.359199   -0.116115  ...   -0.808850  1  0  0\n",
              "100  18.0   0.309571      0.531211   -0.427875  ...   -0.808850  1  0  0\n",
              "108  20.0  -0.862911     -0.930889   -0.427875  ...   -0.808850  0  0  1\n",
              "116  16.0   1.482053      1.964642    3.261284  ...   -0.808850  1  0  0\n",
              "118  24.0  -0.862911     -0.749321   -0.765614  ...   -0.808850  0  1  0\n",
              "133  16.0   0.309571      0.531211   -0.116115  ...   -0.537387  1  0  0\n",
              "135  18.0   0.309571      0.292305    0.013785  ...   -0.537387  1  0  0\n",
              "136  16.0   1.482053      1.028134    0.923085  ...   -0.537387  1  0  0\n",
              "142  26.0  -0.862911     -1.102901   -0.973454  ...   -0.537387  0  1  0\n",
              "143  26.0  -0.862911     -0.930889   -0.687674  ...   -0.537387  0  1  0\n",
              "149  24.0  -0.862911     -0.711097   -0.194055  ...   -0.537387  0  0  1\n",
              "150  26.0  -0.862911     -0.825771   -0.297975  ...   -0.537387  0  0  1\n",
              "..    ...        ...           ...         ...  ...         ... .. .. ..\n",
              "255  25.1  -0.862911     -0.519972   -0.427875  ...    0.548467  1  0  0\n",
              "274  20.3  -0.276670     -0.605978   -0.038175  ...    0.548467  0  1  0\n",
              "278  31.5  -0.862911     -1.007339   -0.869534  ...    0.548467  0  1  0\n",
              "286  17.6   1.482053      1.028134    0.637305  ...    0.819931  1  0  0\n",
              "291  19.2   1.482053      0.693666    0.533385  ...    0.819931  1  0  0\n",
              "293  31.9  -0.862911     -1.007339   -0.869534  ...    0.819931  0  1  0\n",
              "294  34.1  -0.862911     -1.036008   -1.025414  ...    0.819931  0  0  1\n",
              "295  35.7  -0.862911     -0.921333   -0.635714  ...    0.819931  1  0  0\n",
              "296  27.4  -0.862911     -0.701540   -0.635714  ...    0.819931  1  0  0\n",
              "298  23.0   1.482053      1.486832    0.533385  ...    0.819931  1  0  0\n",
              "300  23.9   1.482053      0.626773   -0.375915  ...    0.819931  1  0  0\n",
              "305  28.4  -0.862911     -0.414854   -0.375915  ...    0.819931  1  0  0\n",
              "306  28.8   0.309571     -0.204617    0.273585  ...    0.819931  1  0  0\n",
              "312  37.2  -0.862911     -1.036008   -1.025414  ...    1.091394  0  0  1\n",
              "317  34.3  -0.862911     -0.930889   -0.687674  ...    1.091394  0  1  0\n",
              "324  40.8  -0.862911     -1.045564   -1.025414  ...    1.091394  0  0  1\n",
              "327  36.4  -0.276670     -0.701540   -0.973454  ...    1.091394  0  1  0\n",
              "328  30.0  -0.862911     -0.462635   -0.973454  ...    1.091394  0  1  0\n",
              "329  44.6  -0.862911     -0.988227   -0.973454  ...    1.091394  0  0  1\n",
              "332  29.8  -0.862911     -1.007339   -1.103354  ...    1.091394  0  1  0\n",
              "341  23.5   0.309571     -0.204617    0.143685  ...    1.362858  1  0  0\n",
              "343  39.1  -0.862911     -1.102901   -1.207274  ...    1.362858  0  0  1\n",
              "349  34.1  -0.862911     -0.988227   -0.947474  ...    1.362858  0  0  1\n",
              "359  28.1  -0.862911     -0.510416   -0.635714  ...    1.362858  0  1  0\n",
              "367  28.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "378  38.0  -0.862911     -0.854440   -1.077374  ...    1.634321  1  0  0\n",
              "380  36.0  -0.862911     -0.711097   -0.427875  ...    1.634321  0  0  1\n",
              "382  34.0  -0.862911     -0.825771   -0.895514  ...    1.634321  0  0  1\n",
              "383  38.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "385  38.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "\n",
              "[82 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS6QEn75ukb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = train_df.drop(columns = [df.columns[0]])\n",
        "testX = test_df.drop(columns = [df.columns[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlJINel7u2fR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainY = train_df.iloc[:,0]\n",
        "testY = test_df.iloc[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydRupT53vR1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression().fit(trainX, trainY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z_qhAg0vdmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc111607-b548-4eef-99d8-364131e1c866"
      },
      "source": [
        "reg.score(trainX, trainY)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8179584603381965"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrON98zev0mJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testY_pred = reg.predict(testX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEr46tZQv4pm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "95fe8fc2-3c69-492f-e54f-123b25f2e3b3"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(testY, testY_pred)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.855720325691431"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoXiphUKx8TZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8711b389-0668-4917-b21b-ff1ae5b0c298"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oucWG4yyP8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1000\n",
        "train_validation_split = 0.2\n",
        "verbose = 1\n",
        "input_dims = len(trainX.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUl3cF0_ykls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67dec731-702c-4749-8c67-397801c5dce8"
      },
      "source": [
        "input_dims"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA9bPFE-yN-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "47af5ead-970c-48f9-9db8-6d7568c0054c"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "model = Sequential()\n",
        "model.add(Dense(20,input_dim = input_dims))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('relu'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfoOOsDs1juI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "645aa97c-a34a-4f40-a143-b536b183afe9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 20)                200       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 6         \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 581\n",
            "Trainable params: 581\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4NE3zyH1XTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b3e93e9-e764-4f95-ac59-3f0220982f8a"
      },
      "source": [
        "model.compile(optimizer='sgd',loss='mean_squared_error',metrics=['accuracy'])\n",
        "history = model.fit(trainX,trainY,validation_split=train_validation_split,epochs=epochs,verbose=verbose,shuffle=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 248 samples, validate on 62 samples\n",
            "Epoch 1/1000\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 321.6951 - acc: 0.0000e+00 - val_loss: 397.5189 - val_acc: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 83.5675 - acc: 0.0484 - val_loss: 210.5179 - val_acc: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 49.3548 - acc: 0.0565 - val_loss: 161.2073 - val_acc: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 45.1995 - acc: 0.0242 - val_loss: 147.0171 - val_acc: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 44.4253 - acc: 0.0323 - val_loss: 135.8560 - val_acc: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 41.9583 - acc: 0.0202 - val_loss: 131.9582 - val_acc: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 25.6915 - acc: 0.0685 - val_loss: 109.3224 - val_acc: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 19.5159 - acc: 0.1048 - val_loss: 88.4793 - val_acc: 0.0323\n",
            "Epoch 9/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 15.7824 - acc: 0.1411 - val_loss: 88.6202 - val_acc: 0.0161\n",
            "Epoch 10/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 14.9764 - acc: 0.0806 - val_loss: 67.8088 - val_acc: 0.0323\n",
            "Epoch 11/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 13.5624 - acc: 0.1290 - val_loss: 61.6581 - val_acc: 0.0323\n",
            "Epoch 12/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 12.0773 - acc: 0.1371 - val_loss: 56.9838 - val_acc: 0.0484\n",
            "Epoch 13/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 22.5783 - acc: 0.0524 - val_loss: 58.0367 - val_acc: 0.0806\n",
            "Epoch 14/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 15.6839 - acc: 0.0444 - val_loss: 60.0507 - val_acc: 0.0806\n",
            "Epoch 15/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 14.5643 - acc: 0.0766 - val_loss: 59.2910 - val_acc: 0.0806\n",
            "Epoch 16/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 14.0424 - acc: 0.0766 - val_loss: 59.1333 - val_acc: 0.0806\n",
            "Epoch 17/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 12.4055 - acc: 0.0968 - val_loss: 54.4272 - val_acc: 0.0806\n",
            "Epoch 18/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 12.3841 - acc: 0.0806 - val_loss: 55.3537 - val_acc: 0.0645\n",
            "Epoch 19/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 10.4285 - acc: 0.1331 - val_loss: 45.0004 - val_acc: 0.0161\n",
            "Epoch 20/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 11.0093 - acc: 0.0887 - val_loss: 47.1425 - val_acc: 0.0323\n",
            "Epoch 21/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 11.9716 - acc: 0.1008 - val_loss: 49.7890 - val_acc: 0.0323\n",
            "Epoch 22/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 9.0821 - acc: 0.1210 - val_loss: 56.5561 - val_acc: 0.0323\n",
            "Epoch 23/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 12.1921 - acc: 0.1169 - val_loss: 39.1140 - val_acc: 0.0161\n",
            "Epoch 24/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 7.8431 - acc: 0.1411 - val_loss: 39.6849 - val_acc: 0.0323\n",
            "Epoch 25/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 12.6622 - acc: 0.1492 - val_loss: 39.3648 - val_acc: 0.0323\n",
            "Epoch 26/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 8.3249 - acc: 0.1290 - val_loss: 56.1934 - val_acc: 0.0323\n",
            "Epoch 27/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 11.2240 - acc: 0.1250 - val_loss: 36.9500 - val_acc: 0.0645\n",
            "Epoch 28/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 7.8397 - acc: 0.1290 - val_loss: 42.8309 - val_acc: 0.0968\n",
            "Epoch 29/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 7.3109 - acc: 0.1210 - val_loss: 35.2369 - val_acc: 0.0323\n",
            "Epoch 30/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 8.0281 - acc: 0.1331 - val_loss: 32.4310 - val_acc: 0.0323\n",
            "Epoch 31/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 8.6111 - acc: 0.1008 - val_loss: 38.4111 - val_acc: 0.0161\n",
            "Epoch 32/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 12.1867 - acc: 0.1048 - val_loss: 33.0653 - val_acc: 0.0645\n",
            "Epoch 33/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 7.5490 - acc: 0.1492 - val_loss: 31.8736 - val_acc: 0.0484\n",
            "Epoch 34/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 12.3437 - acc: 0.0605 - val_loss: 32.3391 - val_acc: 0.0323\n",
            "Epoch 35/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 9.4139 - acc: 0.0847 - val_loss: 32.7774 - val_acc: 0.0161\n",
            "Epoch 36/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 6.3020 - acc: 0.1411 - val_loss: 31.8910 - val_acc: 0.0323\n",
            "Epoch 37/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 6.9815 - acc: 0.1169 - val_loss: 33.0320 - val_acc: 0.0484\n",
            "Epoch 38/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 6.8058 - acc: 0.1331 - val_loss: 30.9430 - val_acc: 0.0484\n",
            "Epoch 39/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 5.7872 - acc: 0.1250 - val_loss: 31.4475 - val_acc: 0.0484\n",
            "Epoch 40/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 7.0783 - acc: 0.0847 - val_loss: 29.1688 - val_acc: 0.0484\n",
            "Epoch 41/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 7.6607 - acc: 0.1331 - val_loss: 28.4550 - val_acc: 0.0484\n",
            "Epoch 42/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 8.0648 - acc: 0.1331 - val_loss: 29.3935 - val_acc: 0.0484\n",
            "Epoch 43/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 5.8589 - acc: 0.1411 - val_loss: 28.1351 - val_acc: 0.0484\n",
            "Epoch 44/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 6.4820 - acc: 0.1250 - val_loss: 28.2428 - val_acc: 0.0323\n",
            "Epoch 45/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.3001 - acc: 0.1048 - val_loss: 29.4598 - val_acc: 0.0323\n",
            "Epoch 46/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 5.7174 - acc: 0.1492 - val_loss: 27.9611 - val_acc: 0.0484\n",
            "Epoch 47/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 6.2182 - acc: 0.1250 - val_loss: 30.0292 - val_acc: 0.0323\n",
            "Epoch 48/1000\n",
            "248/248 [==============================] - 0s 79us/step - loss: 5.9988 - acc: 0.1290 - val_loss: 29.6550 - val_acc: 0.0323\n",
            "Epoch 49/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 7.6254 - acc: 0.1089 - val_loss: 28.7151 - val_acc: 0.0323\n",
            "Epoch 50/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 5.0790 - acc: 0.1411 - val_loss: 28.7121 - val_acc: 0.0323\n",
            "Epoch 51/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 6.8115 - acc: 0.1008 - val_loss: 27.6907 - val_acc: 0.0484\n",
            "Epoch 52/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.4917 - acc: 0.1210 - val_loss: 27.6779 - val_acc: 0.0323\n",
            "Epoch 53/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 7.3301 - acc: 0.0927 - val_loss: 28.7024 - val_acc: 0.0323\n",
            "Epoch 54/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 6.2962 - acc: 0.1048 - val_loss: 36.7634 - val_acc: 0.0645\n",
            "Epoch 55/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 8.7357 - acc: 0.1371 - val_loss: 31.1379 - val_acc: 0.0484\n",
            "Epoch 56/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.3463 - acc: 0.1129 - val_loss: 28.4251 - val_acc: 0.0484\n",
            "Epoch 57/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 6.4293 - acc: 0.1290 - val_loss: 27.8097 - val_acc: 0.0323\n",
            "Epoch 58/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 5.4919 - acc: 0.1331 - val_loss: 28.4020 - val_acc: 0.0323\n",
            "Epoch 59/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 7.8702 - acc: 0.0766 - val_loss: 29.0911 - val_acc: 0.0484\n",
            "Epoch 60/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 5.8913 - acc: 0.1048 - val_loss: 29.7630 - val_acc: 0.0323\n",
            "Epoch 61/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 5.9618 - acc: 0.1169 - val_loss: 29.1860 - val_acc: 0.0323\n",
            "Epoch 62/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 5.6714 - acc: 0.1371 - val_loss: 30.8841 - val_acc: 0.0323\n",
            "Epoch 63/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 4.4897 - acc: 0.1694 - val_loss: 29.2372 - val_acc: 0.0323\n",
            "Epoch 64/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.5668 - acc: 0.1169 - val_loss: 28.5669 - val_acc: 0.0484\n",
            "Epoch 65/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 4.5552 - acc: 0.1250 - val_loss: 29.2542 - val_acc: 0.0323\n",
            "Epoch 66/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 7.9118 - acc: 0.0968 - val_loss: 28.1572 - val_acc: 0.0323\n",
            "Epoch 67/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 4.5572 - acc: 0.1895 - val_loss: 29.1948 - val_acc: 0.0323\n",
            "Epoch 68/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 6.8089 - acc: 0.1411 - val_loss: 30.1683 - val_acc: 0.0323\n",
            "Epoch 69/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.0502 - acc: 0.1734 - val_loss: 28.6315 - val_acc: 0.0484\n",
            "Epoch 70/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 4.6092 - acc: 0.1573 - val_loss: 32.1940 - val_acc: 0.0323\n",
            "Epoch 71/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 4.2916 - acc: 0.1935 - val_loss: 29.1177 - val_acc: 0.0161\n",
            "Epoch 72/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 6.1610 - acc: 0.1734 - val_loss: 28.9219 - val_acc: 0.0323\n",
            "Epoch 73/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 5.9742 - acc: 0.1169 - val_loss: 29.0231 - val_acc: 0.0161\n",
            "Epoch 74/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 5.7324 - acc: 0.1250 - val_loss: 29.5926 - val_acc: 0.0161\n",
            "Epoch 75/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 6.7558 - acc: 0.1089 - val_loss: 27.6353 - val_acc: 0.0323\n",
            "Epoch 76/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 5.5011 - acc: 0.1331 - val_loss: 28.7255 - val_acc: 0.0323\n",
            "Epoch 77/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 6.8179 - acc: 0.1129 - val_loss: 28.7355 - val_acc: 0.0484\n",
            "Epoch 78/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.9640 - acc: 0.1210 - val_loss: 29.0435 - val_acc: 0.0323\n",
            "Epoch 79/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 5.2782 - acc: 0.0968 - val_loss: 28.9032 - val_acc: 0.0484\n",
            "Epoch 80/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 4.4021 - acc: 0.1290 - val_loss: 29.2993 - val_acc: 0.0161\n",
            "Epoch 81/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 7.3067 - acc: 0.1290 - val_loss: 28.9861 - val_acc: 0.0484\n",
            "Epoch 82/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.9971 - acc: 0.1411 - val_loss: 29.0895 - val_acc: 0.0161\n",
            "Epoch 83/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 5.3391 - acc: 0.1492 - val_loss: 32.8938 - val_acc: 0.0161\n",
            "Epoch 84/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 6.4067 - acc: 0.1250 - val_loss: 29.9686 - val_acc: 0.0161\n",
            "Epoch 85/1000\n",
            "248/248 [==============================] - 0s 86us/step - loss: 5.3959 - acc: 0.1653 - val_loss: 27.6286 - val_acc: 0.0161\n",
            "Epoch 86/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 5.5583 - acc: 0.1089 - val_loss: 30.2702 - val_acc: 0.0323\n",
            "Epoch 87/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 6.0746 - acc: 0.1371 - val_loss: 28.3029 - val_acc: 0.0323\n",
            "Epoch 88/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 5.9021 - acc: 0.1573 - val_loss: 28.1933 - val_acc: 0.0323\n",
            "Epoch 89/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.7884 - acc: 0.2097 - val_loss: 28.9092 - val_acc: 0.0161\n",
            "Epoch 90/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.3593 - acc: 0.1371 - val_loss: 28.4334 - val_acc: 0.0161\n",
            "Epoch 91/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 4.1232 - acc: 0.2016 - val_loss: 28.9840 - val_acc: 0.0161\n",
            "Epoch 92/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 5.0662 - acc: 0.1492 - val_loss: 28.5837 - val_acc: 0.0161\n",
            "Epoch 93/1000\n",
            "248/248 [==============================] - 0s 93us/step - loss: 4.4544 - acc: 0.1250 - val_loss: 32.3363 - val_acc: 0.0484\n",
            "Epoch 94/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 6.5578 - acc: 0.1048 - val_loss: 28.9480 - val_acc: 0.0161\n",
            "Epoch 95/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 3.6855 - acc: 0.1935 - val_loss: 31.4501 - val_acc: 0.0161\n",
            "Epoch 96/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 6.2984 - acc: 0.1129 - val_loss: 27.9249 - val_acc: 0.0161\n",
            "Epoch 97/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 4.3082 - acc: 0.1089 - val_loss: 27.9017 - val_acc: 0.0323\n",
            "Epoch 98/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 4.3551 - acc: 0.1089 - val_loss: 29.6561 - val_acc: 0.0161\n",
            "Epoch 99/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 6.1950 - acc: 0.1089 - val_loss: 28.3172 - val_acc: 0.0323\n",
            "Epoch 100/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 8.0984 - acc: 0.0766 - val_loss: 29.9259 - val_acc: 0.0161\n",
            "Epoch 101/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 4.2013 - acc: 0.1694 - val_loss: 28.9795 - val_acc: 0.0161\n",
            "Epoch 102/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 15.5233 - acc: 0.0847 - val_loss: 30.6565 - val_acc: 0.0645\n",
            "Epoch 103/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 11.9173 - acc: 0.0565 - val_loss: 40.7501 - val_acc: 0.0323\n",
            "Epoch 104/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 11.9358 - acc: 0.0524 - val_loss: 30.9461 - val_acc: 0.0323\n",
            "Epoch 105/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 12.8720 - acc: 0.0726 - val_loss: 31.5339 - val_acc: 0.0645\n",
            "Epoch 106/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 9.7879 - acc: 0.0524 - val_loss: 33.5102 - val_acc: 0.0484\n",
            "Epoch 107/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 12.5912 - acc: 0.0887 - val_loss: 33.4681 - val_acc: 0.0484\n",
            "Epoch 108/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 11.6128 - acc: 0.0766 - val_loss: 36.1487 - val_acc: 0.0484\n",
            "Epoch 109/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 9.5440 - acc: 0.0726 - val_loss: 34.2157 - val_acc: 0.0645\n",
            "Epoch 110/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 12.6169 - acc: 0.0484 - val_loss: 33.4186 - val_acc: 0.0645\n",
            "Epoch 111/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 11.0568 - acc: 0.0847 - val_loss: 34.7598 - val_acc: 0.0645\n",
            "Epoch 112/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 10.6762 - acc: 0.0806 - val_loss: 32.7857 - val_acc: 0.0484\n",
            "Epoch 113/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 7.6644 - acc: 0.1371 - val_loss: 31.1089 - val_acc: 0.0484\n",
            "Epoch 114/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 8.0975 - acc: 0.1371 - val_loss: 31.6071 - val_acc: 0.0484\n",
            "Epoch 115/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 8.6574 - acc: 0.1532 - val_loss: 32.4142 - val_acc: 0.0323\n",
            "Epoch 116/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 4.8640 - acc: 0.1250 - val_loss: 34.7094 - val_acc: 0.0323\n",
            "Epoch 117/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 8.8463 - acc: 0.1210 - val_loss: 30.6647 - val_acc: 0.0323\n",
            "Epoch 118/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 6.0593 - acc: 0.1411 - val_loss: 32.0451 - val_acc: 0.0484\n",
            "Epoch 119/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 6.4803 - acc: 0.0968 - val_loss: 32.5819 - val_acc: 0.0484\n",
            "Epoch 120/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 6.6010 - acc: 0.1331 - val_loss: 28.1831 - val_acc: 0.0484\n",
            "Epoch 121/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 4.9632 - acc: 0.1492 - val_loss: 28.3916 - val_acc: 0.0484\n",
            "Epoch 122/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 4.8847 - acc: 0.1371 - val_loss: 32.0889 - val_acc: 0.0484\n",
            "Epoch 123/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 6.1195 - acc: 0.1089 - val_loss: 28.0880 - val_acc: 0.0323\n",
            "Epoch 124/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 6.0411 - acc: 0.1129 - val_loss: 28.2286 - val_acc: 0.0484\n",
            "Epoch 125/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 8.2291 - acc: 0.1411 - val_loss: 27.8483 - val_acc: 0.0161\n",
            "Epoch 126/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 7.7989 - acc: 0.1089 - val_loss: 27.5977 - val_acc: 0.0323\n",
            "Epoch 127/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 4.0832 - acc: 0.1774 - val_loss: 27.8950 - val_acc: 0.0161\n",
            "Epoch 128/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 5.6665 - acc: 0.1129 - val_loss: 29.0076 - val_acc: 0.0323\n",
            "Epoch 129/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 5.8639 - acc: 0.1089 - val_loss: 27.9266 - val_acc: 0.0323\n",
            "Epoch 130/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 8.1323 - acc: 0.0927 - val_loss: 28.8098 - val_acc: 0.0484\n",
            "Epoch 131/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 4.8503 - acc: 0.1089 - val_loss: 27.4098 - val_acc: 0.0323\n",
            "Epoch 132/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 6.4238 - acc: 0.1048 - val_loss: 28.4862 - val_acc: 0.0323\n",
            "Epoch 133/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.9238 - acc: 0.1452 - val_loss: 28.7145 - val_acc: 0.0323\n",
            "Epoch 134/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.9929 - acc: 0.1452 - val_loss: 27.9763 - val_acc: 0.0323\n",
            "Epoch 135/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 5.5376 - acc: 0.1129 - val_loss: 28.3091 - val_acc: 0.0161\n",
            "Epoch 136/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 4.3247 - acc: 0.1573 - val_loss: 27.8103 - val_acc: 0.0161\n",
            "Epoch 137/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.0254 - acc: 0.1210 - val_loss: 28.1810 - val_acc: 0.0323\n",
            "Epoch 138/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 5.1709 - acc: 0.1411 - val_loss: 28.1738 - val_acc: 0.0161\n",
            "Epoch 139/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.4243 - acc: 0.1129 - val_loss: 27.9903 - val_acc: 0.0161\n",
            "Epoch 140/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.7235 - acc: 0.1129 - val_loss: 27.2974 - val_acc: 0.0484\n",
            "Epoch 141/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 4.6897 - acc: 0.1774 - val_loss: 28.3877 - val_acc: 0.0161\n",
            "Epoch 142/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.2271 - acc: 0.1250 - val_loss: 27.7461 - val_acc: 0.0161\n",
            "Epoch 143/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 5.3707 - acc: 0.1008 - val_loss: 28.0959 - val_acc: 0.0161\n",
            "Epoch 144/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 4.5581 - acc: 0.1492 - val_loss: 27.1103 - val_acc: 0.0323\n",
            "Epoch 145/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 6.2290 - acc: 0.0806 - val_loss: 33.1648 - val_acc: 0.0323\n",
            "Epoch 146/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 4.9772 - acc: 0.1734 - val_loss: 27.3342 - val_acc: 0.0484\n",
            "Epoch 147/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 5.5640 - acc: 0.1129 - val_loss: 27.7651 - val_acc: 0.0323\n",
            "Epoch 148/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 4.1013 - acc: 0.1371 - val_loss: 26.9062 - val_acc: 0.0323\n",
            "Epoch 149/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.3108 - acc: 0.1411 - val_loss: 30.4164 - val_acc: 0.0323\n",
            "Epoch 150/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 4.7500 - acc: 0.1371 - val_loss: 26.7049 - val_acc: 0.0323\n",
            "Epoch 151/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 5.8358 - acc: 0.1008 - val_loss: 28.5485 - val_acc: 0.0323\n",
            "Epoch 152/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 4.0198 - acc: 0.1210 - val_loss: 26.9384 - val_acc: 0.0161\n",
            "Epoch 153/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 6.7568 - acc: 0.1008 - val_loss: 29.8716 - val_acc: 0.0323\n",
            "Epoch 154/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 4.6864 - acc: 0.1492 - val_loss: 28.3204 - val_acc: 0.0161\n",
            "Epoch 155/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 3.7433 - acc: 0.1573 - val_loss: 28.4503 - val_acc: 0.0161\n",
            "Epoch 156/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 4.8392 - acc: 0.1290 - val_loss: 28.7215 - val_acc: 0.0323\n",
            "Epoch 157/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.5478 - acc: 0.1855 - val_loss: 28.4249 - val_acc: 0.0161\n",
            "Epoch 158/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 6.6936 - acc: 0.1290 - val_loss: 39.6466 - val_acc: 0.0161\n",
            "Epoch 159/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 6.7366 - acc: 0.1169 - val_loss: 27.9380 - val_acc: 0.0161\n",
            "Epoch 160/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.9360 - acc: 0.1734 - val_loss: 27.9779 - val_acc: 0.0323\n",
            "Epoch 161/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 4.8541 - acc: 0.1169 - val_loss: 27.7478 - val_acc: 0.0323\n",
            "Epoch 162/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 4.5513 - acc: 0.1573 - val_loss: 27.4658 - val_acc: 0.0323\n",
            "Epoch 163/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 4.0879 - acc: 0.1653 - val_loss: 26.9857 - val_acc: 0.0323\n",
            "Epoch 164/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 4.9056 - acc: 0.1169 - val_loss: 28.0655 - val_acc: 0.0323\n",
            "Epoch 165/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 3.4304 - acc: 0.1492 - val_loss: 27.7359 - val_acc: 0.0323\n",
            "Epoch 166/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 5.4337 - acc: 0.1048 - val_loss: 27.7236 - val_acc: 0.0323\n",
            "Epoch 167/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 4.0381 - acc: 0.1573 - val_loss: 27.2444 - val_acc: 0.0161\n",
            "Epoch 168/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.1480 - acc: 0.1935 - val_loss: 26.6014 - val_acc: 0.0323\n",
            "Epoch 169/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 6.0868 - acc: 0.0887 - val_loss: 30.5135 - val_acc: 0.0323\n",
            "Epoch 170/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 4.9369 - acc: 0.1532 - val_loss: 30.8209 - val_acc: 0.0000e+00\n",
            "Epoch 171/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 6.6568 - acc: 0.1411 - val_loss: 28.1598 - val_acc: 0.0161\n",
            "Epoch 172/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 4.6170 - acc: 0.1492 - val_loss: 27.3666 - val_acc: 0.0161\n",
            "Epoch 173/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.3810 - acc: 0.1734 - val_loss: 27.8432 - val_acc: 0.0161\n",
            "Epoch 174/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 3.5356 - acc: 0.1976 - val_loss: 27.3461 - val_acc: 0.0161\n",
            "Epoch 175/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 3.4017 - acc: 0.1935 - val_loss: 25.8636 - val_acc: 0.0323\n",
            "Epoch 176/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 4.3927 - acc: 0.1694 - val_loss: 28.2074 - val_acc: 0.0161\n",
            "Epoch 177/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.8245 - acc: 0.1815 - val_loss: 27.4812 - val_acc: 0.0161\n",
            "Epoch 178/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 4.0735 - acc: 0.1492 - val_loss: 27.0721 - val_acc: 0.0323\n",
            "Epoch 179/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 4.1209 - acc: 0.1774 - val_loss: 25.4798 - val_acc: 0.0323\n",
            "Epoch 180/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 3.7342 - acc: 0.1694 - val_loss: 27.3505 - val_acc: 0.0161\n",
            "Epoch 181/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.7850 - acc: 0.1855 - val_loss: 26.7931 - val_acc: 0.0323\n",
            "Epoch 182/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.3502 - acc: 0.2056 - val_loss: 25.4287 - val_acc: 0.0161\n",
            "Epoch 183/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.8858 - acc: 0.1694 - val_loss: 28.1835 - val_acc: 0.0161\n",
            "Epoch 184/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.0677 - acc: 0.2298 - val_loss: 27.7548 - val_acc: 0.0323\n",
            "Epoch 185/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 4.8126 - acc: 0.1169 - val_loss: 26.5534 - val_acc: 0.0323\n",
            "Epoch 186/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.6294 - acc: 0.1411 - val_loss: 27.6959 - val_acc: 0.0323\n",
            "Epoch 187/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 3.5062 - acc: 0.1855 - val_loss: 26.2414 - val_acc: 0.0484\n",
            "Epoch 188/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.4518 - acc: 0.1935 - val_loss: 27.4813 - val_acc: 0.0323\n",
            "Epoch 189/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.8004 - acc: 0.1613 - val_loss: 30.9172 - val_acc: 0.0000e+00\n",
            "Epoch 190/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 3.6908 - acc: 0.1653 - val_loss: 25.6910 - val_acc: 0.0161\n",
            "Epoch 191/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.6816 - acc: 0.1653 - val_loss: 26.7887 - val_acc: 0.0161\n",
            "Epoch 192/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.2654 - acc: 0.1976 - val_loss: 27.3424 - val_acc: 0.0161\n",
            "Epoch 193/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 3.6219 - acc: 0.1734 - val_loss: 28.2292 - val_acc: 0.0000e+00\n",
            "Epoch 194/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 3.7336 - acc: 0.1613 - val_loss: 29.8404 - val_acc: 0.0000e+00\n",
            "Epoch 195/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 4.4099 - acc: 0.1573 - val_loss: 29.1553 - val_acc: 0.0161\n",
            "Epoch 196/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.4517 - acc: 0.1855 - val_loss: 25.6585 - val_acc: 0.0161\n",
            "Epoch 197/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.6866 - acc: 0.2339 - val_loss: 26.2699 - val_acc: 0.0323\n",
            "Epoch 198/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 3.6136 - acc: 0.1815 - val_loss: 27.0301 - val_acc: 0.0000e+00\n",
            "Epoch 199/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.8172 - acc: 0.2419 - val_loss: 28.3559 - val_acc: 0.0161\n",
            "Epoch 200/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 3.6201 - acc: 0.1734 - val_loss: 25.2584 - val_acc: 0.0000e+00\n",
            "Epoch 201/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.7447 - acc: 0.1734 - val_loss: 25.7057 - val_acc: 0.0000e+00\n",
            "Epoch 202/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 3.5096 - acc: 0.1935 - val_loss: 26.7767 - val_acc: 0.0161\n",
            "Epoch 203/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.4219 - acc: 0.2056 - val_loss: 28.3529 - val_acc: 0.0000e+00\n",
            "Epoch 204/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.3431 - acc: 0.1855 - val_loss: 25.2105 - val_acc: 0.0000e+00\n",
            "Epoch 205/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.3446 - acc: 0.2097 - val_loss: 29.0200 - val_acc: 0.0161\n",
            "Epoch 206/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.7656 - acc: 0.1935 - val_loss: 26.0495 - val_acc: 0.0000e+00\n",
            "Epoch 207/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.1317 - acc: 0.1895 - val_loss: 25.5452 - val_acc: 0.0161\n",
            "Epoch 208/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.8487 - acc: 0.1532 - val_loss: 25.1265 - val_acc: 0.0161\n",
            "Epoch 209/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.4822 - acc: 0.1653 - val_loss: 27.2490 - val_acc: 0.0161\n",
            "Epoch 210/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 3.0029 - acc: 0.2177 - val_loss: 29.0634 - val_acc: 0.0161\n",
            "Epoch 211/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.9460 - acc: 0.2258 - val_loss: 24.4275 - val_acc: 0.0161\n",
            "Epoch 212/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.4122 - acc: 0.1855 - val_loss: 24.5480 - val_acc: 0.0323\n",
            "Epoch 213/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 3.4861 - acc: 0.1895 - val_loss: 25.7871 - val_acc: 0.0161\n",
            "Epoch 214/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.7285 - acc: 0.2258 - val_loss: 25.5137 - val_acc: 0.0161\n",
            "Epoch 215/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.8612 - acc: 0.2460 - val_loss: 24.5935 - val_acc: 0.0323\n",
            "Epoch 216/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.7421 - acc: 0.2137 - val_loss: 25.8323 - val_acc: 0.0161\n",
            "Epoch 217/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 3.0598 - acc: 0.2258 - val_loss: 24.6781 - val_acc: 0.0323\n",
            "Epoch 218/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.7563 - acc: 0.2258 - val_loss: 24.7789 - val_acc: 0.0323\n",
            "Epoch 219/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 3.4659 - acc: 0.1976 - val_loss: 24.3766 - val_acc: 0.0323\n",
            "Epoch 220/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.0353 - acc: 0.2056 - val_loss: 28.9704 - val_acc: 0.0161\n",
            "Epoch 221/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.9955 - acc: 0.2177 - val_loss: 23.9277 - val_acc: 0.0161\n",
            "Epoch 222/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 3.7708 - acc: 0.1573 - val_loss: 23.6232 - val_acc: 0.0323\n",
            "Epoch 223/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.9647 - acc: 0.2258 - val_loss: 23.8537 - val_acc: 0.0484\n",
            "Epoch 224/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.8609 - acc: 0.2177 - val_loss: 32.0003 - val_acc: 0.0161\n",
            "Epoch 225/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 4.7636 - acc: 0.1895 - val_loss: 24.5447 - val_acc: 0.0161\n",
            "Epoch 226/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.3178 - acc: 0.2016 - val_loss: 23.4192 - val_acc: 0.0323\n",
            "Epoch 227/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.9548 - acc: 0.1855 - val_loss: 23.5987 - val_acc: 0.0323\n",
            "Epoch 228/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.5031 - acc: 0.1895 - val_loss: 25.2509 - val_acc: 0.0484\n",
            "Epoch 229/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.6087 - acc: 0.2218 - val_loss: 25.8205 - val_acc: 0.0645\n",
            "Epoch 230/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.1696 - acc: 0.2137 - val_loss: 26.0691 - val_acc: 0.0645\n",
            "Epoch 231/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.0853 - acc: 0.1855 - val_loss: 24.6741 - val_acc: 0.0645\n",
            "Epoch 232/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.6887 - acc: 0.2097 - val_loss: 24.0103 - val_acc: 0.0645\n",
            "Epoch 233/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.8339 - acc: 0.2500 - val_loss: 23.8967 - val_acc: 0.0484\n",
            "Epoch 234/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.0363 - acc: 0.2177 - val_loss: 27.4264 - val_acc: 0.0806\n",
            "Epoch 235/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.3084 - acc: 0.1613 - val_loss: 22.3026 - val_acc: 0.0968\n",
            "Epoch 236/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.6826 - acc: 0.1613 - val_loss: 22.2217 - val_acc: 0.0806\n",
            "Epoch 237/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.2049 - acc: 0.2379 - val_loss: 21.8292 - val_acc: 0.1129\n",
            "Epoch 238/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 3.4086 - acc: 0.1613 - val_loss: 21.7820 - val_acc: 0.0806\n",
            "Epoch 239/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 3.7624 - acc: 0.1895 - val_loss: 27.5285 - val_acc: 0.0645\n",
            "Epoch 240/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.8048 - acc: 0.1895 - val_loss: 24.1289 - val_acc: 0.0968\n",
            "Epoch 241/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 3.6637 - acc: 0.1895 - val_loss: 23.1153 - val_acc: 0.1290\n",
            "Epoch 242/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.3162 - acc: 0.1774 - val_loss: 22.3460 - val_acc: 0.0968\n",
            "Epoch 243/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.6992 - acc: 0.2500 - val_loss: 29.8016 - val_acc: 0.0484\n",
            "Epoch 244/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.5998 - acc: 0.1613 - val_loss: 23.3136 - val_acc: 0.0968\n",
            "Epoch 245/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.4671 - acc: 0.2621 - val_loss: 22.8631 - val_acc: 0.0645\n",
            "Epoch 246/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.0732 - acc: 0.1573 - val_loss: 31.1475 - val_acc: 0.0484\n",
            "Epoch 247/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 4.3230 - acc: 0.1734 - val_loss: 22.4891 - val_acc: 0.0968\n",
            "Epoch 248/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.6850 - acc: 0.1855 - val_loss: 22.7719 - val_acc: 0.0323\n",
            "Epoch 249/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.5449 - acc: 0.2339 - val_loss: 23.5909 - val_acc: 0.0645\n",
            "Epoch 250/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.8087 - acc: 0.2056 - val_loss: 24.3226 - val_acc: 0.0806\n",
            "Epoch 251/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.6016 - acc: 0.1492 - val_loss: 23.6861 - val_acc: 0.0968\n",
            "Epoch 252/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.6457 - acc: 0.2177 - val_loss: 24.6377 - val_acc: 0.0806\n",
            "Epoch 253/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.9715 - acc: 0.2298 - val_loss: 21.9421 - val_acc: 0.0806\n",
            "Epoch 254/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.6985 - acc: 0.1976 - val_loss: 21.5713 - val_acc: 0.0968\n",
            "Epoch 255/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 3.1159 - acc: 0.2137 - val_loss: 26.0556 - val_acc: 0.0806\n",
            "Epoch 256/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.9326 - acc: 0.1734 - val_loss: 20.7645 - val_acc: 0.0645\n",
            "Epoch 257/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.1262 - acc: 0.1855 - val_loss: 27.1187 - val_acc: 0.0645\n",
            "Epoch 258/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.8680 - acc: 0.1976 - val_loss: 22.7448 - val_acc: 0.0806\n",
            "Epoch 259/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.5066 - acc: 0.1613 - val_loss: 20.8644 - val_acc: 0.0968\n",
            "Epoch 260/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.7717 - acc: 0.2460 - val_loss: 21.4644 - val_acc: 0.0645\n",
            "Epoch 261/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.7473 - acc: 0.2177 - val_loss: 30.1622 - val_acc: 0.0645\n",
            "Epoch 262/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 4.2823 - acc: 0.1694 - val_loss: 24.2979 - val_acc: 0.0806\n",
            "Epoch 263/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 3.2565 - acc: 0.1815 - val_loss: 23.0999 - val_acc: 0.0806\n",
            "Epoch 264/1000\n",
            "248/248 [==============================] - 0s 91us/step - loss: 2.3636 - acc: 0.2944 - val_loss: 21.5045 - val_acc: 0.0806\n",
            "Epoch 265/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.2794 - acc: 0.1976 - val_loss: 22.8322 - val_acc: 0.0645\n",
            "Epoch 266/1000\n",
            "248/248 [==============================] - 0s 109us/step - loss: 2.2232 - acc: 0.2460 - val_loss: 22.8249 - val_acc: 0.0806\n",
            "Epoch 267/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.4071 - acc: 0.2540 - val_loss: 23.5121 - val_acc: 0.0806\n",
            "Epoch 268/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.5424 - acc: 0.2298 - val_loss: 22.0907 - val_acc: 0.0645\n",
            "Epoch 269/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 3.0727 - acc: 0.1815 - val_loss: 23.8473 - val_acc: 0.0645\n",
            "Epoch 270/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.9353 - acc: 0.2137 - val_loss: 21.2704 - val_acc: 0.1129\n",
            "Epoch 271/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.5892 - acc: 0.2056 - val_loss: 21.3556 - val_acc: 0.0968\n",
            "Epoch 272/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.6411 - acc: 0.1815 - val_loss: 21.8495 - val_acc: 0.0968\n",
            "Epoch 273/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.6183 - acc: 0.2258 - val_loss: 21.1915 - val_acc: 0.0968\n",
            "Epoch 274/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.2333 - acc: 0.2379 - val_loss: 21.4538 - val_acc: 0.0968\n",
            "Epoch 275/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.1280 - acc: 0.2500 - val_loss: 21.1644 - val_acc: 0.0968\n",
            "Epoch 276/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 2.6610 - acc: 0.2298 - val_loss: 20.7127 - val_acc: 0.0968\n",
            "Epoch 277/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.4477 - acc: 0.1976 - val_loss: 24.6591 - val_acc: 0.0806\n",
            "Epoch 278/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.5499 - acc: 0.2016 - val_loss: 20.8145 - val_acc: 0.0968\n",
            "Epoch 279/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.7211 - acc: 0.2460 - val_loss: 20.7153 - val_acc: 0.1290\n",
            "Epoch 280/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.3249 - acc: 0.2540 - val_loss: 22.6998 - val_acc: 0.1129\n",
            "Epoch 281/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2288 - acc: 0.2460 - val_loss: 23.8288 - val_acc: 0.0645\n",
            "Epoch 282/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.4720 - acc: 0.2379 - val_loss: 20.7924 - val_acc: 0.1452\n",
            "Epoch 283/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.3337 - acc: 0.2097 - val_loss: 23.1301 - val_acc: 0.0645\n",
            "Epoch 284/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.2660 - acc: 0.2218 - val_loss: 21.6155 - val_acc: 0.0645\n",
            "Epoch 285/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.6733 - acc: 0.2339 - val_loss: 18.7647 - val_acc: 0.1129\n",
            "Epoch 286/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2272 - acc: 0.2823 - val_loss: 21.4639 - val_acc: 0.0484\n",
            "Epoch 287/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2084 - acc: 0.2419 - val_loss: 24.3955 - val_acc: 0.0484\n",
            "Epoch 288/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.1193 - acc: 0.2097 - val_loss: 22.7147 - val_acc: 0.0645\n",
            "Epoch 289/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.9990 - acc: 0.2863 - val_loss: 21.6435 - val_acc: 0.1129\n",
            "Epoch 290/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.8557 - acc: 0.2137 - val_loss: 26.8897 - val_acc: 0.0806\n",
            "Epoch 291/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2880 - acc: 0.2500 - val_loss: 21.6999 - val_acc: 0.0968\n",
            "Epoch 292/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 2.4238 - acc: 0.2500 - val_loss: 19.4457 - val_acc: 0.0806\n",
            "Epoch 293/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.7700 - acc: 0.2419 - val_loss: 21.6047 - val_acc: 0.0645\n",
            "Epoch 294/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.6262 - acc: 0.2500 - val_loss: 25.4311 - val_acc: 0.0645\n",
            "Epoch 295/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.1151 - acc: 0.2298 - val_loss: 23.6164 - val_acc: 0.0645\n",
            "Epoch 296/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 3.1439 - acc: 0.1895 - val_loss: 24.3719 - val_acc: 0.0645\n",
            "Epoch 297/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.7046 - acc: 0.2097 - val_loss: 20.9092 - val_acc: 0.0806\n",
            "Epoch 298/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.6742 - acc: 0.2177 - val_loss: 20.8259 - val_acc: 0.1290\n",
            "Epoch 299/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.7792 - acc: 0.2056 - val_loss: 21.5378 - val_acc: 0.0484\n",
            "Epoch 300/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.1906 - acc: 0.2540 - val_loss: 21.2802 - val_acc: 0.0323\n",
            "Epoch 301/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.1623 - acc: 0.2258 - val_loss: 23.5078 - val_acc: 0.0645\n",
            "Epoch 302/1000\n",
            "248/248 [==============================] - 0s 89us/step - loss: 3.0288 - acc: 0.2097 - val_loss: 21.5418 - val_acc: 0.0645\n",
            "Epoch 303/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0024 - acc: 0.2944 - val_loss: 21.1503 - val_acc: 0.0968\n",
            "Epoch 304/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.3568 - acc: 0.1734 - val_loss: 20.9879 - val_acc: 0.0968\n",
            "Epoch 305/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.9500 - acc: 0.2218 - val_loss: 21.4805 - val_acc: 0.0968\n",
            "Epoch 306/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.4236 - acc: 0.2500 - val_loss: 20.8446 - val_acc: 0.0645\n",
            "Epoch 307/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.2929 - acc: 0.2177 - val_loss: 22.3887 - val_acc: 0.0645\n",
            "Epoch 308/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.0775 - acc: 0.2702 - val_loss: 19.4437 - val_acc: 0.0968\n",
            "Epoch 309/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.3933 - acc: 0.2298 - val_loss: 20.1834 - val_acc: 0.0645\n",
            "Epoch 310/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.6575 - acc: 0.2097 - val_loss: 20.5610 - val_acc: 0.0806\n",
            "Epoch 311/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.0367 - acc: 0.1935 - val_loss: 21.9227 - val_acc: 0.0484\n",
            "Epoch 312/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.5702 - acc: 0.1976 - val_loss: 20.9739 - val_acc: 0.1129\n",
            "Epoch 313/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.2874 - acc: 0.2016 - val_loss: 22.2752 - val_acc: 0.0645\n",
            "Epoch 314/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.3257 - acc: 0.2621 - val_loss: 24.2139 - val_acc: 0.0806\n",
            "Epoch 315/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.1862 - acc: 0.2298 - val_loss: 21.9397 - val_acc: 0.0645\n",
            "Epoch 316/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.2450 - acc: 0.2500 - val_loss: 31.0487 - val_acc: 0.0484\n",
            "Epoch 317/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.5717 - acc: 0.2218 - val_loss: 20.6755 - val_acc: 0.0645\n",
            "Epoch 318/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.9943 - acc: 0.2581 - val_loss: 20.2978 - val_acc: 0.0484\n",
            "Epoch 319/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.0439 - acc: 0.2339 - val_loss: 21.9755 - val_acc: 0.0000e+00\n",
            "Epoch 320/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0076 - acc: 0.2742 - val_loss: 22.8309 - val_acc: 0.0968\n",
            "Epoch 321/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.8017 - acc: 0.1613 - val_loss: 21.3002 - val_acc: 0.0484\n",
            "Epoch 322/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.8467 - acc: 0.1855 - val_loss: 22.0599 - val_acc: 0.0000e+00\n",
            "Epoch 323/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.9133 - acc: 0.2339 - val_loss: 21.1958 - val_acc: 0.0161\n",
            "Epoch 324/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.0542 - acc: 0.2782 - val_loss: 25.1113 - val_acc: 0.0806\n",
            "Epoch 325/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.2710 - acc: 0.2097 - val_loss: 20.2832 - val_acc: 0.1129\n",
            "Epoch 326/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 2.1724 - acc: 0.2823 - val_loss: 21.0471 - val_acc: 0.0968\n",
            "Epoch 327/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.1176 - acc: 0.2500 - val_loss: 20.2143 - val_acc: 0.0806\n",
            "Epoch 328/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.0374 - acc: 0.2702 - val_loss: 22.5845 - val_acc: 0.0161\n",
            "Epoch 329/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.6193 - acc: 0.2177 - val_loss: 24.6389 - val_acc: 0.0323\n",
            "Epoch 330/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.4373 - acc: 0.2097 - val_loss: 21.1374 - val_acc: 0.0484\n",
            "Epoch 331/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0690 - acc: 0.2419 - val_loss: 21.5593 - val_acc: 0.0806\n",
            "Epoch 332/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.3418 - acc: 0.2460 - val_loss: 21.4311 - val_acc: 0.0484\n",
            "Epoch 333/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.9965 - acc: 0.2258 - val_loss: 20.0436 - val_acc: 0.0645\n",
            "Epoch 334/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.1264 - acc: 0.2661 - val_loss: 20.2307 - val_acc: 0.0323\n",
            "Epoch 335/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.0456 - acc: 0.2339 - val_loss: 21.5691 - val_acc: 0.0000e+00\n",
            "Epoch 336/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.4221 - acc: 0.2621 - val_loss: 21.2728 - val_acc: 0.0000e+00\n",
            "Epoch 337/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.9929 - acc: 0.2177 - val_loss: 20.5868 - val_acc: 0.0161\n",
            "Epoch 338/1000\n",
            "248/248 [==============================] - 0s 46us/step - loss: 2.1582 - acc: 0.2177 - val_loss: 20.1877 - val_acc: 0.0323\n",
            "Epoch 339/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.6807 - acc: 0.2258 - val_loss: 20.9557 - val_acc: 0.0484\n",
            "Epoch 340/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.6100 - acc: 0.2056 - val_loss: 20.5654 - val_acc: 0.0484\n",
            "Epoch 341/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.9313 - acc: 0.2823 - val_loss: 21.2336 - val_acc: 0.0484\n",
            "Epoch 342/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.5443 - acc: 0.2298 - val_loss: 19.2127 - val_acc: 0.0484\n",
            "Epoch 343/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 2.0376 - acc: 0.2419 - val_loss: 23.6584 - val_acc: 0.0968\n",
            "Epoch 344/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.3508 - acc: 0.2258 - val_loss: 20.6141 - val_acc: 0.0645\n",
            "Epoch 345/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.8631 - acc: 0.2782 - val_loss: 23.6096 - val_acc: 0.0323\n",
            "Epoch 346/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.3256 - acc: 0.2621 - val_loss: 20.7470 - val_acc: 0.0000e+00\n",
            "Epoch 347/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.9425 - acc: 0.2581 - val_loss: 21.2056 - val_acc: 0.0484\n",
            "Epoch 348/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 2.1590 - acc: 0.2702 - val_loss: 19.1949 - val_acc: 0.0484\n",
            "Epoch 349/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.8727 - acc: 0.2702 - val_loss: 21.2511 - val_acc: 0.0161\n",
            "Epoch 350/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.9319 - acc: 0.2540 - val_loss: 21.1730 - val_acc: 0.0323\n",
            "Epoch 351/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.1427 - acc: 0.2016 - val_loss: 21.6210 - val_acc: 0.0161\n",
            "Epoch 352/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.8934 - acc: 0.2500 - val_loss: 24.8053 - val_acc: 0.0645\n",
            "Epoch 353/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.9382 - acc: 0.1815 - val_loss: 21.3150 - val_acc: 0.0645\n",
            "Epoch 354/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.8694 - acc: 0.2581 - val_loss: 21.9993 - val_acc: 0.0645\n",
            "Epoch 355/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.9020 - acc: 0.2460 - val_loss: 19.8659 - val_acc: 0.0161\n",
            "Epoch 356/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.8740 - acc: 0.2621 - val_loss: 19.1662 - val_acc: 0.0323\n",
            "Epoch 357/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 3.8456 - acc: 0.1895 - val_loss: 17.7556 - val_acc: 0.0323\n",
            "Epoch 358/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 2.2966 - acc: 0.2944 - val_loss: 22.0298 - val_acc: 0.0645\n",
            "Epoch 359/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.9057 - acc: 0.2540 - val_loss: 20.2871 - val_acc: 0.0000e+00\n",
            "Epoch 360/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.8922 - acc: 0.2823 - val_loss: 24.3400 - val_acc: 0.0806\n",
            "Epoch 361/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.1175 - acc: 0.2540 - val_loss: 23.5145 - val_acc: 0.0806\n",
            "Epoch 362/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0032 - acc: 0.2621 - val_loss: 21.0313 - val_acc: 0.0000e+00\n",
            "Epoch 363/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.8654 - acc: 0.2581 - val_loss: 21.9743 - val_acc: 0.0484\n",
            "Epoch 364/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.8796 - acc: 0.2581 - val_loss: 21.9043 - val_acc: 0.0161\n",
            "Epoch 365/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.4844 - acc: 0.2298 - val_loss: 22.0074 - val_acc: 0.0161\n",
            "Epoch 366/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.1117 - acc: 0.2419 - val_loss: 22.1484 - val_acc: 0.0323\n",
            "Epoch 367/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6898 - acc: 0.2823 - val_loss: 21.5364 - val_acc: 0.0161\n",
            "Epoch 368/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.9211 - acc: 0.2742 - val_loss: 22.8054 - val_acc: 0.0323\n",
            "Epoch 369/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.9485 - acc: 0.2339 - val_loss: 26.4166 - val_acc: 0.0806\n",
            "Epoch 370/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.1612 - acc: 0.2540 - val_loss: 24.3412 - val_acc: 0.0323\n",
            "Epoch 371/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.6629 - acc: 0.2056 - val_loss: 21.3868 - val_acc: 0.0645\n",
            "Epoch 372/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.2118 - acc: 0.2298 - val_loss: 23.1676 - val_acc: 0.0323\n",
            "Epoch 373/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 2.2016 - acc: 0.2097 - val_loss: 23.7609 - val_acc: 0.0323\n",
            "Epoch 374/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.0432 - acc: 0.2581 - val_loss: 23.1059 - val_acc: 0.0806\n",
            "Epoch 375/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.3868 - acc: 0.2379 - val_loss: 21.3308 - val_acc: 0.0161\n",
            "Epoch 376/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.5834 - acc: 0.2056 - val_loss: 23.7090 - val_acc: 0.0323\n",
            "Epoch 377/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.8206 - acc: 0.1976 - val_loss: 24.1476 - val_acc: 0.0645\n",
            "Epoch 378/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.4944 - acc: 0.2056 - val_loss: 24.0165 - val_acc: 0.0000e+00\n",
            "Epoch 379/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.1839 - acc: 0.2016 - val_loss: 24.9170 - val_acc: 0.0645\n",
            "Epoch 380/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.9072 - acc: 0.2782 - val_loss: 25.3072 - val_acc: 0.0323\n",
            "Epoch 381/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 2.1401 - acc: 0.2419 - val_loss: 24.9189 - val_acc: 0.0484\n",
            "Epoch 382/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 2.0199 - acc: 0.2339 - val_loss: 21.2884 - val_acc: 0.0323\n",
            "Epoch 383/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.7172 - acc: 0.2823 - val_loss: 21.3847 - val_acc: 0.0323\n",
            "Epoch 384/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.7901 - acc: 0.2258 - val_loss: 23.0464 - val_acc: 0.0484\n",
            "Epoch 385/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.3619 - acc: 0.1734 - val_loss: 20.7101 - val_acc: 0.0323\n",
            "Epoch 386/1000\n",
            "248/248 [==============================] - 0s 86us/step - loss: 2.2893 - acc: 0.1976 - val_loss: 21.7765 - val_acc: 0.0161\n",
            "Epoch 387/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.8949 - acc: 0.2903 - val_loss: 21.8609 - val_acc: 0.0323\n",
            "Epoch 388/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.6874 - acc: 0.2581 - val_loss: 21.2588 - val_acc: 0.0161\n",
            "Epoch 389/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.8977 - acc: 0.2702 - val_loss: 23.9383 - val_acc: 0.0323\n",
            "Epoch 390/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.9616 - acc: 0.2984 - val_loss: 22.0890 - val_acc: 0.0161\n",
            "Epoch 391/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.9974 - acc: 0.2581 - val_loss: 21.4805 - val_acc: 0.0000e+00\n",
            "Epoch 392/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.2736 - acc: 0.1935 - val_loss: 23.8331 - val_acc: 0.0806\n",
            "Epoch 393/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.8881 - acc: 0.2379 - val_loss: 20.6235 - val_acc: 0.0000e+00\n",
            "Epoch 394/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.6233 - acc: 0.2823 - val_loss: 21.2711 - val_acc: 0.0323\n",
            "Epoch 395/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.7310 - acc: 0.2742 - val_loss: 20.5681 - val_acc: 0.0323\n",
            "Epoch 396/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.7531 - acc: 0.2218 - val_loss: 22.4565 - val_acc: 0.0000e+00\n",
            "Epoch 397/1000\n",
            "248/248 [==============================] - 0s 92us/step - loss: 2.4324 - acc: 0.2339 - val_loss: 20.3290 - val_acc: 0.0323\n",
            "Epoch 398/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.8583 - acc: 0.2500 - val_loss: 22.3096 - val_acc: 0.0645\n",
            "Epoch 399/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.8833 - acc: 0.2621 - val_loss: 20.5521 - val_acc: 0.0323\n",
            "Epoch 400/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.8361 - acc: 0.2944 - val_loss: 20.3021 - val_acc: 0.0323\n",
            "Epoch 401/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.0388 - acc: 0.2258 - val_loss: 30.8885 - val_acc: 0.0645\n",
            "Epoch 402/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.3324 - acc: 0.2581 - val_loss: 27.2903 - val_acc: 0.0484\n",
            "Epoch 403/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.1874 - acc: 0.2298 - val_loss: 21.8212 - val_acc: 0.0000e+00\n",
            "Epoch 404/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.8254 - acc: 0.2218 - val_loss: 22.2468 - val_acc: 0.0161\n",
            "Epoch 405/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.8902 - acc: 0.2661 - val_loss: 22.1537 - val_acc: 0.0484\n",
            "Epoch 406/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.1927 - acc: 0.2177 - val_loss: 22.8098 - val_acc: 0.0161\n",
            "Epoch 407/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.8194 - acc: 0.2702 - val_loss: 20.8716 - val_acc: 0.0484\n",
            "Epoch 408/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.2764 - acc: 0.2258 - val_loss: 22.3139 - val_acc: 0.0000e+00\n",
            "Epoch 409/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.9244 - acc: 0.2540 - val_loss: 21.9912 - val_acc: 0.0484\n",
            "Epoch 410/1000\n",
            "248/248 [==============================] - 0s 91us/step - loss: 1.6846 - acc: 0.2742 - val_loss: 24.0954 - val_acc: 0.0645\n",
            "Epoch 411/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.6395 - acc: 0.2339 - val_loss: 21.8891 - val_acc: 0.0161\n",
            "Epoch 412/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.6426 - acc: 0.2863 - val_loss: 23.3670 - val_acc: 0.0323\n",
            "Epoch 413/1000\n",
            "248/248 [==============================] - 0s 80us/step - loss: 1.7964 - acc: 0.2419 - val_loss: 23.0551 - val_acc: 0.0484\n",
            "Epoch 414/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.9669 - acc: 0.2177 - val_loss: 19.9812 - val_acc: 0.0161\n",
            "Epoch 415/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.8854 - acc: 0.2339 - val_loss: 22.2254 - val_acc: 0.0323\n",
            "Epoch 416/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.9057 - acc: 0.2500 - val_loss: 22.0568 - val_acc: 0.0000e+00\n",
            "Epoch 417/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.5285 - acc: 0.2984 - val_loss: 20.6639 - val_acc: 0.0161\n",
            "Epoch 418/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.8684 - acc: 0.2460 - val_loss: 23.3580 - val_acc: 0.0161\n",
            "Epoch 419/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.8033 - acc: 0.2500 - val_loss: 21.6287 - val_acc: 0.0000e+00\n",
            "Epoch 420/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.6290 - acc: 0.2540 - val_loss: 22.0364 - val_acc: 0.0161\n",
            "Epoch 421/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6514 - acc: 0.2903 - val_loss: 22.5254 - val_acc: 0.0323\n",
            "Epoch 422/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.5877 - acc: 0.2782 - val_loss: 23.8478 - val_acc: 0.0161\n",
            "Epoch 423/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.7504 - acc: 0.2460 - val_loss: 19.8847 - val_acc: 0.0000e+00\n",
            "Epoch 424/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.8877 - acc: 0.2379 - val_loss: 21.7967 - val_acc: 0.0000e+00\n",
            "Epoch 425/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6050 - acc: 0.2782 - val_loss: 21.2103 - val_acc: 0.0323\n",
            "Epoch 426/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.6044 - acc: 0.2500 - val_loss: 23.1027 - val_acc: 0.0000e+00\n",
            "Epoch 427/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.7997 - acc: 0.1855 - val_loss: 23.0363 - val_acc: 0.0484\n",
            "Epoch 428/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.7670 - acc: 0.2782 - val_loss: 24.2370 - val_acc: 0.0323\n",
            "Epoch 429/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.6023 - acc: 0.2379 - val_loss: 22.3169 - val_acc: 0.0484\n",
            "Epoch 430/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.6849 - acc: 0.2581 - val_loss: 24.0740 - val_acc: 0.0645\n",
            "Epoch 431/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.6982 - acc: 0.2177 - val_loss: 20.5794 - val_acc: 0.0484\n",
            "Epoch 432/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.8996 - acc: 0.2661 - val_loss: 23.3314 - val_acc: 0.0484\n",
            "Epoch 433/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6757 - acc: 0.2339 - val_loss: 22.4708 - val_acc: 0.0161\n",
            "Epoch 434/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6973 - acc: 0.2823 - val_loss: 22.6002 - val_acc: 0.0645\n",
            "Epoch 435/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.2072 - acc: 0.2661 - val_loss: 22.6673 - val_acc: 0.0000e+00\n",
            "Epoch 436/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.6843 - acc: 0.2540 - val_loss: 22.1049 - val_acc: 0.0323\n",
            "Epoch 437/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6262 - acc: 0.2500 - val_loss: 24.9788 - val_acc: 0.0161\n",
            "Epoch 438/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.7420 - acc: 0.2702 - val_loss: 23.0718 - val_acc: 0.0484\n",
            "Epoch 439/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.8459 - acc: 0.2258 - val_loss: 21.7281 - val_acc: 0.0161\n",
            "Epoch 440/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.6783 - acc: 0.2984 - val_loss: 22.3564 - val_acc: 0.0323\n",
            "Epoch 441/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.6226 - acc: 0.2702 - val_loss: 22.7994 - val_acc: 0.0000e+00\n",
            "Epoch 442/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.7508 - acc: 0.2419 - val_loss: 21.6413 - val_acc: 0.0161\n",
            "Epoch 443/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.8291 - acc: 0.2460 - val_loss: 27.5993 - val_acc: 0.0323\n",
            "Epoch 444/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.6609 - acc: 0.2177 - val_loss: 24.8576 - val_acc: 0.0484\n",
            "Epoch 445/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.9324 - acc: 0.2500 - val_loss: 21.1589 - val_acc: 0.0161\n",
            "Epoch 446/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.7029 - acc: 0.2782 - val_loss: 22.8308 - val_acc: 0.0484\n",
            "Epoch 447/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.8196 - acc: 0.2702 - val_loss: 21.5997 - val_acc: 0.0323\n",
            "Epoch 448/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 1.4592 - acc: 0.3065 - val_loss: 21.3267 - val_acc: 0.0323\n",
            "Epoch 449/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.5705 - acc: 0.2742 - val_loss: 21.5385 - val_acc: 0.0161\n",
            "Epoch 450/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6367 - acc: 0.2661 - val_loss: 21.1489 - val_acc: 0.0323\n",
            "Epoch 451/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.7012 - acc: 0.2419 - val_loss: 21.7189 - val_acc: 0.0000e+00\n",
            "Epoch 452/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.4379 - acc: 0.2500 - val_loss: 22.9749 - val_acc: 0.0484\n",
            "Epoch 453/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.0400 - acc: 0.2460 - val_loss: 25.2096 - val_acc: 0.0645\n",
            "Epoch 454/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.0704 - acc: 0.2137 - val_loss: 23.3784 - val_acc: 0.0323\n",
            "Epoch 455/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.8171 - acc: 0.2500 - val_loss: 23.1423 - val_acc: 0.0161\n",
            "Epoch 456/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.0978 - acc: 0.1855 - val_loss: 24.0523 - val_acc: 0.0484\n",
            "Epoch 457/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.8382 - acc: 0.2661 - val_loss: 19.6663 - val_acc: 0.0323\n",
            "Epoch 458/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4650 - acc: 0.3024 - val_loss: 24.6019 - val_acc: 0.0484\n",
            "Epoch 459/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.9231 - acc: 0.2016 - val_loss: 21.5663 - val_acc: 0.0323\n",
            "Epoch 460/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.7825 - acc: 0.2581 - val_loss: 22.6477 - val_acc: 0.0323\n",
            "Epoch 461/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.5510 - acc: 0.2460 - val_loss: 24.3345 - val_acc: 0.0484\n",
            "Epoch 462/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.1398 - acc: 0.2621 - val_loss: 23.2106 - val_acc: 0.0323\n",
            "Epoch 463/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.6037 - acc: 0.2460 - val_loss: 29.0888 - val_acc: 0.0323\n",
            "Epoch 464/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.0444 - acc: 0.2419 - val_loss: 23.3276 - val_acc: 0.0484\n",
            "Epoch 465/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.7849 - acc: 0.2540 - val_loss: 23.1053 - val_acc: 0.0161\n",
            "Epoch 466/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.4901 - acc: 0.3105 - val_loss: 20.6975 - val_acc: 0.0484\n",
            "Epoch 467/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.7421 - acc: 0.2137 - val_loss: 22.7393 - val_acc: 0.0323\n",
            "Epoch 468/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.6061 - acc: 0.2218 - val_loss: 22.0595 - val_acc: 0.0323\n",
            "Epoch 469/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.2373 - acc: 0.2097 - val_loss: 22.8561 - val_acc: 0.0484\n",
            "Epoch 470/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.6252 - acc: 0.2016 - val_loss: 23.1643 - val_acc: 0.0323\n",
            "Epoch 471/1000\n",
            "248/248 [==============================] - 0s 89us/step - loss: 1.8461 - acc: 0.2581 - val_loss: 24.2224 - val_acc: 0.0806\n",
            "Epoch 472/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.9992 - acc: 0.1734 - val_loss: 22.2838 - val_acc: 0.0161\n",
            "Epoch 473/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.5939 - acc: 0.2419 - val_loss: 25.3035 - val_acc: 0.0645\n",
            "Epoch 474/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.9638 - acc: 0.2298 - val_loss: 23.3231 - val_acc: 0.0161\n",
            "Epoch 475/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.1232 - acc: 0.2056 - val_loss: 21.2588 - val_acc: 0.0161\n",
            "Epoch 476/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.2463 - acc: 0.1935 - val_loss: 21.3201 - val_acc: 0.0484\n",
            "Epoch 477/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.5326 - acc: 0.2863 - val_loss: 23.4919 - val_acc: 0.0323\n",
            "Epoch 478/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.6557 - acc: 0.2742 - val_loss: 22.7507 - val_acc: 0.0161\n",
            "Epoch 479/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.8747 - acc: 0.2782 - val_loss: 22.3911 - val_acc: 0.0323\n",
            "Epoch 480/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.7289 - acc: 0.2702 - val_loss: 23.2440 - val_acc: 0.0000e+00\n",
            "Epoch 481/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4142 - acc: 0.2823 - val_loss: 22.8965 - val_acc: 0.0323\n",
            "Epoch 482/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.2311 - acc: 0.1855 - val_loss: 23.0425 - val_acc: 0.0484\n",
            "Epoch 483/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.4601 - acc: 0.2581 - val_loss: 23.5967 - val_acc: 0.0161\n",
            "Epoch 484/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.7606 - acc: 0.2460 - val_loss: 23.3738 - val_acc: 0.0161\n",
            "Epoch 485/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.6135 - acc: 0.2339 - val_loss: 26.7072 - val_acc: 0.0161\n",
            "Epoch 486/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.8515 - acc: 0.2298 - val_loss: 22.2840 - val_acc: 0.0484\n",
            "Epoch 487/1000\n",
            "248/248 [==============================] - 0s 83us/step - loss: 1.5258 - acc: 0.2540 - val_loss: 21.5331 - val_acc: 0.0484\n",
            "Epoch 488/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4596 - acc: 0.2661 - val_loss: 21.0340 - val_acc: 0.0161\n",
            "Epoch 489/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.8529 - acc: 0.2460 - val_loss: 21.9215 - val_acc: 0.0161\n",
            "Epoch 490/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.4723 - acc: 0.2823 - val_loss: 23.3283 - val_acc: 0.0323\n",
            "Epoch 491/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3954 - acc: 0.2742 - val_loss: 24.3660 - val_acc: 0.0323\n",
            "Epoch 492/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.6206 - acc: 0.2581 - val_loss: 20.2302 - val_acc: 0.0323\n",
            "Epoch 493/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.6653 - acc: 0.2298 - val_loss: 25.9432 - val_acc: 0.0645\n",
            "Epoch 494/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6779 - acc: 0.2540 - val_loss: 21.4057 - val_acc: 0.0806\n",
            "Epoch 495/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.1108 - acc: 0.2177 - val_loss: 22.2128 - val_acc: 0.0161\n",
            "Epoch 496/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.4150 - acc: 0.2984 - val_loss: 22.6240 - val_acc: 0.0323\n",
            "Epoch 497/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.4052 - acc: 0.2782 - val_loss: 22.8765 - val_acc: 0.0323\n",
            "Epoch 498/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.7836 - acc: 0.2177 - val_loss: 25.2195 - val_acc: 0.0323\n",
            "Epoch 499/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.0935 - acc: 0.2097 - val_loss: 23.9376 - val_acc: 0.0161\n",
            "Epoch 500/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.7614 - acc: 0.2258 - val_loss: 22.4129 - val_acc: 0.0484\n",
            "Epoch 501/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3956 - acc: 0.2944 - val_loss: 22.1097 - val_acc: 0.0323\n",
            "Epoch 502/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4100 - acc: 0.2581 - val_loss: 23.8635 - val_acc: 0.0161\n",
            "Epoch 503/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3374 - acc: 0.2742 - val_loss: 23.2313 - val_acc: 0.0484\n",
            "Epoch 504/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.4676 - acc: 0.2823 - val_loss: 24.6734 - val_acc: 0.0484\n",
            "Epoch 505/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.6342 - acc: 0.2339 - val_loss: 22.0888 - val_acc: 0.0161\n",
            "Epoch 506/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3527 - acc: 0.2903 - val_loss: 24.4208 - val_acc: 0.0484\n",
            "Epoch 507/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.5778 - acc: 0.2218 - val_loss: 22.0946 - val_acc: 0.0323\n",
            "Epoch 508/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6331 - acc: 0.2863 - val_loss: 24.8329 - val_acc: 0.0484\n",
            "Epoch 509/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.6982 - acc: 0.2702 - val_loss: 23.6384 - val_acc: 0.0161\n",
            "Epoch 510/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.8445 - acc: 0.2137 - val_loss: 22.7234 - val_acc: 0.0484\n",
            "Epoch 511/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.8889 - acc: 0.2258 - val_loss: 23.6879 - val_acc: 0.0484\n",
            "Epoch 512/1000\n",
            "248/248 [==============================] - 0s 111us/step - loss: 1.6019 - acc: 0.2339 - val_loss: 20.7346 - val_acc: 0.0323\n",
            "Epoch 513/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4992 - acc: 0.2379 - val_loss: 23.5534 - val_acc: 0.0806\n",
            "Epoch 514/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.5924 - acc: 0.2500 - val_loss: 22.2902 - val_acc: 0.0645\n",
            "Epoch 515/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.5359 - acc: 0.2782 - val_loss: 24.8946 - val_acc: 0.0645\n",
            "Epoch 516/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.7198 - acc: 0.2298 - val_loss: 23.1576 - val_acc: 0.0806\n",
            "Epoch 517/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.4754 - acc: 0.2823 - val_loss: 22.7706 - val_acc: 0.0645\n",
            "Epoch 518/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3887 - acc: 0.2742 - val_loss: 21.6252 - val_acc: 0.0161\n",
            "Epoch 519/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.5777 - acc: 0.2621 - val_loss: 22.3899 - val_acc: 0.0323\n",
            "Epoch 520/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.5004 - acc: 0.2500 - val_loss: 22.8921 - val_acc: 0.0323\n",
            "Epoch 521/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.5684 - acc: 0.2581 - val_loss: 21.7674 - val_acc: 0.0323\n",
            "Epoch 522/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.5236 - acc: 0.2702 - val_loss: 22.7639 - val_acc: 0.0000e+00\n",
            "Epoch 523/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.7261 - acc: 0.2581 - val_loss: 21.8138 - val_acc: 0.0323\n",
            "Epoch 524/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.4343 - acc: 0.2621 - val_loss: 23.2838 - val_acc: 0.0323\n",
            "Epoch 525/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.5116 - acc: 0.2661 - val_loss: 23.2834 - val_acc: 0.0806\n",
            "Epoch 526/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2869 - acc: 0.1855 - val_loss: 21.5025 - val_acc: 0.0806\n",
            "Epoch 527/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.4157 - acc: 0.2823 - val_loss: 23.8604 - val_acc: 0.0484\n",
            "Epoch 528/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6200 - acc: 0.2540 - val_loss: 22.8672 - val_acc: 0.0323\n",
            "Epoch 529/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.3944 - acc: 0.2984 - val_loss: 22.7064 - val_acc: 0.0323\n",
            "Epoch 530/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.4719 - acc: 0.2581 - val_loss: 21.9318 - val_acc: 0.0323\n",
            "Epoch 531/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.4562 - acc: 0.2661 - val_loss: 21.6018 - val_acc: 0.0484\n",
            "Epoch 532/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3583 - acc: 0.2621 - val_loss: 22.6703 - val_acc: 0.0323\n",
            "Epoch 533/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4789 - acc: 0.2823 - val_loss: 22.3878 - val_acc: 0.0323\n",
            "Epoch 534/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2543 - acc: 0.3226 - val_loss: 22.4536 - val_acc: 0.0323\n",
            "Epoch 535/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.4332 - acc: 0.2419 - val_loss: 26.8492 - val_acc: 0.0484\n",
            "Epoch 536/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.7143 - acc: 0.2500 - val_loss: 23.2222 - val_acc: 0.0323\n",
            "Epoch 537/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4941 - acc: 0.2298 - val_loss: 23.7425 - val_acc: 0.0323\n",
            "Epoch 538/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3138 - acc: 0.3105 - val_loss: 21.9409 - val_acc: 0.0323\n",
            "Epoch 539/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2656 - acc: 0.2823 - val_loss: 21.7906 - val_acc: 0.0161\n",
            "Epoch 540/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3949 - acc: 0.2944 - val_loss: 22.4660 - val_acc: 0.0806\n",
            "Epoch 541/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4571 - acc: 0.2298 - val_loss: 22.1755 - val_acc: 0.0323\n",
            "Epoch 542/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3156 - acc: 0.2742 - val_loss: 23.2993 - val_acc: 0.0806\n",
            "Epoch 543/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.7345 - acc: 0.2137 - val_loss: 22.9240 - val_acc: 0.0484\n",
            "Epoch 544/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.3380 - acc: 0.3105 - val_loss: 22.1998 - val_acc: 0.0484\n",
            "Epoch 545/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.5861 - acc: 0.2379 - val_loss: 23.4247 - val_acc: 0.0161\n",
            "Epoch 546/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3946 - acc: 0.2903 - val_loss: 24.5463 - val_acc: 0.0645\n",
            "Epoch 547/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.2763 - acc: 0.3185 - val_loss: 24.0434 - val_acc: 0.0645\n",
            "Epoch 548/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.4095 - acc: 0.2742 - val_loss: 24.5174 - val_acc: 0.0323\n",
            "Epoch 549/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.0312 - acc: 0.2016 - val_loss: 23.8862 - val_acc: 0.0161\n",
            "Epoch 550/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.3522 - acc: 0.2621 - val_loss: 22.1886 - val_acc: 0.0161\n",
            "Epoch 551/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.7827 - acc: 0.2218 - val_loss: 26.3327 - val_acc: 0.0484\n",
            "Epoch 552/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.6982 - acc: 0.2460 - val_loss: 22.4817 - val_acc: 0.0323\n",
            "Epoch 553/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3548 - acc: 0.2702 - val_loss: 21.8907 - val_acc: 0.0323\n",
            "Epoch 554/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4081 - acc: 0.2863 - val_loss: 22.9009 - val_acc: 0.0323\n",
            "Epoch 555/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2937 - acc: 0.2782 - val_loss: 23.4756 - val_acc: 0.0323\n",
            "Epoch 556/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.6441 - acc: 0.2742 - val_loss: 25.6030 - val_acc: 0.0161\n",
            "Epoch 557/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3542 - acc: 0.2500 - val_loss: 22.5861 - val_acc: 0.0323\n",
            "Epoch 558/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.6863 - acc: 0.2621 - val_loss: 20.5919 - val_acc: 0.0323\n",
            "Epoch 559/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6098 - acc: 0.2258 - val_loss: 20.7455 - val_acc: 0.0484\n",
            "Epoch 560/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.3102 - acc: 0.2581 - val_loss: 22.5402 - val_acc: 0.0484\n",
            "Epoch 561/1000\n",
            "248/248 [==============================] - 0s 89us/step - loss: 1.3254 - acc: 0.2742 - val_loss: 24.1150 - val_acc: 0.0484\n",
            "Epoch 562/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4024 - acc: 0.3065 - val_loss: 21.1501 - val_acc: 0.0161\n",
            "Epoch 563/1000\n",
            "248/248 [==============================] - 0s 96us/step - loss: 1.3824 - acc: 0.2379 - val_loss: 21.6193 - val_acc: 0.0323\n",
            "Epoch 564/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.3128 - acc: 0.2702 - val_loss: 22.7320 - val_acc: 0.0000e+00\n",
            "Epoch 565/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.5208 - acc: 0.2702 - val_loss: 22.0168 - val_acc: 0.0484\n",
            "Epoch 566/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4081 - acc: 0.2782 - val_loss: 21.9530 - val_acc: 0.0484\n",
            "Epoch 567/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4347 - acc: 0.2379 - val_loss: 22.9582 - val_acc: 0.0000e+00\n",
            "Epoch 568/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.9483 - acc: 0.2218 - val_loss: 23.1148 - val_acc: 0.0323\n",
            "Epoch 569/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.4730 - acc: 0.2661 - val_loss: 24.8251 - val_acc: 0.0484\n",
            "Epoch 570/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.5916 - acc: 0.2823 - val_loss: 23.9596 - val_acc: 0.0161\n",
            "Epoch 571/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4848 - acc: 0.2581 - val_loss: 27.6740 - val_acc: 0.0000e+00\n",
            "Epoch 572/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.4471 - acc: 0.2661 - val_loss: 24.4434 - val_acc: 0.0645\n",
            "Epoch 573/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6258 - acc: 0.2581 - val_loss: 23.3459 - val_acc: 0.0484\n",
            "Epoch 574/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.5418 - acc: 0.2742 - val_loss: 24.6324 - val_acc: 0.0806\n",
            "Epoch 575/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.0575 - acc: 0.1653 - val_loss: 22.4121 - val_acc: 0.0484\n",
            "Epoch 576/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.3220 - acc: 0.2823 - val_loss: 22.2034 - val_acc: 0.0484\n",
            "Epoch 577/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.5536 - acc: 0.2702 - val_loss: 23.9020 - val_acc: 0.0484\n",
            "Epoch 578/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2733 - acc: 0.3065 - val_loss: 23.2979 - val_acc: 0.0000e+00\n",
            "Epoch 579/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.3646 - acc: 0.2782 - val_loss: 23.2066 - val_acc: 0.0161\n",
            "Epoch 580/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3830 - acc: 0.2581 - val_loss: 24.3195 - val_acc: 0.0161\n",
            "Epoch 581/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.2434 - acc: 0.2702 - val_loss: 23.4745 - val_acc: 0.0323\n",
            "Epoch 582/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4416 - acc: 0.3185 - val_loss: 23.6058 - val_acc: 0.0161\n",
            "Epoch 583/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3134 - acc: 0.2540 - val_loss: 24.6427 - val_acc: 0.0161\n",
            "Epoch 584/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.4622 - acc: 0.2823 - val_loss: 23.3050 - val_acc: 0.0484\n",
            "Epoch 585/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4311 - acc: 0.2540 - val_loss: 21.6759 - val_acc: 0.0323\n",
            "Epoch 586/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 1.5802 - acc: 0.2661 - val_loss: 21.3168 - val_acc: 0.0161\n",
            "Epoch 587/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2387 - acc: 0.2823 - val_loss: 23.9680 - val_acc: 0.0161\n",
            "Epoch 588/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.2447 - acc: 0.2500 - val_loss: 23.1316 - val_acc: 0.0484\n",
            "Epoch 589/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.2800 - acc: 0.3105 - val_loss: 24.0549 - val_acc: 0.0645\n",
            "Epoch 590/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2844 - acc: 0.2782 - val_loss: 22.5621 - val_acc: 0.0323\n",
            "Epoch 591/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.2126 - acc: 0.2903 - val_loss: 23.1324 - val_acc: 0.0323\n",
            "Epoch 592/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.3716 - acc: 0.2863 - val_loss: 23.5283 - val_acc: 0.0161\n",
            "Epoch 593/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.9304 - acc: 0.2258 - val_loss: 23.7185 - val_acc: 0.0484\n",
            "Epoch 594/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.3027 - acc: 0.2581 - val_loss: 21.0807 - val_acc: 0.0484\n",
            "Epoch 595/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4296 - acc: 0.2621 - val_loss: 22.5402 - val_acc: 0.0161\n",
            "Epoch 596/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3071 - acc: 0.2863 - val_loss: 21.6018 - val_acc: 0.0484\n",
            "Epoch 597/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.5019 - acc: 0.3024 - val_loss: 23.4664 - val_acc: 0.0806\n",
            "Epoch 598/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.3468 - acc: 0.2702 - val_loss: 22.7698 - val_acc: 0.0645\n",
            "Epoch 599/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.7629 - acc: 0.2339 - val_loss: 22.8393 - val_acc: 0.0323\n",
            "Epoch 600/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.7663 - acc: 0.2419 - val_loss: 24.5799 - val_acc: 0.0161\n",
            "Epoch 601/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.6121 - acc: 0.2379 - val_loss: 21.5307 - val_acc: 0.0484\n",
            "Epoch 602/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3212 - acc: 0.2903 - val_loss: 23.1223 - val_acc: 0.0000e+00\n",
            "Epoch 603/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.3179 - acc: 0.2863 - val_loss: 22.8799 - val_acc: 0.0645\n",
            "Epoch 604/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.5067 - acc: 0.2177 - val_loss: 23.4482 - val_acc: 0.0161\n",
            "Epoch 605/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3149 - acc: 0.2661 - val_loss: 22.0407 - val_acc: 0.0161\n",
            "Epoch 606/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4713 - acc: 0.2218 - val_loss: 23.1521 - val_acc: 0.0161\n",
            "Epoch 607/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4147 - acc: 0.2540 - val_loss: 21.9525 - val_acc: 0.0323\n",
            "Epoch 608/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2059 - acc: 0.2944 - val_loss: 23.1803 - val_acc: 0.0161\n",
            "Epoch 609/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.2667 - acc: 0.2702 - val_loss: 21.6780 - val_acc: 0.0484\n",
            "Epoch 610/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1956 - acc: 0.2823 - val_loss: 26.2254 - val_acc: 0.0806\n",
            "Epoch 611/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.6375 - acc: 0.2419 - val_loss: 22.9487 - val_acc: 0.0323\n",
            "Epoch 612/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3767 - acc: 0.2258 - val_loss: 23.9068 - val_acc: 0.0161\n",
            "Epoch 613/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.2527 - acc: 0.3024 - val_loss: 20.9298 - val_acc: 0.0645\n",
            "Epoch 614/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.3976 - acc: 0.2419 - val_loss: 22.0116 - val_acc: 0.0484\n",
            "Epoch 615/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.5458 - acc: 0.2702 - val_loss: 22.6691 - val_acc: 0.0161\n",
            "Epoch 616/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2442 - acc: 0.2984 - val_loss: 22.6277 - val_acc: 0.0484\n",
            "Epoch 617/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.0520 - acc: 0.2177 - val_loss: 21.9463 - val_acc: 0.0161\n",
            "Epoch 618/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6890 - acc: 0.1855 - val_loss: 24.0568 - val_acc: 0.0484\n",
            "Epoch 619/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.3904 - acc: 0.2460 - val_loss: 22.9778 - val_acc: 0.0645\n",
            "Epoch 620/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.7324 - acc: 0.2540 - val_loss: 23.2449 - val_acc: 0.0161\n",
            "Epoch 621/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1786 - acc: 0.2661 - val_loss: 24.5630 - val_acc: 0.0161\n",
            "Epoch 622/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1657 - acc: 0.2661 - val_loss: 22.5025 - val_acc: 0.0323\n",
            "Epoch 623/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.2463 - acc: 0.2984 - val_loss: 25.0510 - val_acc: 0.0161\n",
            "Epoch 624/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3475 - acc: 0.2621 - val_loss: 22.7489 - val_acc: 0.0000e+00\n",
            "Epoch 625/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.9076 - acc: 0.1895 - val_loss: 24.4041 - val_acc: 0.0645\n",
            "Epoch 626/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.4036 - acc: 0.2379 - val_loss: 23.6027 - val_acc: 0.0484\n",
            "Epoch 627/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.5086 - acc: 0.2258 - val_loss: 21.9678 - val_acc: 0.0000e+00\n",
            "Epoch 628/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4430 - acc: 0.2460 - val_loss: 22.7113 - val_acc: 0.0161\n",
            "Epoch 629/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.4444 - acc: 0.2581 - val_loss: 22.6282 - val_acc: 0.0000e+00\n",
            "Epoch 630/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3363 - acc: 0.2823 - val_loss: 24.7430 - val_acc: 0.0161\n",
            "Epoch 631/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2570 - acc: 0.2903 - val_loss: 21.6560 - val_acc: 0.0000e+00\n",
            "Epoch 632/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6002 - acc: 0.2621 - val_loss: 21.8646 - val_acc: 0.0323\n",
            "Epoch 633/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1838 - acc: 0.2702 - val_loss: 23.8835 - val_acc: 0.0484\n",
            "Epoch 634/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.2998 - acc: 0.3105 - val_loss: 22.7834 - val_acc: 0.0000e+00\n",
            "Epoch 635/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.2424 - acc: 0.2863 - val_loss: 24.1475 - val_acc: 0.0161\n",
            "Epoch 636/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3669 - acc: 0.2823 - val_loss: 24.1223 - val_acc: 0.0645\n",
            "Epoch 637/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.3982 - acc: 0.2782 - val_loss: 23.4652 - val_acc: 0.0161\n",
            "Epoch 638/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.2139 - acc: 0.2944 - val_loss: 23.9446 - val_acc: 0.0161\n",
            "Epoch 639/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2064 - acc: 0.2661 - val_loss: 24.0275 - val_acc: 0.0323\n",
            "Epoch 640/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 1.3543 - acc: 0.2702 - val_loss: 24.1241 - val_acc: 0.0161\n",
            "Epoch 641/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3660 - acc: 0.2984 - val_loss: 21.9271 - val_acc: 0.0161\n",
            "Epoch 642/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.5430 - acc: 0.2218 - val_loss: 22.9158 - val_acc: 0.0484\n",
            "Epoch 643/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.5471 - acc: 0.2863 - val_loss: 25.3042 - val_acc: 0.0645\n",
            "Epoch 644/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6139 - acc: 0.2339 - val_loss: 22.6204 - val_acc: 0.0161\n",
            "Epoch 645/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.3163 - acc: 0.2460 - val_loss: 22.3040 - val_acc: 0.0000e+00\n",
            "Epoch 646/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2445 - acc: 0.2984 - val_loss: 24.2897 - val_acc: 0.0645\n",
            "Epoch 647/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3194 - acc: 0.2621 - val_loss: 23.4187 - val_acc: 0.0000e+00\n",
            "Epoch 648/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.3205 - acc: 0.2742 - val_loss: 22.7927 - val_acc: 0.0161\n",
            "Epoch 649/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.2599 - acc: 0.2702 - val_loss: 24.3588 - val_acc: 0.0161\n",
            "Epoch 650/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.4568 - acc: 0.2419 - val_loss: 23.2844 - val_acc: 0.0000e+00\n",
            "Epoch 651/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.5873 - acc: 0.2540 - val_loss: 22.5764 - val_acc: 0.0161\n",
            "Epoch 652/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.5233 - acc: 0.2863 - val_loss: 23.2249 - val_acc: 0.0323\n",
            "Epoch 653/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.1295 - acc: 0.2984 - val_loss: 23.3831 - val_acc: 0.0161\n",
            "Epoch 654/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.1947 - acc: 0.2944 - val_loss: 24.9999 - val_acc: 0.0645\n",
            "Epoch 655/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3681 - acc: 0.2661 - val_loss: 23.2026 - val_acc: 0.0323\n",
            "Epoch 656/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.2150 - acc: 0.2581 - val_loss: 22.0603 - val_acc: 0.0000e+00\n",
            "Epoch 657/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3078 - acc: 0.2742 - val_loss: 22.8379 - val_acc: 0.0161\n",
            "Epoch 658/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2635 - acc: 0.2702 - val_loss: 22.0649 - val_acc: 0.0161\n",
            "Epoch 659/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4198 - acc: 0.2581 - val_loss: 22.3611 - val_acc: 0.0161\n",
            "Epoch 660/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.4512 - acc: 0.2661 - val_loss: 22.6977 - val_acc: 0.0161\n",
            "Epoch 661/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1012 - acc: 0.3065 - val_loss: 23.7199 - val_acc: 0.0161\n",
            "Epoch 662/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.2323 - acc: 0.2903 - val_loss: 22.0667 - val_acc: 0.0000e+00\n",
            "Epoch 663/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.2522 - acc: 0.2621 - val_loss: 22.0219 - val_acc: 0.0161\n",
            "Epoch 664/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1852 - acc: 0.2944 - val_loss: 23.8885 - val_acc: 0.0323\n",
            "Epoch 665/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1925 - acc: 0.2782 - val_loss: 22.5889 - val_acc: 0.0323\n",
            "Epoch 666/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.3121 - acc: 0.2621 - val_loss: 23.1621 - val_acc: 0.0161\n",
            "Epoch 667/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3022 - acc: 0.2702 - val_loss: 23.9133 - val_acc: 0.0161\n",
            "Epoch 668/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.4250 - acc: 0.2298 - val_loss: 25.6369 - val_acc: 0.0645\n",
            "Epoch 669/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3212 - acc: 0.2823 - val_loss: 23.1999 - val_acc: 0.0323\n",
            "Epoch 670/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2133 - acc: 0.2460 - val_loss: 22.6732 - val_acc: 0.0484\n",
            "Epoch 671/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2711 - acc: 0.2460 - val_loss: 21.6626 - val_acc: 0.0323\n",
            "Epoch 672/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3755 - acc: 0.2863 - val_loss: 24.7834 - val_acc: 0.0484\n",
            "Epoch 673/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2378 - acc: 0.2742 - val_loss: 22.7408 - val_acc: 0.0161\n",
            "Epoch 674/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.2354 - acc: 0.2661 - val_loss: 24.7418 - val_acc: 0.0161\n",
            "Epoch 675/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2743 - acc: 0.2298 - val_loss: 23.2423 - val_acc: 0.0161\n",
            "Epoch 676/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.4556 - acc: 0.2823 - val_loss: 23.1676 - val_acc: 0.0161\n",
            "Epoch 677/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1920 - acc: 0.3024 - val_loss: 23.7677 - val_acc: 0.0484\n",
            "Epoch 678/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1697 - acc: 0.2984 - val_loss: 24.0147 - val_acc: 0.0323\n",
            "Epoch 679/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1328 - acc: 0.2903 - val_loss: 22.9613 - val_acc: 0.0161\n",
            "Epoch 680/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.1663 - acc: 0.2661 - val_loss: 24.0565 - val_acc: 0.0000e+00\n",
            "Epoch 681/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1107 - acc: 0.2702 - val_loss: 24.2110 - val_acc: 0.0323\n",
            "Epoch 682/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3053 - acc: 0.2742 - val_loss: 24.1225 - val_acc: 0.0323\n",
            "Epoch 683/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2473 - acc: 0.3024 - val_loss: 24.8769 - val_acc: 0.0806\n",
            "Epoch 684/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.5148 - acc: 0.2823 - val_loss: 23.4052 - val_acc: 0.0161\n",
            "Epoch 685/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2233 - acc: 0.2782 - val_loss: 23.4991 - val_acc: 0.0484\n",
            "Epoch 686/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4788 - acc: 0.2661 - val_loss: 25.4033 - val_acc: 0.0161\n",
            "Epoch 687/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6394 - acc: 0.2137 - val_loss: 24.7279 - val_acc: 0.0645\n",
            "Epoch 688/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3825 - acc: 0.2621 - val_loss: 22.5128 - val_acc: 0.0161\n",
            "Epoch 689/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2354 - acc: 0.2823 - val_loss: 22.7867 - val_acc: 0.0323\n",
            "Epoch 690/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2190 - acc: 0.2702 - val_loss: 21.9461 - val_acc: 0.0323\n",
            "Epoch 691/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2808 - acc: 0.2903 - val_loss: 23.9456 - val_acc: 0.0323\n",
            "Epoch 692/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2502 - acc: 0.2621 - val_loss: 23.2668 - val_acc: 0.0323\n",
            "Epoch 693/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.3557 - acc: 0.2661 - val_loss: 24.7696 - val_acc: 0.0323\n",
            "Epoch 694/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.5669 - acc: 0.2460 - val_loss: 22.7715 - val_acc: 0.0000e+00\n",
            "Epoch 695/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3329 - acc: 0.2540 - val_loss: 22.7015 - val_acc: 0.0161\n",
            "Epoch 696/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1898 - acc: 0.2661 - val_loss: 22.8826 - val_acc: 0.0161\n",
            "Epoch 697/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1810 - acc: 0.3024 - val_loss: 22.4507 - val_acc: 0.0000e+00\n",
            "Epoch 698/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3499 - acc: 0.2379 - val_loss: 22.8800 - val_acc: 0.0484\n",
            "Epoch 699/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 1.1490 - acc: 0.3105 - val_loss: 24.6530 - val_acc: 0.0161\n",
            "Epoch 700/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0796 - acc: 0.3024 - val_loss: 24.3296 - val_acc: 0.0484\n",
            "Epoch 701/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.2438 - acc: 0.2863 - val_loss: 23.6149 - val_acc: 0.0484\n",
            "Epoch 702/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1152 - acc: 0.3145 - val_loss: 23.4282 - val_acc: 0.0161\n",
            "Epoch 703/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.2877 - acc: 0.3024 - val_loss: 23.3132 - val_acc: 0.0323\n",
            "Epoch 704/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2910 - acc: 0.2782 - val_loss: 22.4140 - val_acc: 0.0323\n",
            "Epoch 705/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.1397 - acc: 0.3065 - val_loss: 25.1721 - val_acc: 0.0484\n",
            "Epoch 706/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1950 - acc: 0.2984 - val_loss: 24.7196 - val_acc: 0.0484\n",
            "Epoch 707/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1627 - acc: 0.3105 - val_loss: 23.4794 - val_acc: 0.0000e+00\n",
            "Epoch 708/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1213 - acc: 0.2984 - val_loss: 24.9940 - val_acc: 0.0323\n",
            "Epoch 709/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1226 - acc: 0.3105 - val_loss: 23.6495 - val_acc: 0.0323\n",
            "Epoch 710/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.8435 - acc: 0.2258 - val_loss: 22.4432 - val_acc: 0.0645\n",
            "Epoch 711/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3745 - acc: 0.2621 - val_loss: 25.2669 - val_acc: 0.0323\n",
            "Epoch 712/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1393 - acc: 0.3185 - val_loss: 26.7979 - val_acc: 0.0323\n",
            "Epoch 713/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.8874 - acc: 0.2460 - val_loss: 24.8791 - val_acc: 0.0000e+00\n",
            "Epoch 714/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.7541 - acc: 0.2339 - val_loss: 24.9171 - val_acc: 0.0000e+00\n",
            "Epoch 715/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2730 - acc: 0.2782 - val_loss: 23.3493 - val_acc: 0.0484\n",
            "Epoch 716/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2996 - acc: 0.2903 - val_loss: 24.5386 - val_acc: 0.0161\n",
            "Epoch 717/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2494 - acc: 0.2500 - val_loss: 24.2132 - val_acc: 0.0323\n",
            "Epoch 718/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4635 - acc: 0.2379 - val_loss: 22.4548 - val_acc: 0.0161\n",
            "Epoch 719/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0879 - acc: 0.3105 - val_loss: 23.2899 - val_acc: 0.0323\n",
            "Epoch 720/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0147 - acc: 0.3065 - val_loss: 22.9375 - val_acc: 0.0323\n",
            "Epoch 721/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.7720 - acc: 0.2298 - val_loss: 24.1937 - val_acc: 0.0323\n",
            "Epoch 722/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.4021 - acc: 0.2379 - val_loss: 22.8668 - val_acc: 0.0161\n",
            "Epoch 723/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3377 - acc: 0.2903 - val_loss: 22.4867 - val_acc: 0.0323\n",
            "Epoch 724/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2572 - acc: 0.2742 - val_loss: 23.9994 - val_acc: 0.0484\n",
            "Epoch 725/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1885 - acc: 0.3266 - val_loss: 26.0284 - val_acc: 0.0645\n",
            "Epoch 726/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1984 - acc: 0.2823 - val_loss: 23.7914 - val_acc: 0.0484\n",
            "Epoch 727/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.3246 - acc: 0.2500 - val_loss: 21.9274 - val_acc: 0.0323\n",
            "Epoch 728/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.3572 - acc: 0.2540 - val_loss: 24.7600 - val_acc: 0.0484\n",
            "Epoch 729/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1640 - acc: 0.2661 - val_loss: 24.6588 - val_acc: 0.0484\n",
            "Epoch 730/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.6878 - acc: 0.2540 - val_loss: 22.9872 - val_acc: 0.0323\n",
            "Epoch 731/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4761 - acc: 0.2621 - val_loss: 23.1465 - val_acc: 0.0323\n",
            "Epoch 732/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4322 - acc: 0.2782 - val_loss: 23.1280 - val_acc: 0.0484\n",
            "Epoch 733/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.2160 - acc: 0.2823 - val_loss: 23.4421 - val_acc: 0.0323\n",
            "Epoch 734/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.0992 - acc: 0.2742 - val_loss: 23.8562 - val_acc: 0.0161\n",
            "Epoch 735/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0240 - acc: 0.3306 - val_loss: 21.8743 - val_acc: 0.0484\n",
            "Epoch 736/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.4136 - acc: 0.2540 - val_loss: 22.6144 - val_acc: 0.0323\n",
            "Epoch 737/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3164 - acc: 0.2823 - val_loss: 24.8270 - val_acc: 0.0000e+00\n",
            "Epoch 738/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.4859 - acc: 0.2782 - val_loss: 22.1949 - val_acc: 0.0484\n",
            "Epoch 739/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0398 - acc: 0.3387 - val_loss: 22.3715 - val_acc: 0.0484\n",
            "Epoch 740/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.5359 - acc: 0.2258 - val_loss: 24.0362 - val_acc: 0.0161\n",
            "Epoch 741/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1556 - acc: 0.2782 - val_loss: 24.2875 - val_acc: 0.0161\n",
            "Epoch 742/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1116 - acc: 0.2863 - val_loss: 23.9710 - val_acc: 0.0484\n",
            "Epoch 743/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0721 - acc: 0.2823 - val_loss: 23.7564 - val_acc: 0.0161\n",
            "Epoch 744/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1649 - acc: 0.2944 - val_loss: 24.4069 - val_acc: 0.0645\n",
            "Epoch 745/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.3212 - acc: 0.2661 - val_loss: 23.5712 - val_acc: 0.0161\n",
            "Epoch 746/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0684 - acc: 0.2621 - val_loss: 23.8838 - val_acc: 0.0161\n",
            "Epoch 747/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2253 - acc: 0.2944 - val_loss: 28.1588 - val_acc: 0.0323\n",
            "Epoch 748/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3598 - acc: 0.3266 - val_loss: 24.4670 - val_acc: 0.0484\n",
            "Epoch 749/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.0447 - acc: 0.2742 - val_loss: 23.4762 - val_acc: 0.0323\n",
            "Epoch 750/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0593 - acc: 0.3387 - val_loss: 23.6886 - val_acc: 0.0484\n",
            "Epoch 751/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1974 - acc: 0.3185 - val_loss: 24.4503 - val_acc: 0.0161\n",
            "Epoch 752/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0654 - acc: 0.2863 - val_loss: 25.1280 - val_acc: 0.0484\n",
            "Epoch 753/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1895 - acc: 0.2621 - val_loss: 24.7317 - val_acc: 0.0323\n",
            "Epoch 754/1000\n",
            "248/248 [==============================] - 0s 79us/step - loss: 1.0357 - acc: 0.3185 - val_loss: 23.4479 - val_acc: 0.0484\n",
            "Epoch 755/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1506 - acc: 0.2661 - val_loss: 23.7343 - val_acc: 0.0323\n",
            "Epoch 756/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.0699 - acc: 0.3105 - val_loss: 23.7376 - val_acc: 0.0161\n",
            "Epoch 757/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0560 - acc: 0.3105 - val_loss: 25.1042 - val_acc: 0.0323\n",
            "Epoch 758/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2797 - acc: 0.2944 - val_loss: 25.3053 - val_acc: 0.0161\n",
            "Epoch 759/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4918 - acc: 0.2419 - val_loss: 23.0060 - val_acc: 0.0161\n",
            "Epoch 760/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 1.3726 - acc: 0.2742 - val_loss: 21.8671 - val_acc: 0.0161\n",
            "Epoch 761/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9975 - acc: 0.3065 - val_loss: 23.0579 - val_acc: 0.0484\n",
            "Epoch 762/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0930 - acc: 0.3145 - val_loss: 24.3268 - val_acc: 0.0645\n",
            "Epoch 763/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.2266 - acc: 0.2823 - val_loss: 23.8174 - val_acc: 0.0323\n",
            "Epoch 764/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3637 - acc: 0.2863 - val_loss: 25.1986 - val_acc: 0.0645\n",
            "Epoch 765/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1597 - acc: 0.2863 - val_loss: 24.2190 - val_acc: 0.0323\n",
            "Epoch 766/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0519 - acc: 0.2702 - val_loss: 24.0124 - val_acc: 0.0323\n",
            "Epoch 767/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1234 - acc: 0.3468 - val_loss: 24.5666 - val_acc: 0.0323\n",
            "Epoch 768/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.9648 - acc: 0.3065 - val_loss: 24.6988 - val_acc: 0.0161\n",
            "Epoch 769/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0132 - acc: 0.2984 - val_loss: 24.5824 - val_acc: 0.0161\n",
            "Epoch 770/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1141 - acc: 0.2460 - val_loss: 22.6532 - val_acc: 0.0161\n",
            "Epoch 771/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2610 - acc: 0.2984 - val_loss: 22.3476 - val_acc: 0.0484\n",
            "Epoch 772/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1066 - acc: 0.2621 - val_loss: 23.1672 - val_acc: 0.0323\n",
            "Epoch 773/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.7263 - acc: 0.3065 - val_loss: 23.9240 - val_acc: 0.0323\n",
            "Epoch 774/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0527 - acc: 0.2782 - val_loss: 23.5879 - val_acc: 0.0161\n",
            "Epoch 775/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0024 - acc: 0.3387 - val_loss: 25.8530 - val_acc: 0.0000e+00\n",
            "Epoch 776/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 0.9640 - acc: 0.3185 - val_loss: 24.0201 - val_acc: 0.0323\n",
            "Epoch 777/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0931 - acc: 0.2621 - val_loss: 26.6852 - val_acc: 0.0484\n",
            "Epoch 778/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0898 - acc: 0.2581 - val_loss: 23.8229 - val_acc: 0.0161\n",
            "Epoch 779/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2975 - acc: 0.2903 - val_loss: 23.9114 - val_acc: 0.0484\n",
            "Epoch 780/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0416 - acc: 0.3185 - val_loss: 25.8389 - val_acc: 0.0806\n",
            "Epoch 781/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0320 - acc: 0.3145 - val_loss: 23.6975 - val_acc: 0.0161\n",
            "Epoch 782/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9938 - acc: 0.2944 - val_loss: 24.3369 - val_acc: 0.0323\n",
            "Epoch 783/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0963 - acc: 0.3024 - val_loss: 27.7428 - val_acc: 0.0161\n",
            "Epoch 784/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 8.3485 - acc: 0.1774 - val_loss: 37.8560 - val_acc: 0.0645\n",
            "Epoch 785/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.4137 - acc: 0.2540 - val_loss: 26.3728 - val_acc: 0.0161\n",
            "Epoch 786/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4122 - acc: 0.2581 - val_loss: 26.0367 - val_acc: 0.0161\n",
            "Epoch 787/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.2806 - acc: 0.2823 - val_loss: 28.8987 - val_acc: 0.0645\n",
            "Epoch 788/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.8060 - acc: 0.2702 - val_loss: 28.2912 - val_acc: 0.0000e+00\n",
            "Epoch 789/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.3442 - acc: 0.2661 - val_loss: 22.9925 - val_acc: 0.0161\n",
            "Epoch 790/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1107 - acc: 0.2782 - val_loss: 24.0311 - val_acc: 0.0161\n",
            "Epoch 791/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1662 - acc: 0.2339 - val_loss: 25.7009 - val_acc: 0.0161\n",
            "Epoch 792/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1699 - acc: 0.2661 - val_loss: 26.6190 - val_acc: 0.0161\n",
            "Epoch 793/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3043 - acc: 0.2016 - val_loss: 24.4893 - val_acc: 0.0645\n",
            "Epoch 794/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1613 - acc: 0.3065 - val_loss: 24.1986 - val_acc: 0.0000e+00\n",
            "Epoch 795/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0693 - acc: 0.2581 - val_loss: 24.9772 - val_acc: 0.0000e+00\n",
            "Epoch 796/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2515 - acc: 0.2903 - val_loss: 23.0280 - val_acc: 0.0000e+00\n",
            "Epoch 797/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2933 - acc: 0.2702 - val_loss: 23.5701 - val_acc: 0.0161\n",
            "Epoch 798/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1016 - acc: 0.3387 - val_loss: 24.5140 - val_acc: 0.0161\n",
            "Epoch 799/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0592 - acc: 0.2823 - val_loss: 23.7447 - val_acc: 0.0323\n",
            "Epoch 800/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.0805 - acc: 0.2984 - val_loss: 25.4182 - val_acc: 0.0645\n",
            "Epoch 801/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.0943 - acc: 0.2984 - val_loss: 23.3197 - val_acc: 0.0000e+00\n",
            "Epoch 802/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9471 - acc: 0.2863 - val_loss: 24.4294 - val_acc: 0.0000e+00\n",
            "Epoch 803/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 1.1247 - acc: 0.2984 - val_loss: 25.1538 - val_acc: 0.0000e+00\n",
            "Epoch 804/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0131 - acc: 0.2984 - val_loss: 27.1968 - val_acc: 0.0484\n",
            "Epoch 805/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1709 - acc: 0.3024 - val_loss: 23.9734 - val_acc: 0.0161\n",
            "Epoch 806/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1756 - acc: 0.2984 - val_loss: 25.4194 - val_acc: 0.0000e+00\n",
            "Epoch 807/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9912 - acc: 0.2984 - val_loss: 24.9946 - val_acc: 0.0161\n",
            "Epoch 808/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9127 - acc: 0.3145 - val_loss: 24.4287 - val_acc: 0.0161\n",
            "Epoch 809/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9397 - acc: 0.2863 - val_loss: 25.4837 - val_acc: 0.0323\n",
            "Epoch 810/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0883 - acc: 0.2782 - val_loss: 25.0423 - val_acc: 0.0323\n",
            "Epoch 811/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.2706 - acc: 0.3226 - val_loss: 23.6376 - val_acc: 0.0161\n",
            "Epoch 812/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0408 - acc: 0.2863 - val_loss: 26.7274 - val_acc: 0.0484\n",
            "Epoch 813/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2294 - acc: 0.2984 - val_loss: 24.4504 - val_acc: 0.0000e+00\n",
            "Epoch 814/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0694 - acc: 0.3024 - val_loss: 24.5096 - val_acc: 0.0323\n",
            "Epoch 815/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0424 - acc: 0.3427 - val_loss: 24.5579 - val_acc: 0.0484\n",
            "Epoch 816/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.4262 - acc: 0.2540 - val_loss: 22.5735 - val_acc: 0.0323\n",
            "Epoch 817/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3218 - acc: 0.2702 - val_loss: 26.1559 - val_acc: 0.0806\n",
            "Epoch 818/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9430 - acc: 0.3185 - val_loss: 26.2793 - val_acc: 0.0484\n",
            "Epoch 819/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9931 - acc: 0.3105 - val_loss: 24.7177 - val_acc: 0.0323\n",
            "Epoch 820/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9696 - acc: 0.3145 - val_loss: 25.1499 - val_acc: 0.0161\n",
            "Epoch 821/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.5975 - acc: 0.2460 - val_loss: 24.5135 - val_acc: 0.0161\n",
            "Epoch 822/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9133 - acc: 0.3427 - val_loss: 24.4746 - val_acc: 0.0323\n",
            "Epoch 823/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9767 - acc: 0.2944 - val_loss: 24.5194 - val_acc: 0.0161\n",
            "Epoch 824/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0863 - acc: 0.2742 - val_loss: 23.9503 - val_acc: 0.0484\n",
            "Epoch 825/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9843 - acc: 0.2823 - val_loss: 25.6327 - val_acc: 0.0323\n",
            "Epoch 826/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1783 - acc: 0.2823 - val_loss: 26.8030 - val_acc: 0.0000e+00\n",
            "Epoch 827/1000\n",
            "248/248 [==============================] - 0s 46us/step - loss: 1.3288 - acc: 0.3145 - val_loss: 26.9962 - val_acc: 0.0484\n",
            "Epoch 828/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3851 - acc: 0.2782 - val_loss: 24.8497 - val_acc: 0.0323\n",
            "Epoch 829/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9107 - acc: 0.3226 - val_loss: 26.2291 - val_acc: 0.0161\n",
            "Epoch 830/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 0.8999 - acc: 0.2863 - val_loss: 25.2049 - val_acc: 0.0161\n",
            "Epoch 831/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0654 - acc: 0.2621 - val_loss: 25.3493 - val_acc: 0.0323\n",
            "Epoch 832/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9533 - acc: 0.3226 - val_loss: 25.4328 - val_acc: 0.0484\n",
            "Epoch 833/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.0226 - acc: 0.3306 - val_loss: 25.9948 - val_acc: 0.0645\n",
            "Epoch 834/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3043 - acc: 0.2742 - val_loss: 27.3108 - val_acc: 0.0645\n",
            "Epoch 835/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 2.2045 - acc: 0.2137 - val_loss: 22.4088 - val_acc: 0.0806\n",
            "Epoch 836/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.5970 - acc: 0.2581 - val_loss: 20.8892 - val_acc: 0.0484\n",
            "Epoch 837/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9052 - acc: 0.3306 - val_loss: 23.6286 - val_acc: 0.0161\n",
            "Epoch 838/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 0.9302 - acc: 0.3145 - val_loss: 24.4428 - val_acc: 0.0161\n",
            "Epoch 839/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9241 - acc: 0.3266 - val_loss: 24.2563 - val_acc: 0.0645\n",
            "Epoch 840/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9735 - acc: 0.3347 - val_loss: 22.7922 - val_acc: 0.0161\n",
            "Epoch 841/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1993 - acc: 0.2661 - val_loss: 22.8955 - val_acc: 0.0000e+00\n",
            "Epoch 842/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1316 - acc: 0.3024 - val_loss: 23.4076 - val_acc: 0.0323\n",
            "Epoch 843/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0036 - acc: 0.3306 - val_loss: 23.8277 - val_acc: 0.0323\n",
            "Epoch 844/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9305 - acc: 0.3508 - val_loss: 24.7415 - val_acc: 0.0484\n",
            "Epoch 845/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9941 - acc: 0.3145 - val_loss: 23.4568 - val_acc: 0.0484\n",
            "Epoch 846/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6579 - acc: 0.2621 - val_loss: 25.1680 - val_acc: 0.0000e+00\n",
            "Epoch 847/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0779 - acc: 0.2782 - val_loss: 25.5577 - val_acc: 0.0161\n",
            "Epoch 848/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0978 - acc: 0.3065 - val_loss: 25.1624 - val_acc: 0.0161\n",
            "Epoch 849/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9747 - acc: 0.2903 - val_loss: 25.1878 - val_acc: 0.0161\n",
            "Epoch 850/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0367 - acc: 0.3024 - val_loss: 26.5536 - val_acc: 0.0161\n",
            "Epoch 851/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.2733 - acc: 0.2460 - val_loss: 22.2978 - val_acc: 0.0323\n",
            "Epoch 852/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9586 - acc: 0.3387 - val_loss: 25.0084 - val_acc: 0.0484\n",
            "Epoch 853/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0756 - acc: 0.2903 - val_loss: 24.8713 - val_acc: 0.0323\n",
            "Epoch 854/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9588 - acc: 0.3105 - val_loss: 24.5825 - val_acc: 0.0806\n",
            "Epoch 855/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.8990 - acc: 0.3347 - val_loss: 24.5442 - val_acc: 0.0000e+00\n",
            "Epoch 856/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.2949 - acc: 0.2702 - val_loss: 24.2116 - val_acc: 0.0806\n",
            "Epoch 857/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0193 - acc: 0.3145 - val_loss: 24.3378 - val_acc: 0.0645\n",
            "Epoch 858/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9748 - acc: 0.3024 - val_loss: 24.7630 - val_acc: 0.0484\n",
            "Epoch 859/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9581 - acc: 0.3589 - val_loss: 22.8142 - val_acc: 0.0161\n",
            "Epoch 860/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.9363 - acc: 0.3266 - val_loss: 23.6087 - val_acc: 0.0323\n",
            "Epoch 861/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8501 - acc: 0.3185 - val_loss: 23.7134 - val_acc: 0.0645\n",
            "Epoch 862/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.8741 - acc: 0.2984 - val_loss: 25.1080 - val_acc: 0.0484\n",
            "Epoch 863/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.9729 - acc: 0.3347 - val_loss: 24.9503 - val_acc: 0.0161\n",
            "Epoch 864/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9965 - acc: 0.3266 - val_loss: 22.9952 - val_acc: 0.0323\n",
            "Epoch 865/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.0580 - acc: 0.3508 - val_loss: 23.7501 - val_acc: 0.0645\n",
            "Epoch 866/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2371 - acc: 0.2581 - val_loss: 25.8628 - val_acc: 0.0323\n",
            "Epoch 867/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0386 - acc: 0.3185 - val_loss: 26.0943 - val_acc: 0.0484\n",
            "Epoch 868/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 0.9404 - acc: 0.3306 - val_loss: 23.8454 - val_acc: 0.0161\n",
            "Epoch 869/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.9852 - acc: 0.2863 - val_loss: 24.4530 - val_acc: 0.0645\n",
            "Epoch 870/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0924 - acc: 0.3145 - val_loss: 24.6900 - val_acc: 0.0484\n",
            "Epoch 871/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0579 - acc: 0.3065 - val_loss: 25.0509 - val_acc: 0.0806\n",
            "Epoch 872/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9722 - acc: 0.3024 - val_loss: 23.5844 - val_acc: 0.0484\n",
            "Epoch 873/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0578 - acc: 0.3145 - val_loss: 27.2014 - val_acc: 0.0161\n",
            "Epoch 874/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.9705 - acc: 0.2782 - val_loss: 26.6960 - val_acc: 0.0161\n",
            "Epoch 875/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0982 - acc: 0.3226 - val_loss: 22.5801 - val_acc: 0.0000e+00\n",
            "Epoch 876/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 1.0626 - acc: 0.2863 - val_loss: 24.2311 - val_acc: 0.0484\n",
            "Epoch 877/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.8893 - acc: 0.3347 - val_loss: 23.2512 - val_acc: 0.0645\n",
            "Epoch 878/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.8869 - acc: 0.3226 - val_loss: 24.7165 - val_acc: 0.0000e+00\n",
            "Epoch 879/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.0045 - acc: 0.3185 - val_loss: 26.7623 - val_acc: 0.0645\n",
            "Epoch 880/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9333 - acc: 0.3266 - val_loss: 24.8575 - val_acc: 0.0323\n",
            "Epoch 881/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0699 - acc: 0.2863 - val_loss: 26.9164 - val_acc: 0.0161\n",
            "Epoch 882/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1840 - acc: 0.2984 - val_loss: 25.2695 - val_acc: 0.0000e+00\n",
            "Epoch 883/1000\n",
            "248/248 [==============================] - 0s 106us/step - loss: 1.5831 - acc: 0.2540 - val_loss: 26.0957 - val_acc: 0.0806\n",
            "Epoch 884/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1441 - acc: 0.3145 - val_loss: 25.8501 - val_acc: 0.0000e+00\n",
            "Epoch 885/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.8990 - acc: 0.3226 - val_loss: 27.8151 - val_acc: 0.0000e+00\n",
            "Epoch 886/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4373 - acc: 0.2621 - val_loss: 24.4594 - val_acc: 0.0323\n",
            "Epoch 887/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9266 - acc: 0.3185 - val_loss: 26.0317 - val_acc: 0.0484\n",
            "Epoch 888/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1662 - acc: 0.2823 - val_loss: 24.6976 - val_acc: 0.0323\n",
            "Epoch 889/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0068 - acc: 0.3185 - val_loss: 27.0387 - val_acc: 0.0323\n",
            "Epoch 890/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1997 - acc: 0.2742 - val_loss: 22.7328 - val_acc: 0.0645\n",
            "Epoch 891/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1782 - acc: 0.2742 - val_loss: 25.3499 - val_acc: 0.0484\n",
            "Epoch 892/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 0.8912 - acc: 0.3226 - val_loss: 24.9531 - val_acc: 0.0323\n",
            "Epoch 893/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8359 - acc: 0.3387 - val_loss: 24.6418 - val_acc: 0.0323\n",
            "Epoch 894/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1859 - acc: 0.3065 - val_loss: 26.3090 - val_acc: 0.0645\n",
            "Epoch 895/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0126 - acc: 0.3105 - val_loss: 24.5644 - val_acc: 0.0484\n",
            "Epoch 896/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9290 - acc: 0.3105 - val_loss: 26.7929 - val_acc: 0.0645\n",
            "Epoch 897/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 1.3324 - acc: 0.2863 - val_loss: 23.6830 - val_acc: 0.0484\n",
            "Epoch 898/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9089 - acc: 0.3387 - val_loss: 24.7759 - val_acc: 0.0161\n",
            "Epoch 899/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.8937 - acc: 0.3589 - val_loss: 23.9877 - val_acc: 0.0484\n",
            "Epoch 900/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8310 - acc: 0.3387 - val_loss: 27.0208 - val_acc: 0.0000e+00\n",
            "Epoch 901/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3964 - acc: 0.2823 - val_loss: 23.0233 - val_acc: 0.0645\n",
            "Epoch 902/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.0223 - acc: 0.3024 - val_loss: 26.1123 - val_acc: 0.0484\n",
            "Epoch 903/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.4063 - acc: 0.2984 - val_loss: 23.0069 - val_acc: 0.0645\n",
            "Epoch 904/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0633 - acc: 0.2621 - val_loss: 24.7471 - val_acc: 0.0323\n",
            "Epoch 905/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.0788 - acc: 0.2944 - val_loss: 25.2895 - val_acc: 0.0806\n",
            "Epoch 906/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0225 - acc: 0.3427 - val_loss: 25.1539 - val_acc: 0.0000e+00\n",
            "Epoch 907/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0632 - acc: 0.2984 - val_loss: 24.4101 - val_acc: 0.0806\n",
            "Epoch 908/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 0.9479 - acc: 0.3387 - val_loss: 25.5846 - val_acc: 0.0806\n",
            "Epoch 909/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0668 - acc: 0.2984 - val_loss: 24.0753 - val_acc: 0.0645\n",
            "Epoch 910/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0824 - acc: 0.2823 - val_loss: 24.8990 - val_acc: 0.0484\n",
            "Epoch 911/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9515 - acc: 0.2984 - val_loss: 25.5261 - val_acc: 0.0645\n",
            "Epoch 912/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9586 - acc: 0.3145 - val_loss: 22.8686 - val_acc: 0.0323\n",
            "Epoch 913/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8016 - acc: 0.3790 - val_loss: 23.8217 - val_acc: 0.0484\n",
            "Epoch 914/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 0.9694 - acc: 0.3145 - val_loss: 25.5317 - val_acc: 0.0645\n",
            "Epoch 915/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.8900 - acc: 0.3427 - val_loss: 24.5592 - val_acc: 0.0806\n",
            "Epoch 916/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9368 - acc: 0.2823 - val_loss: 25.4989 - val_acc: 0.0161\n",
            "Epoch 917/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9538 - acc: 0.3266 - val_loss: 24.9592 - val_acc: 0.0484\n",
            "Epoch 918/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0880 - acc: 0.2782 - val_loss: 22.7292 - val_acc: 0.0323\n",
            "Epoch 919/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0376 - acc: 0.2984 - val_loss: 25.5255 - val_acc: 0.0645\n",
            "Epoch 920/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.8552 - acc: 0.3145 - val_loss: 24.5091 - val_acc: 0.0323\n",
            "Epoch 921/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.8954 - acc: 0.3427 - val_loss: 24.5626 - val_acc: 0.0484\n",
            "Epoch 922/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8920 - acc: 0.3024 - val_loss: 24.4208 - val_acc: 0.0161\n",
            "Epoch 923/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8784 - acc: 0.3065 - val_loss: 24.4274 - val_acc: 0.0484\n",
            "Epoch 924/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0786 - acc: 0.3387 - val_loss: 24.3435 - val_acc: 0.0323\n",
            "Epoch 925/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9657 - acc: 0.2944 - val_loss: 23.8877 - val_acc: 0.0806\n",
            "Epoch 926/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0969 - acc: 0.2782 - val_loss: 26.7439 - val_acc: 0.0484\n",
            "Epoch 927/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1158 - acc: 0.2742 - val_loss: 23.3008 - val_acc: 0.0484\n",
            "Epoch 928/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.8838 - acc: 0.3427 - val_loss: 25.7688 - val_acc: 0.0000e+00\n",
            "Epoch 929/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 0.8737 - acc: 0.3185 - val_loss: 24.2148 - val_acc: 0.0645\n",
            "Epoch 930/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1015 - acc: 0.3306 - val_loss: 25.5087 - val_acc: 0.0323\n",
            "Epoch 931/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.9433 - acc: 0.3185 - val_loss: 24.6914 - val_acc: 0.0484\n",
            "Epoch 932/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1161 - acc: 0.2984 - val_loss: 27.5140 - val_acc: 0.0806\n",
            "Epoch 933/1000\n",
            "248/248 [==============================] - 0s 86us/step - loss: 0.9838 - acc: 0.2903 - val_loss: 26.0744 - val_acc: 0.0323\n",
            "Epoch 934/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.8580 - acc: 0.3750 - val_loss: 24.0988 - val_acc: 0.0806\n",
            "Epoch 935/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0998 - acc: 0.3347 - val_loss: 25.9057 - val_acc: 0.0161\n",
            "Epoch 936/1000\n",
            "248/248 [==============================] - 0s 83us/step - loss: 1.1323 - acc: 0.3306 - val_loss: 26.3845 - val_acc: 0.0323\n",
            "Epoch 937/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0347 - acc: 0.3548 - val_loss: 26.6328 - val_acc: 0.0484\n",
            "Epoch 938/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 0.8585 - acc: 0.3710 - val_loss: 26.1646 - val_acc: 0.0323\n",
            "Epoch 939/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9847 - acc: 0.3347 - val_loss: 23.7619 - val_acc: 0.0323\n",
            "Epoch 940/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.8573 - acc: 0.3589 - val_loss: 23.5892 - val_acc: 0.0323\n",
            "Epoch 941/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0641 - acc: 0.2863 - val_loss: 24.5066 - val_acc: 0.0806\n",
            "Epoch 942/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.8743 - acc: 0.3105 - val_loss: 24.3917 - val_acc: 0.0645\n",
            "Epoch 943/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 0.8388 - acc: 0.3669 - val_loss: 24.7839 - val_acc: 0.0323\n",
            "Epoch 944/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.8706 - acc: 0.3226 - val_loss: 26.0660 - val_acc: 0.0161\n",
            "Epoch 945/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 0.8147 - acc: 0.3226 - val_loss: 24.3624 - val_acc: 0.0323\n",
            "Epoch 946/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0936 - acc: 0.3185 - val_loss: 24.2019 - val_acc: 0.0323\n",
            "Epoch 947/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 0.9335 - acc: 0.3065 - val_loss: 26.0337 - val_acc: 0.0161\n",
            "Epoch 948/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 0.7642 - acc: 0.3226 - val_loss: 26.2565 - val_acc: 0.0161\n",
            "Epoch 949/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.7993 - acc: 0.3508 - val_loss: 25.8869 - val_acc: 0.0645\n",
            "Epoch 950/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 0.8842 - acc: 0.3145 - val_loss: 24.5727 - val_acc: 0.0484\n",
            "Epoch 951/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0687 - acc: 0.3226 - val_loss: 24.6619 - val_acc: 0.0484\n",
            "Epoch 952/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.8582 - acc: 0.3427 - val_loss: 27.2310 - val_acc: 0.0484\n",
            "Epoch 953/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0730 - acc: 0.3226 - val_loss: 25.7139 - val_acc: 0.0161\n",
            "Epoch 954/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8794 - acc: 0.3105 - val_loss: 26.4586 - val_acc: 0.0645\n",
            "Epoch 955/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8619 - acc: 0.3226 - val_loss: 24.6230 - val_acc: 0.0323\n",
            "Epoch 956/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9993 - acc: 0.3145 - val_loss: 25.7106 - val_acc: 0.0323\n",
            "Epoch 957/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.8519 - acc: 0.3508 - val_loss: 24.5485 - val_acc: 0.0161\n",
            "Epoch 958/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.8747 - acc: 0.3589 - val_loss: 24.9477 - val_acc: 0.0645\n",
            "Epoch 959/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9364 - acc: 0.2984 - val_loss: 24.5543 - val_acc: 0.0484\n",
            "Epoch 960/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.8428 - acc: 0.3185 - val_loss: 24.9186 - val_acc: 0.0645\n",
            "Epoch 961/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.8691 - acc: 0.3427 - val_loss: 25.8394 - val_acc: 0.0484\n",
            "Epoch 962/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.8590 - acc: 0.2944 - val_loss: 25.8408 - val_acc: 0.0484\n",
            "Epoch 963/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1839 - acc: 0.2661 - val_loss: 24.8842 - val_acc: 0.0161\n",
            "Epoch 964/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 0.9374 - acc: 0.3508 - val_loss: 23.5511 - val_acc: 0.0645\n",
            "Epoch 965/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1496 - acc: 0.3065 - val_loss: 26.4399 - val_acc: 0.0323\n",
            "Epoch 966/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9924 - acc: 0.3347 - val_loss: 24.5844 - val_acc: 0.0645\n",
            "Epoch 967/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.8372 - acc: 0.3185 - val_loss: 23.1779 - val_acc: 0.0806\n",
            "Epoch 968/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9088 - acc: 0.3589 - val_loss: 24.3406 - val_acc: 0.0484\n",
            "Epoch 969/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3368 - acc: 0.2379 - val_loss: 25.1868 - val_acc: 0.0484\n",
            "Epoch 970/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9056 - acc: 0.3226 - val_loss: 25.0772 - val_acc: 0.0161\n",
            "Epoch 971/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2955 - acc: 0.2581 - val_loss: 26.1557 - val_acc: 0.0484\n",
            "Epoch 972/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9772 - acc: 0.3508 - val_loss: 24.5106 - val_acc: 0.0645\n",
            "Epoch 973/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0845 - acc: 0.2661 - val_loss: 24.1512 - val_acc: 0.0323\n",
            "Epoch 974/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9965 - acc: 0.2903 - val_loss: 25.3334 - val_acc: 0.0161\n",
            "Epoch 975/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8201 - acc: 0.3468 - val_loss: 26.3263 - val_acc: 0.0645\n",
            "Epoch 976/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9932 - acc: 0.3185 - val_loss: 25.1151 - val_acc: 0.0000e+00\n",
            "Epoch 977/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1965 - acc: 0.2903 - val_loss: 25.5885 - val_acc: 0.0484\n",
            "Epoch 978/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9572 - acc: 0.3226 - val_loss: 25.2392 - val_acc: 0.0484\n",
            "Epoch 979/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9206 - acc: 0.3306 - val_loss: 24.3976 - val_acc: 0.0645\n",
            "Epoch 980/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 0.8856 - acc: 0.3145 - val_loss: 26.3316 - val_acc: 0.0645\n",
            "Epoch 981/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0495 - acc: 0.2540 - val_loss: 24.0787 - val_acc: 0.0484\n",
            "Epoch 982/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.7869 - acc: 0.3710 - val_loss: 25.2550 - val_acc: 0.0484\n",
            "Epoch 983/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 0.8504 - acc: 0.3427 - val_loss: 24.1098 - val_acc: 0.0645\n",
            "Epoch 984/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0532 - acc: 0.2984 - val_loss: 23.3076 - val_acc: 0.0645\n",
            "Epoch 985/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0244 - acc: 0.3548 - val_loss: 25.0271 - val_acc: 0.0645\n",
            "Epoch 986/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.7520 - acc: 0.3508 - val_loss: 24.3142 - val_acc: 0.0323\n",
            "Epoch 987/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0242 - acc: 0.2903 - val_loss: 26.0908 - val_acc: 0.0323\n",
            "Epoch 988/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0423 - acc: 0.2702 - val_loss: 26.8921 - val_acc: 0.0000e+00\n",
            "Epoch 989/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2569 - acc: 0.3266 - val_loss: 25.7327 - val_acc: 0.0645\n",
            "Epoch 990/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0262 - acc: 0.3024 - val_loss: 24.8669 - val_acc: 0.0161\n",
            "Epoch 991/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.8647 - acc: 0.3427 - val_loss: 26.3379 - val_acc: 0.0806\n",
            "Epoch 992/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9625 - acc: 0.2903 - val_loss: 25.3053 - val_acc: 0.0484\n",
            "Epoch 993/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2546 - acc: 0.2621 - val_loss: 24.8305 - val_acc: 0.0484\n",
            "Epoch 994/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.8600 - acc: 0.3266 - val_loss: 29.3296 - val_acc: 0.0000e+00\n",
            "Epoch 995/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0386 - acc: 0.3065 - val_loss: 23.6489 - val_acc: 0.0484\n",
            "Epoch 996/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8740 - acc: 0.3589 - val_loss: 25.2266 - val_acc: 0.0645\n",
            "Epoch 997/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8348 - acc: 0.3347 - val_loss: 23.8700 - val_acc: 0.0645\n",
            "Epoch 998/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 0.8464 - acc: 0.3589 - val_loss: 24.1887 - val_acc: 0.0484\n",
            "Epoch 999/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.8968 - acc: 0.2823 - val_loss: 25.4982 - val_acc: 0.0323\n",
            "Epoch 1000/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0811 - acc: 0.3105 - val_loss: 24.0427 - val_acc: 0.0645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVfqtwxL26ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "466da091-38d5-4ffc-c1c8-fe0a22167d6e"
      },
      "source": [
        "model.evaluate(testX, testY)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82/82 [==============================] - 0s 56us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.038924984815644, 0.10975609756097561]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-QuBleZ8ChG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_graph(history):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('No. of epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('No. of epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSDwxxDD8Zxc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "76eba77d-c1c5-4af4-ca42-19cd9b94479a"
      },
      "source": [
        "show_graph(history)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
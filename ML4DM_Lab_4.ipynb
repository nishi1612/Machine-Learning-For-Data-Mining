{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML4DM_Lab_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishi1612/Machine-Learning-For-Data-Mining/blob/master/ML4DM_Lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVx4M-h9hqnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThExCsAihtUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igXAcITIiTzG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "53b2da59-0929-4fc8-85b4-751c8e70e08a"
      },
      "source": [
        "id = \"18Ml33QE8vQiOPhUCbqxGRpeSlzcOt7FA\"\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('auto-mpg.csv')  \n",
        "df = pd.read_csv('auto-mpg.csv')\n",
        "df.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>origin</th>\n",
              "      <th>car name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>chevrolet chevelle malibu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>buick skylark 320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>plymouth satellite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>amc rebel sst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>ford torino</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  ... model year  origin                   car name\n",
              "0  18.0          8         307.0  ...         70       1  chevrolet chevelle malibu\n",
              "1  15.0          8         350.0  ...         70       1          buick skylark 320\n",
              "2  18.0          8         318.0  ...         70       1         plymouth satellite\n",
              "3  16.0          8         304.0  ...         70       1              amc rebel sst\n",
              "4  17.0          8         302.0  ...         70       1                ford torino\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IRXmhXNjV83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1dbea428-b59b-47a7-d8b0-4b7a670444a5"
      },
      "source": [
        "df.index"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RangeIndex(start=0, stop=398, step=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02nmGGKbjX8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(columns = df.columns[len(df.columns)-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUW23DNJjpKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8fb149d6-c125-4e5b-d91f-21626075951c"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  ... acceleration  model year  origin\n",
              "0  18.0          8         307.0  ...         12.0          70       1\n",
              "1  15.0          8         350.0  ...         11.5          70       1\n",
              "2  18.0          8         318.0  ...         11.0          70       1\n",
              "3  16.0          8         304.0  ...         12.0          70       1\n",
              "4  17.0          8         302.0  ...         10.5          70       1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr3I9R4xjvUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwAssswZkghz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2ae3b94b-af4b-47a8-bace-c36103ff28e9"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  ... acceleration  model year  origin\n",
              "0  18.0          8         307.0  ...         12.0          70       1\n",
              "1  15.0          8         350.0  ...         11.5          70       1\n",
              "2  18.0          8         318.0  ...         11.0          70       1\n",
              "3  16.0          8         304.0  ...         12.0          70       1\n",
              "4  17.0          8         302.0  ...         10.5          70       1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJA9RFF3j7Sz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6b71c8b4-ba11-42a8-d3b3-721cb416ae0d"
      },
      "source": [
        "for cols in df.columns:\n",
        "  df = df[~df[cols].isin(['?'])]\n",
        "df.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  ... acceleration  model year  origin\n",
              "0  18.0          8         307.0  ...         12.0          70       1\n",
              "1  15.0          8         350.0  ...         11.5          70       1\n",
              "2  18.0          8         318.0  ...         11.0          70       1\n",
              "3  16.0          8         304.0  ...         12.0          70       1\n",
              "4  17.0          8         302.0  ...         10.5          70       1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg9CShG5lf3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "49587f24-ea30-43af-fc1e-fdf65c2b7e8f"
      },
      "source": [
        "for col in df.columns:\n",
        "  vals = df[col].values\n",
        "  print(type(vals[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.float64'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.float64'>\n",
            "<class 'str'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.float64'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.int64'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWr61AWsn9Rk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa30379d-def5-4634-98b8-cee42d5177d2"
      },
      "source": [
        "df[df.columns[len(df.columns)-1]].unique()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvLqRAeLlw95",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "3a5b4fd1-5f46-4252-8c85-1a84df69d5bf"
      },
      "source": [
        "one_hot = pd.get_dummies(df[df.columns[len(df.columns)-1]])\n",
        "df = df.drop(df.columns[len(df.columns)-1],axis = 1)\n",
        "df = df.join(one_hot)\n",
        "df.head(5)  "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement horsepower  ...  model year  1  2  3\n",
              "0  18.0          8         307.0        130  ...          70  1  0  0\n",
              "1  15.0          8         350.0        165  ...          70  1  0  0\n",
              "2  18.0          8         318.0        150  ...          70  1  0  0\n",
              "3  16.0          8         304.0        150  ...          70  1  0  0\n",
              "4  17.0          8         302.0        140  ...          70  1  0  0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Q6WGdBrTQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "07841f8e-e306-41d1-cbe3-6af6e28c7ca0"
      },
      "source": [
        "for col in df.columns:\n",
        "  vals = df[col].values\n",
        "  print(type(vals[0]))\n",
        "x = df[df.columns[3]]\n",
        "x = np.asfarray(x,float)\n",
        "df[df.columns[3]] = x"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.float64'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.float64'>\n",
            "<class 'str'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.float64'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.uint8'>\n",
            "<class 'numpy.uint8'>\n",
            "<class 'numpy.uint8'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69rIKe50tYy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df.drop(columns=[df.columns[0],df.columns[len(df.columns)-1],df.columns[len(df.columns)-2],df.columns[len(df.columns)-3]])\n",
        "normalized_df = (df2-df2.mean())/df2.std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRX-jtO2tb7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8ddb94b-abdc-4a63-9bd0-304ccb0504dc"
      },
      "source": [
        "for cols in normalized_df.columns:\n",
        "  df[cols] = normalized_df[cols]\n",
        "df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.075915</td>\n",
              "      <td>0.663285</td>\n",
              "      <td>0.619748</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.572585</td>\n",
              "      <td>0.842258</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.539692</td>\n",
              "      <td>-1.646086</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.536160</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.028134</td>\n",
              "      <td>0.923085</td>\n",
              "      <td>0.554997</td>\n",
              "      <td>-1.827320</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.241772</td>\n",
              "      <td>2.429924</td>\n",
              "      <td>1.605147</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.480677</td>\n",
              "      <td>3.001484</td>\n",
              "      <td>1.620452</td>\n",
              "      <td>-2.371022</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.346890</td>\n",
              "      <td>2.871584</td>\n",
              "      <td>1.571005</td>\n",
              "      <td>-2.552256</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.490234</td>\n",
              "      <td>3.131384</td>\n",
              "      <td>1.704040</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.869080</td>\n",
              "      <td>2.222085</td>\n",
              "      <td>1.027093</td>\n",
              "      <td>-2.552256</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.802186</td>\n",
              "      <td>1.702485</td>\n",
              "      <td>0.689209</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.391269</td>\n",
              "      <td>1.442685</td>\n",
              "      <td>0.743365</td>\n",
              "      <td>-2.733490</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.964642</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.922314</td>\n",
              "      <td>-2.189788</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.490234</td>\n",
              "      <td>3.131384</td>\n",
              "      <td>0.127638</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.777990</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.712953</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.034288</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.170219</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>-0.194055</td>\n",
              "      <td>-0.239679</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>-0.459834</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-1.519034</td>\n",
              "      <td>-1.345162</td>\n",
              "      <td>1.797361</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.806659</td>\n",
              "      <td>-0.453855</td>\n",
              "      <td>-0.359764</td>\n",
              "      <td>0.709956</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.835327</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.644670</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.863996</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.709421</td>\n",
              "      <td>0.709956</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.701540</td>\n",
              "      <td>0.221625</td>\n",
              "      <td>-0.875420</td>\n",
              "      <td>-1.102384</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.388019</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.582394</td>\n",
              "      <td>2.871584</td>\n",
              "      <td>1.927726</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.075915</td>\n",
              "      <td>2.481884</td>\n",
              "      <td>1.646352</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>11.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>2.741684</td>\n",
              "      <td>1.653416</td>\n",
              "      <td>-0.739916</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>9.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>2.300025</td>\n",
              "      <td>2.065470</td>\n",
              "      <td>1.072424</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.438643</td>\n",
              "      <td>1.471139</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.397437</td>\n",
              "      <td>1.108671</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>34.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.685875</td>\n",
              "      <td>0.891190</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>-0.473962</td>\n",
              "      <td>0.238748</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>29.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.532826</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.285594</td>\n",
              "      <td>0.891190</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.132545</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.854440</td>\n",
              "      <td>-0.791594</td>\n",
              "      <td>-1.174454</td>\n",
              "      <td>-0.087473</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>37.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.121476</td>\n",
              "      <td>0.963684</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.186227</td>\n",
              "      <td>0.746203</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.854440</td>\n",
              "      <td>-1.077374</td>\n",
              "      <td>-1.003746</td>\n",
              "      <td>-0.304954</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.921333</td>\n",
              "      <td>-0.895514</td>\n",
              "      <td>-1.003746</td>\n",
              "      <td>0.637463</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.962540</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.835327</td>\n",
              "      <td>-0.765614</td>\n",
              "      <td>-0.909562</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>34.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.825771</td>\n",
              "      <td>-0.895514</td>\n",
              "      <td>-0.862470</td>\n",
              "      <td>0.492476</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.192113</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.192113</td>\n",
              "      <td>0.057514</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.156794</td>\n",
              "      <td>0.238748</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>25.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.128168</td>\n",
              "      <td>0.143685</td>\n",
              "      <td>-0.038361</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>38.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.645885</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>0.044050</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.367073</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.462189</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.359199</td>\n",
              "      <td>0.195645</td>\n",
              "      <td>-0.167864</td>\n",
              "      <td>-0.304954</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.481748</td>\n",
              "      <td>-0.220035</td>\n",
              "      <td>-0.368005</td>\n",
              "      <td>-0.594928</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.715308</td>\n",
              "      <td>-0.921150</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.032475</td>\n",
              "      <td>0.637463</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.479835</td>\n",
              "      <td>-0.220842</td>\n",
              "      <td>0.021267</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>44.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-1.363154</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>3.283479</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.803605</td>\n",
              "      <td>-1.428605</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.661694</td>\n",
              "      <td>-0.415097</td>\n",
              "      <td>1.108671</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.720653</td>\n",
              "      <td>-0.583754</td>\n",
              "      <td>-0.303253</td>\n",
              "      <td>1.398646</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>392 rows  10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      mpg  cylinders  displacement  horsepower  ...  model year  1  2  3\n",
              "0    18.0   1.482053      1.075915    0.663285  ...   -1.623241  1  0  0\n",
              "1    15.0   1.482053      1.486832    1.572585  ...   -1.623241  1  0  0\n",
              "2    18.0   1.482053      1.181033    1.182885  ...   -1.623241  1  0  0\n",
              "3    16.0   1.482053      1.047246    1.182885  ...   -1.623241  1  0  0\n",
              "4    17.0   1.482053      1.028134    0.923085  ...   -1.623241  1  0  0\n",
              "5    15.0   1.482053      2.241772    2.429924  ...   -1.623241  1  0  0\n",
              "6    14.0   1.482053      2.480677    3.001484  ...   -1.623241  1  0  0\n",
              "7    14.0   1.482053      2.346890    2.871584  ...   -1.623241  1  0  0\n",
              "8    14.0   1.482053      2.490234    3.131384  ...   -1.623241  1  0  0\n",
              "9    15.0   1.482053      1.869080    2.222085  ...   -1.623241  1  0  0\n",
              "10   15.0   1.482053      1.802186    1.702485  ...   -1.623241  1  0  0\n",
              "11   14.0   1.482053      1.391269    1.442685  ...   -1.623241  1  0  0\n",
              "12   15.0   1.482053      1.964642    1.182885  ...   -1.623241  1  0  0\n",
              "13   14.0   1.482053      2.490234    3.131384  ...   -1.623241  1  0  0\n",
              "14   24.0  -0.862911     -0.777990   -0.246015  ...   -1.623241  0  0  1\n",
              "15   22.0   0.309571      0.034288   -0.246015  ...   -1.623241  1  0  0\n",
              "16   18.0   0.309571      0.043844   -0.194055  ...   -1.623241  1  0  0\n",
              "17   21.0   0.309571      0.053400   -0.505815  ...   -1.623241  1  0  0\n",
              "18   27.0  -0.862911     -0.930889   -0.427875  ...   -1.623241  0  0  1\n",
              "19   26.0  -0.862911     -0.930889   -1.519034  ...   -1.623241  0  1  0\n",
              "20   25.0  -0.862911     -0.806659   -0.453855  ...   -1.623241  0  1  0\n",
              "21   24.0  -0.862911     -0.835327   -0.375915  ...   -1.623241  0  1  0\n",
              "22   25.0  -0.862911     -0.863996   -0.246015  ...   -1.623241  0  1  0\n",
              "23   26.0  -0.862911     -0.701540    0.221625  ...   -1.623241  0  1  0\n",
              "24   21.0   0.309571      0.043844   -0.375915  ...   -1.623241  1  0  0\n",
              "25   10.0   1.482053      1.582394    2.871584  ...   -1.623241  1  0  0\n",
              "26   10.0   1.482053      1.075915    2.481884  ...   -1.623241  1  0  0\n",
              "27   11.0   1.482053      1.181033    2.741684  ...   -1.623241  1  0  0\n",
              "28    9.0   1.482053      1.047246    2.300025  ...   -1.623241  1  0  0\n",
              "29   27.0  -0.862911     -0.930889   -0.427875  ...   -1.351777  0  0  1\n",
              "..    ...        ...           ...         ...  ...         ... .. .. ..\n",
              "367  28.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "368  27.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "369  34.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "370  31.0  -0.862911     -0.787546   -0.505815  ...    1.634321  1  0  0\n",
              "371  29.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "372  27.0  -0.862911     -0.414854   -0.375915  ...    1.634321  1  0  0\n",
              "373  24.0  -0.862911     -0.519972   -0.323955  ...    1.634321  1  0  0\n",
              "375  36.0  -0.862911     -0.854440   -0.791594  ...    1.634321  0  1  0\n",
              "376  37.0  -0.862911     -0.988227   -0.947474  ...    1.634321  0  0  1\n",
              "377  31.0  -0.862911     -0.988227   -0.947474  ...    1.634321  0  0  1\n",
              "378  38.0  -0.862911     -0.854440   -1.077374  ...    1.634321  1  0  0\n",
              "379  36.0  -0.862911     -0.921333   -0.895514  ...    1.634321  1  0  0\n",
              "380  36.0  -0.862911     -0.711097   -0.427875  ...    1.634321  0  0  1\n",
              "381  36.0  -0.862911     -0.835327   -0.765614  ...    1.634321  0  0  1\n",
              "382  34.0  -0.862911     -0.825771   -0.895514  ...    1.634321  0  0  1\n",
              "383  38.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "384  32.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "385  38.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "386  25.0   0.309571     -0.128168    0.143685  ...    1.634321  1  0  0\n",
              "387  38.0   0.309571      0.645885   -0.505815  ...    1.634321  1  0  0\n",
              "388  26.0  -0.862911     -0.367073   -0.323955  ...    1.634321  1  0  0\n",
              "389  22.0   0.309571      0.359199    0.195645  ...    1.634321  1  0  0\n",
              "390  32.0  -0.862911     -0.481748   -0.220035  ...    1.634321  0  0  1\n",
              "391  36.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "392  27.0  -0.862911     -0.414854   -0.375915  ...    1.634321  1  0  0\n",
              "393  27.0  -0.862911     -0.519972   -0.479835  ...    1.634321  1  0  0\n",
              "394  44.0  -0.862911     -0.930889   -1.363154  ...    1.634321  0  1  0\n",
              "395  32.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "396  28.0  -0.862911     -0.711097   -0.661694  ...    1.634321  1  0  0\n",
              "397  31.0  -0.862911     -0.720653   -0.583754  ...    1.634321  1  0  0\n",
              "\n",
              "[392 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9HrCzDRpTRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msk = np.random.rand(len(df)) < 0.8\n",
        "train_df = df[msk]\n",
        "test_df = df[~msk]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vL38AoGq921",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc97c511-cfa9-41e7-9d4d-22fe6ec3aa3d"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.075915</td>\n",
              "      <td>0.663285</td>\n",
              "      <td>0.619748</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.572585</td>\n",
              "      <td>0.842258</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.539692</td>\n",
              "      <td>-1.646086</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.536160</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.028134</td>\n",
              "      <td>0.923085</td>\n",
              "      <td>0.554997</td>\n",
              "      <td>-1.827320</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.241772</td>\n",
              "      <td>2.429924</td>\n",
              "      <td>1.605147</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.480677</td>\n",
              "      <td>3.001484</td>\n",
              "      <td>1.620452</td>\n",
              "      <td>-2.371022</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.346890</td>\n",
              "      <td>2.871584</td>\n",
              "      <td>1.571005</td>\n",
              "      <td>-2.552256</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.490234</td>\n",
              "      <td>3.131384</td>\n",
              "      <td>1.704040</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.802186</td>\n",
              "      <td>1.702485</td>\n",
              "      <td>0.689209</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.391269</td>\n",
              "      <td>1.442685</td>\n",
              "      <td>0.743365</td>\n",
              "      <td>-2.733490</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.964642</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.922314</td>\n",
              "      <td>-2.189788</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>2.490234</td>\n",
              "      <td>3.131384</td>\n",
              "      <td>0.127638</td>\n",
              "      <td>-2.008554</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.777990</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.712953</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.034288</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.170219</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>-0.194055</td>\n",
              "      <td>-0.239679</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>-0.459834</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.806659</td>\n",
              "      <td>-0.453855</td>\n",
              "      <td>-0.359764</td>\n",
              "      <td>0.709956</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.835327</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.644670</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.863996</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.709421</td>\n",
              "      <td>0.709956</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.701540</td>\n",
              "      <td>0.221625</td>\n",
              "      <td>-0.875420</td>\n",
              "      <td>-1.102384</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.388019</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.582394</td>\n",
              "      <td>2.871584</td>\n",
              "      <td>1.927726</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>10.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.075915</td>\n",
              "      <td>2.481884</td>\n",
              "      <td>1.646352</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>11.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>2.741684</td>\n",
              "      <td>1.653416</td>\n",
              "      <td>-0.739916</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.840101</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.777990</td>\n",
              "      <td>-0.246015</td>\n",
              "      <td>-0.882484</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>19.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.359199</td>\n",
              "      <td>-0.116115</td>\n",
              "      <td>-0.404501</td>\n",
              "      <td>-0.921150</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>16.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.292305</td>\n",
              "      <td>0.013785</td>\n",
              "      <td>0.543224</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>25.4</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.252399</td>\n",
              "      <td>0.299565</td>\n",
              "      <td>-0.091340</td>\n",
              "      <td>-1.066137</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>24.2</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.462635</td>\n",
              "      <td>0.403485</td>\n",
              "      <td>-0.056021</td>\n",
              "      <td>-0.631175</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>22.4</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.349643</td>\n",
              "      <td>0.143685</td>\n",
              "      <td>0.514969</td>\n",
              "      <td>0.093761</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>26.6</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>0.013785</td>\n",
              "      <td>0.879931</td>\n",
              "      <td>1.253659</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>20.2</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>0.097028</td>\n",
              "      <td>0.564969</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>17.6</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.292305</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>0.573834</td>\n",
              "      <td>0.383735</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.397437</td>\n",
              "      <td>1.108671</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>34.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.685875</td>\n",
              "      <td>0.891190</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>-0.473962</td>\n",
              "      <td>0.238748</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>29.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.532826</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.285594</td>\n",
              "      <td>0.891190</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.132545</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.854440</td>\n",
              "      <td>-0.791594</td>\n",
              "      <td>-1.174454</td>\n",
              "      <td>-0.087473</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>37.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.121476</td>\n",
              "      <td>0.963684</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.186227</td>\n",
              "      <td>0.746203</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.921333</td>\n",
              "      <td>-0.895514</td>\n",
              "      <td>-1.003746</td>\n",
              "      <td>0.637463</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.835327</td>\n",
              "      <td>-0.765614</td>\n",
              "      <td>-0.909562</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.192113</td>\n",
              "      <td>0.057514</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>25.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.128168</td>\n",
              "      <td>0.143685</td>\n",
              "      <td>-0.038361</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>38.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.645885</td>\n",
              "      <td>-0.505815</td>\n",
              "      <td>0.044050</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.367073</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.462189</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.359199</td>\n",
              "      <td>0.195645</td>\n",
              "      <td>-0.167864</td>\n",
              "      <td>-0.304954</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.481748</td>\n",
              "      <td>-0.220035</td>\n",
              "      <td>-0.368005</td>\n",
              "      <td>-0.594928</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.715308</td>\n",
              "      <td>-0.921150</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.032475</td>\n",
              "      <td>0.637463</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.479835</td>\n",
              "      <td>-0.220842</td>\n",
              "      <td>0.021267</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>44.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-1.363154</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>3.283479</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>32.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.567753</td>\n",
              "      <td>-0.531795</td>\n",
              "      <td>-0.803605</td>\n",
              "      <td>-1.428605</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.661694</td>\n",
              "      <td>-0.415097</td>\n",
              "      <td>1.108671</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.720653</td>\n",
              "      <td>-0.583754</td>\n",
              "      <td>-0.303253</td>\n",
              "      <td>1.398646</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>310 rows  10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      mpg  cylinders  displacement  horsepower  ...  model year  1  2  3\n",
              "0    18.0   1.482053      1.075915    0.663285  ...   -1.623241  1  0  0\n",
              "1    15.0   1.482053      1.486832    1.572585  ...   -1.623241  1  0  0\n",
              "2    18.0   1.482053      1.181033    1.182885  ...   -1.623241  1  0  0\n",
              "3    16.0   1.482053      1.047246    1.182885  ...   -1.623241  1  0  0\n",
              "4    17.0   1.482053      1.028134    0.923085  ...   -1.623241  1  0  0\n",
              "5    15.0   1.482053      2.241772    2.429924  ...   -1.623241  1  0  0\n",
              "6    14.0   1.482053      2.480677    3.001484  ...   -1.623241  1  0  0\n",
              "7    14.0   1.482053      2.346890    2.871584  ...   -1.623241  1  0  0\n",
              "8    14.0   1.482053      2.490234    3.131384  ...   -1.623241  1  0  0\n",
              "10   15.0   1.482053      1.802186    1.702485  ...   -1.623241  1  0  0\n",
              "11   14.0   1.482053      1.391269    1.442685  ...   -1.623241  1  0  0\n",
              "12   15.0   1.482053      1.964642    1.182885  ...   -1.623241  1  0  0\n",
              "13   14.0   1.482053      2.490234    3.131384  ...   -1.623241  1  0  0\n",
              "14   24.0  -0.862911     -0.777990   -0.246015  ...   -1.623241  0  0  1\n",
              "15   22.0   0.309571      0.034288   -0.246015  ...   -1.623241  1  0  0\n",
              "16   18.0   0.309571      0.043844   -0.194055  ...   -1.623241  1  0  0\n",
              "17   21.0   0.309571      0.053400   -0.505815  ...   -1.623241  1  0  0\n",
              "18   27.0  -0.862911     -0.930889   -0.427875  ...   -1.623241  0  0  1\n",
              "20   25.0  -0.862911     -0.806659   -0.453855  ...   -1.623241  0  1  0\n",
              "21   24.0  -0.862911     -0.835327   -0.375915  ...   -1.623241  0  1  0\n",
              "22   25.0  -0.862911     -0.863996   -0.246015  ...   -1.623241  0  1  0\n",
              "23   26.0  -0.862911     -0.701540    0.221625  ...   -1.623241  0  1  0\n",
              "24   21.0   0.309571      0.043844   -0.375915  ...   -1.623241  1  0  0\n",
              "25   10.0   1.482053      1.582394    2.871584  ...   -1.623241  1  0  0\n",
              "26   10.0   1.482053      1.075915    2.481884  ...   -1.623241  1  0  0\n",
              "27   11.0   1.482053      1.181033    2.741684  ...   -1.623241  1  0  0\n",
              "30   28.0  -0.862911     -0.519972   -0.375915  ...   -1.351777  1  0  0\n",
              "31   25.0  -0.862911     -0.777990   -0.246015  ...   -1.351777  0  0  1\n",
              "33   19.0   0.309571      0.359199   -0.116115  ...   -1.351777  1  0  0\n",
              "34   16.0   0.309571      0.292305    0.013785  ...   -1.351777  1  0  0\n",
              "..    ...        ...           ...         ...  ...         ... .. .. ..\n",
              "361  25.4   0.309571     -0.252399    0.299565  ...    1.362858  0  0  1\n",
              "362  24.2   0.309571     -0.462635    0.403485  ...    1.362858  0  0  1\n",
              "363  22.4   0.309571      0.349643    0.143685  ...    1.362858  1  0  0\n",
              "364  26.6   1.482053      1.486832    0.013785  ...    1.362858  1  0  0\n",
              "365  20.2   0.309571      0.053400   -0.427875  ...    1.362858  1  0  0\n",
              "366  17.6   0.309571      0.292305   -0.505815  ...    1.362858  1  0  0\n",
              "368  27.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "369  34.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "370  31.0  -0.862911     -0.787546   -0.505815  ...    1.634321  1  0  0\n",
              "371  29.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "372  27.0  -0.862911     -0.414854   -0.375915  ...    1.634321  1  0  0\n",
              "373  24.0  -0.862911     -0.519972   -0.323955  ...    1.634321  1  0  0\n",
              "375  36.0  -0.862911     -0.854440   -0.791594  ...    1.634321  0  1  0\n",
              "376  37.0  -0.862911     -0.988227   -0.947474  ...    1.634321  0  0  1\n",
              "377  31.0  -0.862911     -0.988227   -0.947474  ...    1.634321  0  0  1\n",
              "379  36.0  -0.862911     -0.921333   -0.895514  ...    1.634321  1  0  0\n",
              "381  36.0  -0.862911     -0.835327   -0.765614  ...    1.634321  0  0  1\n",
              "384  32.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "386  25.0   0.309571     -0.128168    0.143685  ...    1.634321  1  0  0\n",
              "387  38.0   0.309571      0.645885   -0.505815  ...    1.634321  1  0  0\n",
              "388  26.0  -0.862911     -0.367073   -0.323955  ...    1.634321  1  0  0\n",
              "389  22.0   0.309571      0.359199    0.195645  ...    1.634321  1  0  0\n",
              "390  32.0  -0.862911     -0.481748   -0.220035  ...    1.634321  0  0  1\n",
              "391  36.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "392  27.0  -0.862911     -0.414854   -0.375915  ...    1.634321  1  0  0\n",
              "393  27.0  -0.862911     -0.519972   -0.479835  ...    1.634321  1  0  0\n",
              "394  44.0  -0.862911     -0.930889   -1.363154  ...    1.634321  0  1  0\n",
              "395  32.0  -0.862911     -0.567753   -0.531795  ...    1.634321  1  0  0\n",
              "396  28.0  -0.862911     -0.711097   -0.661694  ...    1.634321  1  0  0\n",
              "397  31.0  -0.862911     -0.720653   -0.583754  ...    1.634321  1  0  0\n",
              "\n",
              "[310 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbmXLkJ4rKLB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89bdbcce-d879-4f01-91e7-11d99e56040c"
      },
      "source": [
        "test_df"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>model year</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.869080</td>\n",
              "      <td>2.222085</td>\n",
              "      <td>1.027093</td>\n",
              "      <td>-2.552256</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-1.519034</td>\n",
              "      <td>-1.345162</td>\n",
              "      <td>1.797361</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>9.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>2.300025</td>\n",
              "      <td>2.065470</td>\n",
              "      <td>1.072424</td>\n",
              "      <td>-1.623241</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.997859</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.964642</td>\n",
              "      <td>1.832385</td>\n",
              "      <td>1.749954</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>12.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.802186</td>\n",
              "      <td>1.962285</td>\n",
              "      <td>2.328008</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>19.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.531211</td>\n",
              "      <td>-0.116115</td>\n",
              "      <td>0.358388</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>30.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.016895</td>\n",
              "      <td>-0.739634</td>\n",
              "      <td>-1.074384</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-1.351777</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>25.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.926111</td>\n",
              "      <td>-0.635714</td>\n",
              "      <td>-1.002568</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>21.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.691984</td>\n",
              "      <td>-0.479835</td>\n",
              "      <td>-0.884839</td>\n",
              "      <td>0.347488</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>13.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.572585</td>\n",
              "      <td>1.526268</td>\n",
              "      <td>-1.283618</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.496388</td>\n",
              "      <td>1.260825</td>\n",
              "      <td>1.355560</td>\n",
              "      <td>-0.921150</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>13.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.312785</td>\n",
              "      <td>1.794692</td>\n",
              "      <td>-0.739916</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>12.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>1.442685</td>\n",
              "      <td>1.740536</td>\n",
              "      <td>-0.739916</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.323955</td>\n",
              "      <td>-0.811846</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-1.033178</td>\n",
              "      <td>0.347488</td>\n",
              "      <td>-1.080314</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.047246</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.817534</td>\n",
              "      <td>-1.464852</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.181033</td>\n",
              "      <td>1.182885</td>\n",
              "      <td>0.941151</td>\n",
              "      <td>-1.102384</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.359199</td>\n",
              "      <td>-0.116115</td>\n",
              "      <td>-0.038361</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.531211</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>0.051113</td>\n",
              "      <td>0.347488</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>20.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.822442</td>\n",
              "      <td>1.253659</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>16.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.964642</td>\n",
              "      <td>3.261284</td>\n",
              "      <td>1.530977</td>\n",
              "      <td>-2.189788</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.749321</td>\n",
              "      <td>-0.765614</td>\n",
              "      <td>-0.964895</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-0.808850</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>16.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.531211</td>\n",
              "      <td>-0.116115</td>\n",
              "      <td>0.945860</td>\n",
              "      <td>0.528722</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>0.292305</td>\n",
              "      <td>0.013785</td>\n",
              "      <td>0.748074</td>\n",
              "      <td>0.347488</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>16.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.028134</td>\n",
              "      <td>0.923085</td>\n",
              "      <td>1.369687</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.102901</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.194468</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.687674</td>\n",
              "      <td>-0.797719</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>24.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.194055</td>\n",
              "      <td>-0.575209</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.825771</td>\n",
              "      <td>-0.297975</td>\n",
              "      <td>-0.690584</td>\n",
              "      <td>-0.014980</td>\n",
              "      <td>-0.537387</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>25.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.519972</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.303253</td>\n",
              "      <td>-0.051226</td>\n",
              "      <td>0.548467</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>20.3</td>\n",
              "      <td>-0.276670</td>\n",
              "      <td>-0.605978</td>\n",
              "      <td>-0.038175</td>\n",
              "      <td>-0.173751</td>\n",
              "      <td>0.130008</td>\n",
              "      <td>0.548467</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>31.5</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.007339</td>\n",
              "      <td>-0.869534</td>\n",
              "      <td>-1.162681</td>\n",
              "      <td>-0.232460</td>\n",
              "      <td>0.548467</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>17.6</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.028134</td>\n",
              "      <td>0.637305</td>\n",
              "      <td>0.879931</td>\n",
              "      <td>-0.776162</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>19.2</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>0.693666</td>\n",
              "      <td>0.533385</td>\n",
              "      <td>0.738655</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>31.9</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.007339</td>\n",
              "      <td>-0.869534</td>\n",
              "      <td>-1.239205</td>\n",
              "      <td>-0.558682</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>34.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.036008</td>\n",
              "      <td>-1.025414</td>\n",
              "      <td>-1.180340</td>\n",
              "      <td>-0.123720</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>35.7</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.921333</td>\n",
              "      <td>-0.635714</td>\n",
              "      <td>-1.250978</td>\n",
              "      <td>-0.413694</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>27.4</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.701540</td>\n",
              "      <td>-0.635714</td>\n",
              "      <td>-0.362118</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>23.0</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>1.486832</td>\n",
              "      <td>0.533385</td>\n",
              "      <td>1.085958</td>\n",
              "      <td>0.673710</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>23.9</td>\n",
              "      <td>1.482053</td>\n",
              "      <td>0.626773</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>0.520855</td>\n",
              "      <td>2.413556</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>28.4</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.414854</td>\n",
              "      <td>-0.375915</td>\n",
              "      <td>-0.362118</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>28.8</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.204617</td>\n",
              "      <td>0.273585</td>\n",
              "      <td>-0.450416</td>\n",
              "      <td>-1.537345</td>\n",
              "      <td>0.819931</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>37.2</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.036008</td>\n",
              "      <td>-1.025414</td>\n",
              "      <td>-1.128539</td>\n",
              "      <td>0.311242</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>34.3</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.930889</td>\n",
              "      <td>-0.687674</td>\n",
              "      <td>-0.929576</td>\n",
              "      <td>0.093761</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>40.8</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.045564</td>\n",
              "      <td>-1.025414</td>\n",
              "      <td>-1.021405</td>\n",
              "      <td>1.326152</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>36.4</td>\n",
              "      <td>-0.276670</td>\n",
              "      <td>-0.701540</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-0.032475</td>\n",
              "      <td>1.579880</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>30.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.462635</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>0.320715</td>\n",
              "      <td>2.268569</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329</th>\n",
              "      <td>44.6</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.327503</td>\n",
              "      <td>-0.631175</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>29.8</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.007339</td>\n",
              "      <td>-1.103354</td>\n",
              "      <td>-1.333389</td>\n",
              "      <td>-0.087473</td>\n",
              "      <td>1.091394</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>23.5</td>\n",
              "      <td>0.309571</td>\n",
              "      <td>-0.204617</td>\n",
              "      <td>0.143685</td>\n",
              "      <td>-0.297367</td>\n",
              "      <td>-1.066137</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>39.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-1.102901</td>\n",
              "      <td>-1.207274</td>\n",
              "      <td>-1.439346</td>\n",
              "      <td>0.492476</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>34.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.947474</td>\n",
              "      <td>-1.168567</td>\n",
              "      <td>0.166254</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>28.1</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.510416</td>\n",
              "      <td>-0.635714</td>\n",
              "      <td>0.297169</td>\n",
              "      <td>1.761114</td>\n",
              "      <td>1.362858</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.787546</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.438643</td>\n",
              "      <td>1.471139</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.854440</td>\n",
              "      <td>-1.077374</td>\n",
              "      <td>-1.003746</td>\n",
              "      <td>-0.304954</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>36.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.711097</td>\n",
              "      <td>-0.427875</td>\n",
              "      <td>-0.962540</td>\n",
              "      <td>-0.377448</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>34.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.825771</td>\n",
              "      <td>-0.895514</td>\n",
              "      <td>-0.862470</td>\n",
              "      <td>0.492476</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.192113</td>\n",
              "      <td>-0.196214</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>38.0</td>\n",
              "      <td>-0.862911</td>\n",
              "      <td>-0.988227</td>\n",
              "      <td>-0.973454</td>\n",
              "      <td>-1.156794</td>\n",
              "      <td>0.238748</td>\n",
              "      <td>1.634321</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82 rows  10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      mpg  cylinders  displacement  horsepower  ...  model year  1  2  3\n",
              "9    15.0   1.482053      1.869080    2.222085  ...   -1.623241  1  0  0\n",
              "19   26.0  -0.862911     -0.930889   -1.519034  ...   -1.623241  0  1  0\n",
              "28    9.0   1.482053      1.047246    2.300025  ...   -1.623241  1  0  0\n",
              "29   27.0  -0.862911     -0.930889   -0.427875  ...   -1.351777  0  0  1\n",
              "39   14.0   1.482053      1.964642    1.832385  ...   -1.351777  1  0  0\n",
              "42   12.0   1.482053      1.802186    1.962285  ...   -1.351777  1  0  0\n",
              "47   19.0   0.309571      0.531211   -0.116115  ...   -1.351777  1  0  0\n",
              "52   30.0  -0.862911     -1.016895   -0.739634  ...   -1.351777  0  1  0\n",
              "58   25.0  -0.862911     -0.926111   -0.635714  ...   -1.080314  1  0  0\n",
              "61   21.0  -0.862911     -0.691984   -0.479835  ...   -1.080314  1  0  0\n",
              "62   13.0   1.482053      1.486832    1.572585  ...   -1.080314  1  0  0\n",
              "65   14.0   1.482053      1.496388    1.260825  ...   -1.080314  1  0  0\n",
              "68   13.0   1.482053      1.486832    1.312785  ...   -1.080314  1  0  0\n",
              "69   12.0   1.482053      1.486832    1.442685  ...   -1.080314  1  0  0\n",
              "81   28.0  -0.862911     -0.930889   -0.323955  ...   -1.080314  0  0  1\n",
              "84   27.0  -0.862911     -0.930889   -0.427875  ...   -1.080314  0  0  1\n",
              "86   14.0   1.482053      1.047246    1.182885  ...   -0.808850  1  0  0\n",
              "89   15.0   1.482053      1.181033    1.182885  ...   -0.808850  1  0  0\n",
              "99   18.0   0.309571      0.359199   -0.116115  ...   -0.808850  1  0  0\n",
              "100  18.0   0.309571      0.531211   -0.427875  ...   -0.808850  1  0  0\n",
              "108  20.0  -0.862911     -0.930889   -0.427875  ...   -0.808850  0  0  1\n",
              "116  16.0   1.482053      1.964642    3.261284  ...   -0.808850  1  0  0\n",
              "118  24.0  -0.862911     -0.749321   -0.765614  ...   -0.808850  0  1  0\n",
              "133  16.0   0.309571      0.531211   -0.116115  ...   -0.537387  1  0  0\n",
              "135  18.0   0.309571      0.292305    0.013785  ...   -0.537387  1  0  0\n",
              "136  16.0   1.482053      1.028134    0.923085  ...   -0.537387  1  0  0\n",
              "142  26.0  -0.862911     -1.102901   -0.973454  ...   -0.537387  0  1  0\n",
              "143  26.0  -0.862911     -0.930889   -0.687674  ...   -0.537387  0  1  0\n",
              "149  24.0  -0.862911     -0.711097   -0.194055  ...   -0.537387  0  0  1\n",
              "150  26.0  -0.862911     -0.825771   -0.297975  ...   -0.537387  0  0  1\n",
              "..    ...        ...           ...         ...  ...         ... .. .. ..\n",
              "255  25.1  -0.862911     -0.519972   -0.427875  ...    0.548467  1  0  0\n",
              "274  20.3  -0.276670     -0.605978   -0.038175  ...    0.548467  0  1  0\n",
              "278  31.5  -0.862911     -1.007339   -0.869534  ...    0.548467  0  1  0\n",
              "286  17.6   1.482053      1.028134    0.637305  ...    0.819931  1  0  0\n",
              "291  19.2   1.482053      0.693666    0.533385  ...    0.819931  1  0  0\n",
              "293  31.9  -0.862911     -1.007339   -0.869534  ...    0.819931  0  1  0\n",
              "294  34.1  -0.862911     -1.036008   -1.025414  ...    0.819931  0  0  1\n",
              "295  35.7  -0.862911     -0.921333   -0.635714  ...    0.819931  1  0  0\n",
              "296  27.4  -0.862911     -0.701540   -0.635714  ...    0.819931  1  0  0\n",
              "298  23.0   1.482053      1.486832    0.533385  ...    0.819931  1  0  0\n",
              "300  23.9   1.482053      0.626773   -0.375915  ...    0.819931  1  0  0\n",
              "305  28.4  -0.862911     -0.414854   -0.375915  ...    0.819931  1  0  0\n",
              "306  28.8   0.309571     -0.204617    0.273585  ...    0.819931  1  0  0\n",
              "312  37.2  -0.862911     -1.036008   -1.025414  ...    1.091394  0  0  1\n",
              "317  34.3  -0.862911     -0.930889   -0.687674  ...    1.091394  0  1  0\n",
              "324  40.8  -0.862911     -1.045564   -1.025414  ...    1.091394  0  0  1\n",
              "327  36.4  -0.276670     -0.701540   -0.973454  ...    1.091394  0  1  0\n",
              "328  30.0  -0.862911     -0.462635   -0.973454  ...    1.091394  0  1  0\n",
              "329  44.6  -0.862911     -0.988227   -0.973454  ...    1.091394  0  0  1\n",
              "332  29.8  -0.862911     -1.007339   -1.103354  ...    1.091394  0  1  0\n",
              "341  23.5   0.309571     -0.204617    0.143685  ...    1.362858  1  0  0\n",
              "343  39.1  -0.862911     -1.102901   -1.207274  ...    1.362858  0  0  1\n",
              "349  34.1  -0.862911     -0.988227   -0.947474  ...    1.362858  0  0  1\n",
              "359  28.1  -0.862911     -0.510416   -0.635714  ...    1.362858  0  1  0\n",
              "367  28.0  -0.862911     -0.787546   -0.427875  ...    1.634321  1  0  0\n",
              "378  38.0  -0.862911     -0.854440   -1.077374  ...    1.634321  1  0  0\n",
              "380  36.0  -0.862911     -0.711097   -0.427875  ...    1.634321  0  0  1\n",
              "382  34.0  -0.862911     -0.825771   -0.895514  ...    1.634321  0  0  1\n",
              "383  38.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "385  38.0  -0.862911     -0.988227   -0.973454  ...    1.634321  0  0  1\n",
              "\n",
              "[82 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS6QEn75ukb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = train_df.drop(columns = [df.columns[0]])\n",
        "testX = test_df.drop(columns = [df.columns[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlJINel7u2fR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainY = train_df.iloc[:,0]\n",
        "testY = test_df.iloc[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydRupT53vR1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression().fit(trainX, trainY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z_qhAg0vdmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc111607-b548-4eef-99d8-364131e1c866"
      },
      "source": [
        "reg.score(trainX, trainY)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8179584603381965"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrON98zev0mJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testY_pred = reg.predict(testX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEr46tZQv4pm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "95fe8fc2-3c69-492f-e54f-123b25f2e3b3"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(testY, testY_pred)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.855720325691431"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoXiphUKx8TZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8711b389-0668-4917-b21b-ff1ae5b0c298"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oucWG4yyP8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1000\n",
        "train_validation_split = 0.2\n",
        "verbose = 1\n",
        "input_dims = len(trainX.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUl3cF0_ykls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67dec731-702c-4749-8c67-397801c5dce8"
      },
      "source": [
        "input_dims"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA9bPFE-yN-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "47af5ead-970c-48f9-9db8-6d7568c0054c"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "model = Sequential()\n",
        "model.add(Dense(20,input_dim = input_dims))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('relu'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfoOOsDs1juI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "645aa97c-a34a-4f40-a143-b536b183afe9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 20)                200       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 6         \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 581\n",
            "Trainable params: 581\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4NE3zyH1XTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b3e93e9-e764-4f95-ac59-3f0220982f8a"
      },
      "source": [
        "model.compile(optimizer='sgd',loss='mean_squared_error',metrics=['accuracy'])\n",
        "history = model.fit(trainX,trainY,validation_split=train_validation_split,epochs=epochs,verbose=verbose,shuffle=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 248 samples, validate on 62 samples\n",
            "Epoch 1/1000\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 321.6951 - acc: 0.0000e+00 - val_loss: 397.5189 - val_acc: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 83.5675 - acc: 0.0484 - val_loss: 210.5179 - val_acc: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 49.3548 - acc: 0.0565 - val_loss: 161.2073 - val_acc: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 45.1995 - acc: 0.0242 - val_loss: 147.0171 - val_acc: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 44.4253 - acc: 0.0323 - val_loss: 135.8560 - val_acc: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 41.9583 - acc: 0.0202 - val_loss: 131.9582 - val_acc: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 25.6915 - acc: 0.0685 - val_loss: 109.3224 - val_acc: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 19.5159 - acc: 0.1048 - val_loss: 88.4793 - val_acc: 0.0323\n",
            "Epoch 9/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 15.7824 - acc: 0.1411 - val_loss: 88.6202 - val_acc: 0.0161\n",
            "Epoch 10/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 14.9764 - acc: 0.0806 - val_loss: 67.8088 - val_acc: 0.0323\n",
            "Epoch 11/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 13.5624 - acc: 0.1290 - val_loss: 61.6581 - val_acc: 0.0323\n",
            "Epoch 12/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 12.0773 - acc: 0.1371 - val_loss: 56.9838 - val_acc: 0.0484\n",
            "Epoch 13/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 22.5783 - acc: 0.0524 - val_loss: 58.0367 - val_acc: 0.0806\n",
            "Epoch 14/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 15.6839 - acc: 0.0444 - val_loss: 60.0507 - val_acc: 0.0806\n",
            "Epoch 15/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 14.5643 - acc: 0.0766 - val_loss: 59.2910 - val_acc: 0.0806\n",
            "Epoch 16/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 14.0424 - acc: 0.0766 - val_loss: 59.1333 - val_acc: 0.0806\n",
            "Epoch 17/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 12.4055 - acc: 0.0968 - val_loss: 54.4272 - val_acc: 0.0806\n",
            "Epoch 18/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 12.3841 - acc: 0.0806 - val_loss: 55.3537 - val_acc: 0.0645\n",
            "Epoch 19/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 10.4285 - acc: 0.1331 - val_loss: 45.0004 - val_acc: 0.0161\n",
            "Epoch 20/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 11.0093 - acc: 0.0887 - val_loss: 47.1425 - val_acc: 0.0323\n",
            "Epoch 21/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 11.9716 - acc: 0.1008 - val_loss: 49.7890 - val_acc: 0.0323\n",
            "Epoch 22/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 9.0821 - acc: 0.1210 - val_loss: 56.5561 - val_acc: 0.0323\n",
            "Epoch 23/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 12.1921 - acc: 0.1169 - val_loss: 39.1140 - val_acc: 0.0161\n",
            "Epoch 24/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 7.8431 - acc: 0.1411 - val_loss: 39.6849 - val_acc: 0.0323\n",
            "Epoch 25/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 12.6622 - acc: 0.1492 - val_loss: 39.3648 - val_acc: 0.0323\n",
            "Epoch 26/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 8.3249 - acc: 0.1290 - val_loss: 56.1934 - val_acc: 0.0323\n",
            "Epoch 27/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 11.2240 - acc: 0.1250 - val_loss: 36.9500 - val_acc: 0.0645\n",
            "Epoch 28/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 7.8397 - acc: 0.1290 - val_loss: 42.8309 - val_acc: 0.0968\n",
            "Epoch 29/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 7.3109 - acc: 0.1210 - val_loss: 35.2369 - val_acc: 0.0323\n",
            "Epoch 30/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 8.0281 - acc: 0.1331 - val_loss: 32.4310 - val_acc: 0.0323\n",
            "Epoch 31/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 8.6111 - acc: 0.1008 - val_loss: 38.4111 - val_acc: 0.0161\n",
            "Epoch 32/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 12.1867 - acc: 0.1048 - val_loss: 33.0653 - val_acc: 0.0645\n",
            "Epoch 33/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 7.5490 - acc: 0.1492 - val_loss: 31.8736 - val_acc: 0.0484\n",
            "Epoch 34/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 12.3437 - acc: 0.0605 - val_loss: 32.3391 - val_acc: 0.0323\n",
            "Epoch 35/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 9.4139 - acc: 0.0847 - val_loss: 32.7774 - val_acc: 0.0161\n",
            "Epoch 36/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 6.3020 - acc: 0.1411 - val_loss: 31.8910 - val_acc: 0.0323\n",
            "Epoch 37/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 6.9815 - acc: 0.1169 - val_loss: 33.0320 - val_acc: 0.0484\n",
            "Epoch 38/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 6.8058 - acc: 0.1331 - val_loss: 30.9430 - val_acc: 0.0484\n",
            "Epoch 39/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 5.7872 - acc: 0.1250 - val_loss: 31.4475 - val_acc: 0.0484\n",
            "Epoch 40/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 7.0783 - acc: 0.0847 - val_loss: 29.1688 - val_acc: 0.0484\n",
            "Epoch 41/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 7.6607 - acc: 0.1331 - val_loss: 28.4550 - val_acc: 0.0484\n",
            "Epoch 42/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 8.0648 - acc: 0.1331 - val_loss: 29.3935 - val_acc: 0.0484\n",
            "Epoch 43/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 5.8589 - acc: 0.1411 - val_loss: 28.1351 - val_acc: 0.0484\n",
            "Epoch 44/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 6.4820 - acc: 0.1250 - val_loss: 28.2428 - val_acc: 0.0323\n",
            "Epoch 45/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.3001 - acc: 0.1048 - val_loss: 29.4598 - val_acc: 0.0323\n",
            "Epoch 46/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 5.7174 - acc: 0.1492 - val_loss: 27.9611 - val_acc: 0.0484\n",
            "Epoch 47/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 6.2182 - acc: 0.1250 - val_loss: 30.0292 - val_acc: 0.0323\n",
            "Epoch 48/1000\n",
            "248/248 [==============================] - 0s 79us/step - loss: 5.9988 - acc: 0.1290 - val_loss: 29.6550 - val_acc: 0.0323\n",
            "Epoch 49/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 7.6254 - acc: 0.1089 - val_loss: 28.7151 - val_acc: 0.0323\n",
            "Epoch 50/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 5.0790 - acc: 0.1411 - val_loss: 28.7121 - val_acc: 0.0323\n",
            "Epoch 51/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 6.8115 - acc: 0.1008 - val_loss: 27.6907 - val_acc: 0.0484\n",
            "Epoch 52/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.4917 - acc: 0.1210 - val_loss: 27.6779 - val_acc: 0.0323\n",
            "Epoch 53/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 7.3301 - acc: 0.0927 - val_loss: 28.7024 - val_acc: 0.0323\n",
            "Epoch 54/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 6.2962 - acc: 0.1048 - val_loss: 36.7634 - val_acc: 0.0645\n",
            "Epoch 55/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 8.7357 - acc: 0.1371 - val_loss: 31.1379 - val_acc: 0.0484\n",
            "Epoch 56/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.3463 - acc: 0.1129 - val_loss: 28.4251 - val_acc: 0.0484\n",
            "Epoch 57/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 6.4293 - acc: 0.1290 - val_loss: 27.8097 - val_acc: 0.0323\n",
            "Epoch 58/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 5.4919 - acc: 0.1331 - val_loss: 28.4020 - val_acc: 0.0323\n",
            "Epoch 59/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 7.8702 - acc: 0.0766 - val_loss: 29.0911 - val_acc: 0.0484\n",
            "Epoch 60/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 5.8913 - acc: 0.1048 - val_loss: 29.7630 - val_acc: 0.0323\n",
            "Epoch 61/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 5.9618 - acc: 0.1169 - val_loss: 29.1860 - val_acc: 0.0323\n",
            "Epoch 62/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 5.6714 - acc: 0.1371 - val_loss: 30.8841 - val_acc: 0.0323\n",
            "Epoch 63/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 4.4897 - acc: 0.1694 - val_loss: 29.2372 - val_acc: 0.0323\n",
            "Epoch 64/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.5668 - acc: 0.1169 - val_loss: 28.5669 - val_acc: 0.0484\n",
            "Epoch 65/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 4.5552 - acc: 0.1250 - val_loss: 29.2542 - val_acc: 0.0323\n",
            "Epoch 66/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 7.9118 - acc: 0.0968 - val_loss: 28.1572 - val_acc: 0.0323\n",
            "Epoch 67/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 4.5572 - acc: 0.1895 - val_loss: 29.1948 - val_acc: 0.0323\n",
            "Epoch 68/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 6.8089 - acc: 0.1411 - val_loss: 30.1683 - val_acc: 0.0323\n",
            "Epoch 69/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.0502 - acc: 0.1734 - val_loss: 28.6315 - val_acc: 0.0484\n",
            "Epoch 70/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 4.6092 - acc: 0.1573 - val_loss: 32.1940 - val_acc: 0.0323\n",
            "Epoch 71/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 4.2916 - acc: 0.1935 - val_loss: 29.1177 - val_acc: 0.0161\n",
            "Epoch 72/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 6.1610 - acc: 0.1734 - val_loss: 28.9219 - val_acc: 0.0323\n",
            "Epoch 73/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 5.9742 - acc: 0.1169 - val_loss: 29.0231 - val_acc: 0.0161\n",
            "Epoch 74/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 5.7324 - acc: 0.1250 - val_loss: 29.5926 - val_acc: 0.0161\n",
            "Epoch 75/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 6.7558 - acc: 0.1089 - val_loss: 27.6353 - val_acc: 0.0323\n",
            "Epoch 76/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 5.5011 - acc: 0.1331 - val_loss: 28.7255 - val_acc: 0.0323\n",
            "Epoch 77/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 6.8179 - acc: 0.1129 - val_loss: 28.7355 - val_acc: 0.0484\n",
            "Epoch 78/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.9640 - acc: 0.1210 - val_loss: 29.0435 - val_acc: 0.0323\n",
            "Epoch 79/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 5.2782 - acc: 0.0968 - val_loss: 28.9032 - val_acc: 0.0484\n",
            "Epoch 80/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 4.4021 - acc: 0.1290 - val_loss: 29.2993 - val_acc: 0.0161\n",
            "Epoch 81/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 7.3067 - acc: 0.1290 - val_loss: 28.9861 - val_acc: 0.0484\n",
            "Epoch 82/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.9971 - acc: 0.1411 - val_loss: 29.0895 - val_acc: 0.0161\n",
            "Epoch 83/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 5.3391 - acc: 0.1492 - val_loss: 32.8938 - val_acc: 0.0161\n",
            "Epoch 84/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 6.4067 - acc: 0.1250 - val_loss: 29.9686 - val_acc: 0.0161\n",
            "Epoch 85/1000\n",
            "248/248 [==============================] - 0s 86us/step - loss: 5.3959 - acc: 0.1653 - val_loss: 27.6286 - val_acc: 0.0161\n",
            "Epoch 86/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 5.5583 - acc: 0.1089 - val_loss: 30.2702 - val_acc: 0.0323\n",
            "Epoch 87/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 6.0746 - acc: 0.1371 - val_loss: 28.3029 - val_acc: 0.0323\n",
            "Epoch 88/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 5.9021 - acc: 0.1573 - val_loss: 28.1933 - val_acc: 0.0323\n",
            "Epoch 89/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.7884 - acc: 0.2097 - val_loss: 28.9092 - val_acc: 0.0161\n",
            "Epoch 90/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.3593 - acc: 0.1371 - val_loss: 28.4334 - val_acc: 0.0161\n",
            "Epoch 91/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 4.1232 - acc: 0.2016 - val_loss: 28.9840 - val_acc: 0.0161\n",
            "Epoch 92/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 5.0662 - acc: 0.1492 - val_loss: 28.5837 - val_acc: 0.0161\n",
            "Epoch 93/1000\n",
            "248/248 [==============================] - 0s 93us/step - loss: 4.4544 - acc: 0.1250 - val_loss: 32.3363 - val_acc: 0.0484\n",
            "Epoch 94/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 6.5578 - acc: 0.1048 - val_loss: 28.9480 - val_acc: 0.0161\n",
            "Epoch 95/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 3.6855 - acc: 0.1935 - val_loss: 31.4501 - val_acc: 0.0161\n",
            "Epoch 96/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 6.2984 - acc: 0.1129 - val_loss: 27.9249 - val_acc: 0.0161\n",
            "Epoch 97/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 4.3082 - acc: 0.1089 - val_loss: 27.9017 - val_acc: 0.0323\n",
            "Epoch 98/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 4.3551 - acc: 0.1089 - val_loss: 29.6561 - val_acc: 0.0161\n",
            "Epoch 99/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 6.1950 - acc: 0.1089 - val_loss: 28.3172 - val_acc: 0.0323\n",
            "Epoch 100/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 8.0984 - acc: 0.0766 - val_loss: 29.9259 - val_acc: 0.0161\n",
            "Epoch 101/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 4.2013 - acc: 0.1694 - val_loss: 28.9795 - val_acc: 0.0161\n",
            "Epoch 102/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 15.5233 - acc: 0.0847 - val_loss: 30.6565 - val_acc: 0.0645\n",
            "Epoch 103/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 11.9173 - acc: 0.0565 - val_loss: 40.7501 - val_acc: 0.0323\n",
            "Epoch 104/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 11.9358 - acc: 0.0524 - val_loss: 30.9461 - val_acc: 0.0323\n",
            "Epoch 105/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 12.8720 - acc: 0.0726 - val_loss: 31.5339 - val_acc: 0.0645\n",
            "Epoch 106/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 9.7879 - acc: 0.0524 - val_loss: 33.5102 - val_acc: 0.0484\n",
            "Epoch 107/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 12.5912 - acc: 0.0887 - val_loss: 33.4681 - val_acc: 0.0484\n",
            "Epoch 108/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 11.6128 - acc: 0.0766 - val_loss: 36.1487 - val_acc: 0.0484\n",
            "Epoch 109/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 9.5440 - acc: 0.0726 - val_loss: 34.2157 - val_acc: 0.0645\n",
            "Epoch 110/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 12.6169 - acc: 0.0484 - val_loss: 33.4186 - val_acc: 0.0645\n",
            "Epoch 111/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 11.0568 - acc: 0.0847 - val_loss: 34.7598 - val_acc: 0.0645\n",
            "Epoch 112/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 10.6762 - acc: 0.0806 - val_loss: 32.7857 - val_acc: 0.0484\n",
            "Epoch 113/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 7.6644 - acc: 0.1371 - val_loss: 31.1089 - val_acc: 0.0484\n",
            "Epoch 114/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 8.0975 - acc: 0.1371 - val_loss: 31.6071 - val_acc: 0.0484\n",
            "Epoch 115/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 8.6574 - acc: 0.1532 - val_loss: 32.4142 - val_acc: 0.0323\n",
            "Epoch 116/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 4.8640 - acc: 0.1250 - val_loss: 34.7094 - val_acc: 0.0323\n",
            "Epoch 117/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 8.8463 - acc: 0.1210 - val_loss: 30.6647 - val_acc: 0.0323\n",
            "Epoch 118/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 6.0593 - acc: 0.1411 - val_loss: 32.0451 - val_acc: 0.0484\n",
            "Epoch 119/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 6.4803 - acc: 0.0968 - val_loss: 32.5819 - val_acc: 0.0484\n",
            "Epoch 120/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 6.6010 - acc: 0.1331 - val_loss: 28.1831 - val_acc: 0.0484\n",
            "Epoch 121/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 4.9632 - acc: 0.1492 - val_loss: 28.3916 - val_acc: 0.0484\n",
            "Epoch 122/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 4.8847 - acc: 0.1371 - val_loss: 32.0889 - val_acc: 0.0484\n",
            "Epoch 123/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 6.1195 - acc: 0.1089 - val_loss: 28.0880 - val_acc: 0.0323\n",
            "Epoch 124/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 6.0411 - acc: 0.1129 - val_loss: 28.2286 - val_acc: 0.0484\n",
            "Epoch 125/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 8.2291 - acc: 0.1411 - val_loss: 27.8483 - val_acc: 0.0161\n",
            "Epoch 126/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 7.7989 - acc: 0.1089 - val_loss: 27.5977 - val_acc: 0.0323\n",
            "Epoch 127/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 4.0832 - acc: 0.1774 - val_loss: 27.8950 - val_acc: 0.0161\n",
            "Epoch 128/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 5.6665 - acc: 0.1129 - val_loss: 29.0076 - val_acc: 0.0323\n",
            "Epoch 129/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 5.8639 - acc: 0.1089 - val_loss: 27.9266 - val_acc: 0.0323\n",
            "Epoch 130/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 8.1323 - acc: 0.0927 - val_loss: 28.8098 - val_acc: 0.0484\n",
            "Epoch 131/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 4.8503 - acc: 0.1089 - val_loss: 27.4098 - val_acc: 0.0323\n",
            "Epoch 132/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 6.4238 - acc: 0.1048 - val_loss: 28.4862 - val_acc: 0.0323\n",
            "Epoch 133/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.9238 - acc: 0.1452 - val_loss: 28.7145 - val_acc: 0.0323\n",
            "Epoch 134/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.9929 - acc: 0.1452 - val_loss: 27.9763 - val_acc: 0.0323\n",
            "Epoch 135/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 5.5376 - acc: 0.1129 - val_loss: 28.3091 - val_acc: 0.0161\n",
            "Epoch 136/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 4.3247 - acc: 0.1573 - val_loss: 27.8103 - val_acc: 0.0161\n",
            "Epoch 137/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.0254 - acc: 0.1210 - val_loss: 28.1810 - val_acc: 0.0323\n",
            "Epoch 138/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 5.1709 - acc: 0.1411 - val_loss: 28.1738 - val_acc: 0.0161\n",
            "Epoch 139/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.4243 - acc: 0.1129 - val_loss: 27.9903 - val_acc: 0.0161\n",
            "Epoch 140/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.7235 - acc: 0.1129 - val_loss: 27.2974 - val_acc: 0.0484\n",
            "Epoch 141/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 4.6897 - acc: 0.1774 - val_loss: 28.3877 - val_acc: 0.0161\n",
            "Epoch 142/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.2271 - acc: 0.1250 - val_loss: 27.7461 - val_acc: 0.0161\n",
            "Epoch 143/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 5.3707 - acc: 0.1008 - val_loss: 28.0959 - val_acc: 0.0161\n",
            "Epoch 144/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 4.5581 - acc: 0.1492 - val_loss: 27.1103 - val_acc: 0.0323\n",
            "Epoch 145/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 6.2290 - acc: 0.0806 - val_loss: 33.1648 - val_acc: 0.0323\n",
            "Epoch 146/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 4.9772 - acc: 0.1734 - val_loss: 27.3342 - val_acc: 0.0484\n",
            "Epoch 147/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 5.5640 - acc: 0.1129 - val_loss: 27.7651 - val_acc: 0.0323\n",
            "Epoch 148/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 4.1013 - acc: 0.1371 - val_loss: 26.9062 - val_acc: 0.0323\n",
            "Epoch 149/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.3108 - acc: 0.1411 - val_loss: 30.4164 - val_acc: 0.0323\n",
            "Epoch 150/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 4.7500 - acc: 0.1371 - val_loss: 26.7049 - val_acc: 0.0323\n",
            "Epoch 151/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 5.8358 - acc: 0.1008 - val_loss: 28.5485 - val_acc: 0.0323\n",
            "Epoch 152/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 4.0198 - acc: 0.1210 - val_loss: 26.9384 - val_acc: 0.0161\n",
            "Epoch 153/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 6.7568 - acc: 0.1008 - val_loss: 29.8716 - val_acc: 0.0323\n",
            "Epoch 154/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 4.6864 - acc: 0.1492 - val_loss: 28.3204 - val_acc: 0.0161\n",
            "Epoch 155/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 3.7433 - acc: 0.1573 - val_loss: 28.4503 - val_acc: 0.0161\n",
            "Epoch 156/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 4.8392 - acc: 0.1290 - val_loss: 28.7215 - val_acc: 0.0323\n",
            "Epoch 157/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.5478 - acc: 0.1855 - val_loss: 28.4249 - val_acc: 0.0161\n",
            "Epoch 158/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 6.6936 - acc: 0.1290 - val_loss: 39.6466 - val_acc: 0.0161\n",
            "Epoch 159/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 6.7366 - acc: 0.1169 - val_loss: 27.9380 - val_acc: 0.0161\n",
            "Epoch 160/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.9360 - acc: 0.1734 - val_loss: 27.9779 - val_acc: 0.0323\n",
            "Epoch 161/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 4.8541 - acc: 0.1169 - val_loss: 27.7478 - val_acc: 0.0323\n",
            "Epoch 162/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 4.5513 - acc: 0.1573 - val_loss: 27.4658 - val_acc: 0.0323\n",
            "Epoch 163/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 4.0879 - acc: 0.1653 - val_loss: 26.9857 - val_acc: 0.0323\n",
            "Epoch 164/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 4.9056 - acc: 0.1169 - val_loss: 28.0655 - val_acc: 0.0323\n",
            "Epoch 165/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 3.4304 - acc: 0.1492 - val_loss: 27.7359 - val_acc: 0.0323\n",
            "Epoch 166/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 5.4337 - acc: 0.1048 - val_loss: 27.7236 - val_acc: 0.0323\n",
            "Epoch 167/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 4.0381 - acc: 0.1573 - val_loss: 27.2444 - val_acc: 0.0161\n",
            "Epoch 168/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.1480 - acc: 0.1935 - val_loss: 26.6014 - val_acc: 0.0323\n",
            "Epoch 169/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 6.0868 - acc: 0.0887 - val_loss: 30.5135 - val_acc: 0.0323\n",
            "Epoch 170/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 4.9369 - acc: 0.1532 - val_loss: 30.8209 - val_acc: 0.0000e+00\n",
            "Epoch 171/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 6.6568 - acc: 0.1411 - val_loss: 28.1598 - val_acc: 0.0161\n",
            "Epoch 172/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 4.6170 - acc: 0.1492 - val_loss: 27.3666 - val_acc: 0.0161\n",
            "Epoch 173/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.3810 - acc: 0.1734 - val_loss: 27.8432 - val_acc: 0.0161\n",
            "Epoch 174/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 3.5356 - acc: 0.1976 - val_loss: 27.3461 - val_acc: 0.0161\n",
            "Epoch 175/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 3.4017 - acc: 0.1935 - val_loss: 25.8636 - val_acc: 0.0323\n",
            "Epoch 176/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 4.3927 - acc: 0.1694 - val_loss: 28.2074 - val_acc: 0.0161\n",
            "Epoch 177/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.8245 - acc: 0.1815 - val_loss: 27.4812 - val_acc: 0.0161\n",
            "Epoch 178/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 4.0735 - acc: 0.1492 - val_loss: 27.0721 - val_acc: 0.0323\n",
            "Epoch 179/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 4.1209 - acc: 0.1774 - val_loss: 25.4798 - val_acc: 0.0323\n",
            "Epoch 180/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 3.7342 - acc: 0.1694 - val_loss: 27.3505 - val_acc: 0.0161\n",
            "Epoch 181/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.7850 - acc: 0.1855 - val_loss: 26.7931 - val_acc: 0.0323\n",
            "Epoch 182/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.3502 - acc: 0.2056 - val_loss: 25.4287 - val_acc: 0.0161\n",
            "Epoch 183/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.8858 - acc: 0.1694 - val_loss: 28.1835 - val_acc: 0.0161\n",
            "Epoch 184/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.0677 - acc: 0.2298 - val_loss: 27.7548 - val_acc: 0.0323\n",
            "Epoch 185/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 4.8126 - acc: 0.1169 - val_loss: 26.5534 - val_acc: 0.0323\n",
            "Epoch 186/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.6294 - acc: 0.1411 - val_loss: 27.6959 - val_acc: 0.0323\n",
            "Epoch 187/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 3.5062 - acc: 0.1855 - val_loss: 26.2414 - val_acc: 0.0484\n",
            "Epoch 188/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.4518 - acc: 0.1935 - val_loss: 27.4813 - val_acc: 0.0323\n",
            "Epoch 189/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.8004 - acc: 0.1613 - val_loss: 30.9172 - val_acc: 0.0000e+00\n",
            "Epoch 190/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 3.6908 - acc: 0.1653 - val_loss: 25.6910 - val_acc: 0.0161\n",
            "Epoch 191/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 3.6816 - acc: 0.1653 - val_loss: 26.7887 - val_acc: 0.0161\n",
            "Epoch 192/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.2654 - acc: 0.1976 - val_loss: 27.3424 - val_acc: 0.0161\n",
            "Epoch 193/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 3.6219 - acc: 0.1734 - val_loss: 28.2292 - val_acc: 0.0000e+00\n",
            "Epoch 194/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 3.7336 - acc: 0.1613 - val_loss: 29.8404 - val_acc: 0.0000e+00\n",
            "Epoch 195/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 4.4099 - acc: 0.1573 - val_loss: 29.1553 - val_acc: 0.0161\n",
            "Epoch 196/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.4517 - acc: 0.1855 - val_loss: 25.6585 - val_acc: 0.0161\n",
            "Epoch 197/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.6866 - acc: 0.2339 - val_loss: 26.2699 - val_acc: 0.0323\n",
            "Epoch 198/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 3.6136 - acc: 0.1815 - val_loss: 27.0301 - val_acc: 0.0000e+00\n",
            "Epoch 199/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.8172 - acc: 0.2419 - val_loss: 28.3559 - val_acc: 0.0161\n",
            "Epoch 200/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 3.6201 - acc: 0.1734 - val_loss: 25.2584 - val_acc: 0.0000e+00\n",
            "Epoch 201/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.7447 - acc: 0.1734 - val_loss: 25.7057 - val_acc: 0.0000e+00\n",
            "Epoch 202/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 3.5096 - acc: 0.1935 - val_loss: 26.7767 - val_acc: 0.0161\n",
            "Epoch 203/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.4219 - acc: 0.2056 - val_loss: 28.3529 - val_acc: 0.0000e+00\n",
            "Epoch 204/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.3431 - acc: 0.1855 - val_loss: 25.2105 - val_acc: 0.0000e+00\n",
            "Epoch 205/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.3446 - acc: 0.2097 - val_loss: 29.0200 - val_acc: 0.0161\n",
            "Epoch 206/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.7656 - acc: 0.1935 - val_loss: 26.0495 - val_acc: 0.0000e+00\n",
            "Epoch 207/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.1317 - acc: 0.1895 - val_loss: 25.5452 - val_acc: 0.0161\n",
            "Epoch 208/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.8487 - acc: 0.1532 - val_loss: 25.1265 - val_acc: 0.0161\n",
            "Epoch 209/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.4822 - acc: 0.1653 - val_loss: 27.2490 - val_acc: 0.0161\n",
            "Epoch 210/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 3.0029 - acc: 0.2177 - val_loss: 29.0634 - val_acc: 0.0161\n",
            "Epoch 211/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.9460 - acc: 0.2258 - val_loss: 24.4275 - val_acc: 0.0161\n",
            "Epoch 212/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.4122 - acc: 0.1855 - val_loss: 24.5480 - val_acc: 0.0323\n",
            "Epoch 213/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 3.4861 - acc: 0.1895 - val_loss: 25.7871 - val_acc: 0.0161\n",
            "Epoch 214/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.7285 - acc: 0.2258 - val_loss: 25.5137 - val_acc: 0.0161\n",
            "Epoch 215/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.8612 - acc: 0.2460 - val_loss: 24.5935 - val_acc: 0.0323\n",
            "Epoch 216/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.7421 - acc: 0.2137 - val_loss: 25.8323 - val_acc: 0.0161\n",
            "Epoch 217/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 3.0598 - acc: 0.2258 - val_loss: 24.6781 - val_acc: 0.0323\n",
            "Epoch 218/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.7563 - acc: 0.2258 - val_loss: 24.7789 - val_acc: 0.0323\n",
            "Epoch 219/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 3.4659 - acc: 0.1976 - val_loss: 24.3766 - val_acc: 0.0323\n",
            "Epoch 220/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.0353 - acc: 0.2056 - val_loss: 28.9704 - val_acc: 0.0161\n",
            "Epoch 221/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.9955 - acc: 0.2177 - val_loss: 23.9277 - val_acc: 0.0161\n",
            "Epoch 222/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 3.7708 - acc: 0.1573 - val_loss: 23.6232 - val_acc: 0.0323\n",
            "Epoch 223/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.9647 - acc: 0.2258 - val_loss: 23.8537 - val_acc: 0.0484\n",
            "Epoch 224/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.8609 - acc: 0.2177 - val_loss: 32.0003 - val_acc: 0.0161\n",
            "Epoch 225/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 4.7636 - acc: 0.1895 - val_loss: 24.5447 - val_acc: 0.0161\n",
            "Epoch 226/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.3178 - acc: 0.2016 - val_loss: 23.4192 - val_acc: 0.0323\n",
            "Epoch 227/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.9548 - acc: 0.1855 - val_loss: 23.5987 - val_acc: 0.0323\n",
            "Epoch 228/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.5031 - acc: 0.1895 - val_loss: 25.2509 - val_acc: 0.0484\n",
            "Epoch 229/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.6087 - acc: 0.2218 - val_loss: 25.8205 - val_acc: 0.0645\n",
            "Epoch 230/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.1696 - acc: 0.2137 - val_loss: 26.0691 - val_acc: 0.0645\n",
            "Epoch 231/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.0853 - acc: 0.1855 - val_loss: 24.6741 - val_acc: 0.0645\n",
            "Epoch 232/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.6887 - acc: 0.2097 - val_loss: 24.0103 - val_acc: 0.0645\n",
            "Epoch 233/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.8339 - acc: 0.2500 - val_loss: 23.8967 - val_acc: 0.0484\n",
            "Epoch 234/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.0363 - acc: 0.2177 - val_loss: 27.4264 - val_acc: 0.0806\n",
            "Epoch 235/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.3084 - acc: 0.1613 - val_loss: 22.3026 - val_acc: 0.0968\n",
            "Epoch 236/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.6826 - acc: 0.1613 - val_loss: 22.2217 - val_acc: 0.0806\n",
            "Epoch 237/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.2049 - acc: 0.2379 - val_loss: 21.8292 - val_acc: 0.1129\n",
            "Epoch 238/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 3.4086 - acc: 0.1613 - val_loss: 21.7820 - val_acc: 0.0806\n",
            "Epoch 239/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 3.7624 - acc: 0.1895 - val_loss: 27.5285 - val_acc: 0.0645\n",
            "Epoch 240/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.8048 - acc: 0.1895 - val_loss: 24.1289 - val_acc: 0.0968\n",
            "Epoch 241/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 3.6637 - acc: 0.1895 - val_loss: 23.1153 - val_acc: 0.1290\n",
            "Epoch 242/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.3162 - acc: 0.1774 - val_loss: 22.3460 - val_acc: 0.0968\n",
            "Epoch 243/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.6992 - acc: 0.2500 - val_loss: 29.8016 - val_acc: 0.0484\n",
            "Epoch 244/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.5998 - acc: 0.1613 - val_loss: 23.3136 - val_acc: 0.0968\n",
            "Epoch 245/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.4671 - acc: 0.2621 - val_loss: 22.8631 - val_acc: 0.0645\n",
            "Epoch 246/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.0732 - acc: 0.1573 - val_loss: 31.1475 - val_acc: 0.0484\n",
            "Epoch 247/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 4.3230 - acc: 0.1734 - val_loss: 22.4891 - val_acc: 0.0968\n",
            "Epoch 248/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.6850 - acc: 0.1855 - val_loss: 22.7719 - val_acc: 0.0323\n",
            "Epoch 249/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.5449 - acc: 0.2339 - val_loss: 23.5909 - val_acc: 0.0645\n",
            "Epoch 250/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.8087 - acc: 0.2056 - val_loss: 24.3226 - val_acc: 0.0806\n",
            "Epoch 251/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.6016 - acc: 0.1492 - val_loss: 23.6861 - val_acc: 0.0968\n",
            "Epoch 252/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.6457 - acc: 0.2177 - val_loss: 24.6377 - val_acc: 0.0806\n",
            "Epoch 253/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.9715 - acc: 0.2298 - val_loss: 21.9421 - val_acc: 0.0806\n",
            "Epoch 254/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.6985 - acc: 0.1976 - val_loss: 21.5713 - val_acc: 0.0968\n",
            "Epoch 255/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 3.1159 - acc: 0.2137 - val_loss: 26.0556 - val_acc: 0.0806\n",
            "Epoch 256/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.9326 - acc: 0.1734 - val_loss: 20.7645 - val_acc: 0.0645\n",
            "Epoch 257/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.1262 - acc: 0.1855 - val_loss: 27.1187 - val_acc: 0.0645\n",
            "Epoch 258/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.8680 - acc: 0.1976 - val_loss: 22.7448 - val_acc: 0.0806\n",
            "Epoch 259/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.5066 - acc: 0.1613 - val_loss: 20.8644 - val_acc: 0.0968\n",
            "Epoch 260/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.7717 - acc: 0.2460 - val_loss: 21.4644 - val_acc: 0.0645\n",
            "Epoch 261/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.7473 - acc: 0.2177 - val_loss: 30.1622 - val_acc: 0.0645\n",
            "Epoch 262/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 4.2823 - acc: 0.1694 - val_loss: 24.2979 - val_acc: 0.0806\n",
            "Epoch 263/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 3.2565 - acc: 0.1815 - val_loss: 23.0999 - val_acc: 0.0806\n",
            "Epoch 264/1000\n",
            "248/248 [==============================] - 0s 91us/step - loss: 2.3636 - acc: 0.2944 - val_loss: 21.5045 - val_acc: 0.0806\n",
            "Epoch 265/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.2794 - acc: 0.1976 - val_loss: 22.8322 - val_acc: 0.0645\n",
            "Epoch 266/1000\n",
            "248/248 [==============================] - 0s 109us/step - loss: 2.2232 - acc: 0.2460 - val_loss: 22.8249 - val_acc: 0.0806\n",
            "Epoch 267/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.4071 - acc: 0.2540 - val_loss: 23.5121 - val_acc: 0.0806\n",
            "Epoch 268/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.5424 - acc: 0.2298 - val_loss: 22.0907 - val_acc: 0.0645\n",
            "Epoch 269/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 3.0727 - acc: 0.1815 - val_loss: 23.8473 - val_acc: 0.0645\n",
            "Epoch 270/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.9353 - acc: 0.2137 - val_loss: 21.2704 - val_acc: 0.1129\n",
            "Epoch 271/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.5892 - acc: 0.2056 - val_loss: 21.3556 - val_acc: 0.0968\n",
            "Epoch 272/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.6411 - acc: 0.1815 - val_loss: 21.8495 - val_acc: 0.0968\n",
            "Epoch 273/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.6183 - acc: 0.2258 - val_loss: 21.1915 - val_acc: 0.0968\n",
            "Epoch 274/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.2333 - acc: 0.2379 - val_loss: 21.4538 - val_acc: 0.0968\n",
            "Epoch 275/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.1280 - acc: 0.2500 - val_loss: 21.1644 - val_acc: 0.0968\n",
            "Epoch 276/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 2.6610 - acc: 0.2298 - val_loss: 20.7127 - val_acc: 0.0968\n",
            "Epoch 277/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.4477 - acc: 0.1976 - val_loss: 24.6591 - val_acc: 0.0806\n",
            "Epoch 278/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.5499 - acc: 0.2016 - val_loss: 20.8145 - val_acc: 0.0968\n",
            "Epoch 279/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.7211 - acc: 0.2460 - val_loss: 20.7153 - val_acc: 0.1290\n",
            "Epoch 280/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.3249 - acc: 0.2540 - val_loss: 22.6998 - val_acc: 0.1129\n",
            "Epoch 281/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2288 - acc: 0.2460 - val_loss: 23.8288 - val_acc: 0.0645\n",
            "Epoch 282/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.4720 - acc: 0.2379 - val_loss: 20.7924 - val_acc: 0.1452\n",
            "Epoch 283/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.3337 - acc: 0.2097 - val_loss: 23.1301 - val_acc: 0.0645\n",
            "Epoch 284/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.2660 - acc: 0.2218 - val_loss: 21.6155 - val_acc: 0.0645\n",
            "Epoch 285/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.6733 - acc: 0.2339 - val_loss: 18.7647 - val_acc: 0.1129\n",
            "Epoch 286/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2272 - acc: 0.2823 - val_loss: 21.4639 - val_acc: 0.0484\n",
            "Epoch 287/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2084 - acc: 0.2419 - val_loss: 24.3955 - val_acc: 0.0484\n",
            "Epoch 288/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.1193 - acc: 0.2097 - val_loss: 22.7147 - val_acc: 0.0645\n",
            "Epoch 289/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.9990 - acc: 0.2863 - val_loss: 21.6435 - val_acc: 0.1129\n",
            "Epoch 290/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.8557 - acc: 0.2137 - val_loss: 26.8897 - val_acc: 0.0806\n",
            "Epoch 291/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2880 - acc: 0.2500 - val_loss: 21.6999 - val_acc: 0.0968\n",
            "Epoch 292/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 2.4238 - acc: 0.2500 - val_loss: 19.4457 - val_acc: 0.0806\n",
            "Epoch 293/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.7700 - acc: 0.2419 - val_loss: 21.6047 - val_acc: 0.0645\n",
            "Epoch 294/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.6262 - acc: 0.2500 - val_loss: 25.4311 - val_acc: 0.0645\n",
            "Epoch 295/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.1151 - acc: 0.2298 - val_loss: 23.6164 - val_acc: 0.0645\n",
            "Epoch 296/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 3.1439 - acc: 0.1895 - val_loss: 24.3719 - val_acc: 0.0645\n",
            "Epoch 297/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.7046 - acc: 0.2097 - val_loss: 20.9092 - val_acc: 0.0806\n",
            "Epoch 298/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.6742 - acc: 0.2177 - val_loss: 20.8259 - val_acc: 0.1290\n",
            "Epoch 299/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.7792 - acc: 0.2056 - val_loss: 21.5378 - val_acc: 0.0484\n",
            "Epoch 300/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.1906 - acc: 0.2540 - val_loss: 21.2802 - val_acc: 0.0323\n",
            "Epoch 301/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.1623 - acc: 0.2258 - val_loss: 23.5078 - val_acc: 0.0645\n",
            "Epoch 302/1000\n",
            "248/248 [==============================] - 0s 89us/step - loss: 3.0288 - acc: 0.2097 - val_loss: 21.5418 - val_acc: 0.0645\n",
            "Epoch 303/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0024 - acc: 0.2944 - val_loss: 21.1503 - val_acc: 0.0968\n",
            "Epoch 304/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.3568 - acc: 0.1734 - val_loss: 20.9879 - val_acc: 0.0968\n",
            "Epoch 305/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.9500 - acc: 0.2218 - val_loss: 21.4805 - val_acc: 0.0968\n",
            "Epoch 306/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.4236 - acc: 0.2500 - val_loss: 20.8446 - val_acc: 0.0645\n",
            "Epoch 307/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.2929 - acc: 0.2177 - val_loss: 22.3887 - val_acc: 0.0645\n",
            "Epoch 308/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.0775 - acc: 0.2702 - val_loss: 19.4437 - val_acc: 0.0968\n",
            "Epoch 309/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.3933 - acc: 0.2298 - val_loss: 20.1834 - val_acc: 0.0645\n",
            "Epoch 310/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.6575 - acc: 0.2097 - val_loss: 20.5610 - val_acc: 0.0806\n",
            "Epoch 311/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.0367 - acc: 0.1935 - val_loss: 21.9227 - val_acc: 0.0484\n",
            "Epoch 312/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.5702 - acc: 0.1976 - val_loss: 20.9739 - val_acc: 0.1129\n",
            "Epoch 313/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.2874 - acc: 0.2016 - val_loss: 22.2752 - val_acc: 0.0645\n",
            "Epoch 314/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.3257 - acc: 0.2621 - val_loss: 24.2139 - val_acc: 0.0806\n",
            "Epoch 315/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.1862 - acc: 0.2298 - val_loss: 21.9397 - val_acc: 0.0645\n",
            "Epoch 316/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.2450 - acc: 0.2500 - val_loss: 31.0487 - val_acc: 0.0484\n",
            "Epoch 317/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.5717 - acc: 0.2218 - val_loss: 20.6755 - val_acc: 0.0645\n",
            "Epoch 318/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.9943 - acc: 0.2581 - val_loss: 20.2978 - val_acc: 0.0484\n",
            "Epoch 319/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.0439 - acc: 0.2339 - val_loss: 21.9755 - val_acc: 0.0000e+00\n",
            "Epoch 320/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0076 - acc: 0.2742 - val_loss: 22.8309 - val_acc: 0.0968\n",
            "Epoch 321/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.8017 - acc: 0.1613 - val_loss: 21.3002 - val_acc: 0.0484\n",
            "Epoch 322/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.8467 - acc: 0.1855 - val_loss: 22.0599 - val_acc: 0.0000e+00\n",
            "Epoch 323/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.9133 - acc: 0.2339 - val_loss: 21.1958 - val_acc: 0.0161\n",
            "Epoch 324/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.0542 - acc: 0.2782 - val_loss: 25.1113 - val_acc: 0.0806\n",
            "Epoch 325/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.2710 - acc: 0.2097 - val_loss: 20.2832 - val_acc: 0.1129\n",
            "Epoch 326/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 2.1724 - acc: 0.2823 - val_loss: 21.0471 - val_acc: 0.0968\n",
            "Epoch 327/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.1176 - acc: 0.2500 - val_loss: 20.2143 - val_acc: 0.0806\n",
            "Epoch 328/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.0374 - acc: 0.2702 - val_loss: 22.5845 - val_acc: 0.0161\n",
            "Epoch 329/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.6193 - acc: 0.2177 - val_loss: 24.6389 - val_acc: 0.0323\n",
            "Epoch 330/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.4373 - acc: 0.2097 - val_loss: 21.1374 - val_acc: 0.0484\n",
            "Epoch 331/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0690 - acc: 0.2419 - val_loss: 21.5593 - val_acc: 0.0806\n",
            "Epoch 332/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.3418 - acc: 0.2460 - val_loss: 21.4311 - val_acc: 0.0484\n",
            "Epoch 333/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.9965 - acc: 0.2258 - val_loss: 20.0436 - val_acc: 0.0645\n",
            "Epoch 334/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.1264 - acc: 0.2661 - val_loss: 20.2307 - val_acc: 0.0323\n",
            "Epoch 335/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.0456 - acc: 0.2339 - val_loss: 21.5691 - val_acc: 0.0000e+00\n",
            "Epoch 336/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.4221 - acc: 0.2621 - val_loss: 21.2728 - val_acc: 0.0000e+00\n",
            "Epoch 337/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.9929 - acc: 0.2177 - val_loss: 20.5868 - val_acc: 0.0161\n",
            "Epoch 338/1000\n",
            "248/248 [==============================] - 0s 46us/step - loss: 2.1582 - acc: 0.2177 - val_loss: 20.1877 - val_acc: 0.0323\n",
            "Epoch 339/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.6807 - acc: 0.2258 - val_loss: 20.9557 - val_acc: 0.0484\n",
            "Epoch 340/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.6100 - acc: 0.2056 - val_loss: 20.5654 - val_acc: 0.0484\n",
            "Epoch 341/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.9313 - acc: 0.2823 - val_loss: 21.2336 - val_acc: 0.0484\n",
            "Epoch 342/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.5443 - acc: 0.2298 - val_loss: 19.2127 - val_acc: 0.0484\n",
            "Epoch 343/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 2.0376 - acc: 0.2419 - val_loss: 23.6584 - val_acc: 0.0968\n",
            "Epoch 344/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.3508 - acc: 0.2258 - val_loss: 20.6141 - val_acc: 0.0645\n",
            "Epoch 345/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.8631 - acc: 0.2782 - val_loss: 23.6096 - val_acc: 0.0323\n",
            "Epoch 346/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.3256 - acc: 0.2621 - val_loss: 20.7470 - val_acc: 0.0000e+00\n",
            "Epoch 347/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.9425 - acc: 0.2581 - val_loss: 21.2056 - val_acc: 0.0484\n",
            "Epoch 348/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 2.1590 - acc: 0.2702 - val_loss: 19.1949 - val_acc: 0.0484\n",
            "Epoch 349/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.8727 - acc: 0.2702 - val_loss: 21.2511 - val_acc: 0.0161\n",
            "Epoch 350/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.9319 - acc: 0.2540 - val_loss: 21.1730 - val_acc: 0.0323\n",
            "Epoch 351/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.1427 - acc: 0.2016 - val_loss: 21.6210 - val_acc: 0.0161\n",
            "Epoch 352/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.8934 - acc: 0.2500 - val_loss: 24.8053 - val_acc: 0.0645\n",
            "Epoch 353/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.9382 - acc: 0.1815 - val_loss: 21.3150 - val_acc: 0.0645\n",
            "Epoch 354/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.8694 - acc: 0.2581 - val_loss: 21.9993 - val_acc: 0.0645\n",
            "Epoch 355/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.9020 - acc: 0.2460 - val_loss: 19.8659 - val_acc: 0.0161\n",
            "Epoch 356/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.8740 - acc: 0.2621 - val_loss: 19.1662 - val_acc: 0.0323\n",
            "Epoch 357/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 3.8456 - acc: 0.1895 - val_loss: 17.7556 - val_acc: 0.0323\n",
            "Epoch 358/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 2.2966 - acc: 0.2944 - val_loss: 22.0298 - val_acc: 0.0645\n",
            "Epoch 359/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.9057 - acc: 0.2540 - val_loss: 20.2871 - val_acc: 0.0000e+00\n",
            "Epoch 360/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.8922 - acc: 0.2823 - val_loss: 24.3400 - val_acc: 0.0806\n",
            "Epoch 361/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.1175 - acc: 0.2540 - val_loss: 23.5145 - val_acc: 0.0806\n",
            "Epoch 362/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0032 - acc: 0.2621 - val_loss: 21.0313 - val_acc: 0.0000e+00\n",
            "Epoch 363/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.8654 - acc: 0.2581 - val_loss: 21.9743 - val_acc: 0.0484\n",
            "Epoch 364/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.8796 - acc: 0.2581 - val_loss: 21.9043 - val_acc: 0.0161\n",
            "Epoch 365/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.4844 - acc: 0.2298 - val_loss: 22.0074 - val_acc: 0.0161\n",
            "Epoch 366/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.1117 - acc: 0.2419 - val_loss: 22.1484 - val_acc: 0.0323\n",
            "Epoch 367/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6898 - acc: 0.2823 - val_loss: 21.5364 - val_acc: 0.0161\n",
            "Epoch 368/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.9211 - acc: 0.2742 - val_loss: 22.8054 - val_acc: 0.0323\n",
            "Epoch 369/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.9485 - acc: 0.2339 - val_loss: 26.4166 - val_acc: 0.0806\n",
            "Epoch 370/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.1612 - acc: 0.2540 - val_loss: 24.3412 - val_acc: 0.0323\n",
            "Epoch 371/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.6629 - acc: 0.2056 - val_loss: 21.3868 - val_acc: 0.0645\n",
            "Epoch 372/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.2118 - acc: 0.2298 - val_loss: 23.1676 - val_acc: 0.0323\n",
            "Epoch 373/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 2.2016 - acc: 0.2097 - val_loss: 23.7609 - val_acc: 0.0323\n",
            "Epoch 374/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.0432 - acc: 0.2581 - val_loss: 23.1059 - val_acc: 0.0806\n",
            "Epoch 375/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.3868 - acc: 0.2379 - val_loss: 21.3308 - val_acc: 0.0161\n",
            "Epoch 376/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.5834 - acc: 0.2056 - val_loss: 23.7090 - val_acc: 0.0323\n",
            "Epoch 377/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.8206 - acc: 0.1976 - val_loss: 24.1476 - val_acc: 0.0645\n",
            "Epoch 378/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.4944 - acc: 0.2056 - val_loss: 24.0165 - val_acc: 0.0000e+00\n",
            "Epoch 379/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.1839 - acc: 0.2016 - val_loss: 24.9170 - val_acc: 0.0645\n",
            "Epoch 380/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.9072 - acc: 0.2782 - val_loss: 25.3072 - val_acc: 0.0323\n",
            "Epoch 381/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 2.1401 - acc: 0.2419 - val_loss: 24.9189 - val_acc: 0.0484\n",
            "Epoch 382/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 2.0199 - acc: 0.2339 - val_loss: 21.2884 - val_acc: 0.0323\n",
            "Epoch 383/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.7172 - acc: 0.2823 - val_loss: 21.3847 - val_acc: 0.0323\n",
            "Epoch 384/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.7901 - acc: 0.2258 - val_loss: 23.0464 - val_acc: 0.0484\n",
            "Epoch 385/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.3619 - acc: 0.1734 - val_loss: 20.7101 - val_acc: 0.0323\n",
            "Epoch 386/1000\n",
            "248/248 [==============================] - 0s 86us/step - loss: 2.2893 - acc: 0.1976 - val_loss: 21.7765 - val_acc: 0.0161\n",
            "Epoch 387/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.8949 - acc: 0.2903 - val_loss: 21.8609 - val_acc: 0.0323\n",
            "Epoch 388/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.6874 - acc: 0.2581 - val_loss: 21.2588 - val_acc: 0.0161\n",
            "Epoch 389/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.8977 - acc: 0.2702 - val_loss: 23.9383 - val_acc: 0.0323\n",
            "Epoch 390/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.9616 - acc: 0.2984 - val_loss: 22.0890 - val_acc: 0.0161\n",
            "Epoch 391/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.9974 - acc: 0.2581 - val_loss: 21.4805 - val_acc: 0.0000e+00\n",
            "Epoch 392/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.2736 - acc: 0.1935 - val_loss: 23.8331 - val_acc: 0.0806\n",
            "Epoch 393/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.8881 - acc: 0.2379 - val_loss: 20.6235 - val_acc: 0.0000e+00\n",
            "Epoch 394/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.6233 - acc: 0.2823 - val_loss: 21.2711 - val_acc: 0.0323\n",
            "Epoch 395/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.7310 - acc: 0.2742 - val_loss: 20.5681 - val_acc: 0.0323\n",
            "Epoch 396/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.7531 - acc: 0.2218 - val_loss: 22.4565 - val_acc: 0.0000e+00\n",
            "Epoch 397/1000\n",
            "248/248 [==============================] - 0s 92us/step - loss: 2.4324 - acc: 0.2339 - val_loss: 20.3290 - val_acc: 0.0323\n",
            "Epoch 398/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.8583 - acc: 0.2500 - val_loss: 22.3096 - val_acc: 0.0645\n",
            "Epoch 399/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.8833 - acc: 0.2621 - val_loss: 20.5521 - val_acc: 0.0323\n",
            "Epoch 400/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.8361 - acc: 0.2944 - val_loss: 20.3021 - val_acc: 0.0323\n",
            "Epoch 401/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.0388 - acc: 0.2258 - val_loss: 30.8885 - val_acc: 0.0645\n",
            "Epoch 402/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.3324 - acc: 0.2581 - val_loss: 27.2903 - val_acc: 0.0484\n",
            "Epoch 403/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.1874 - acc: 0.2298 - val_loss: 21.8212 - val_acc: 0.0000e+00\n",
            "Epoch 404/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.8254 - acc: 0.2218 - val_loss: 22.2468 - val_acc: 0.0161\n",
            "Epoch 405/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.8902 - acc: 0.2661 - val_loss: 22.1537 - val_acc: 0.0484\n",
            "Epoch 406/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.1927 - acc: 0.2177 - val_loss: 22.8098 - val_acc: 0.0161\n",
            "Epoch 407/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.8194 - acc: 0.2702 - val_loss: 20.8716 - val_acc: 0.0484\n",
            "Epoch 408/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.2764 - acc: 0.2258 - val_loss: 22.3139 - val_acc: 0.0000e+00\n",
            "Epoch 409/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.9244 - acc: 0.2540 - val_loss: 21.9912 - val_acc: 0.0484\n",
            "Epoch 410/1000\n",
            "248/248 [==============================] - 0s 91us/step - loss: 1.6846 - acc: 0.2742 - val_loss: 24.0954 - val_acc: 0.0645\n",
            "Epoch 411/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.6395 - acc: 0.2339 - val_loss: 21.8891 - val_acc: 0.0161\n",
            "Epoch 412/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.6426 - acc: 0.2863 - val_loss: 23.3670 - val_acc: 0.0323\n",
            "Epoch 413/1000\n",
            "248/248 [==============================] - 0s 80us/step - loss: 1.7964 - acc: 0.2419 - val_loss: 23.0551 - val_acc: 0.0484\n",
            "Epoch 414/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.9669 - acc: 0.2177 - val_loss: 19.9812 - val_acc: 0.0161\n",
            "Epoch 415/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.8854 - acc: 0.2339 - val_loss: 22.2254 - val_acc: 0.0323\n",
            "Epoch 416/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.9057 - acc: 0.2500 - val_loss: 22.0568 - val_acc: 0.0000e+00\n",
            "Epoch 417/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.5285 - acc: 0.2984 - val_loss: 20.6639 - val_acc: 0.0161\n",
            "Epoch 418/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.8684 - acc: 0.2460 - val_loss: 23.3580 - val_acc: 0.0161\n",
            "Epoch 419/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.8033 - acc: 0.2500 - val_loss: 21.6287 - val_acc: 0.0000e+00\n",
            "Epoch 420/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.6290 - acc: 0.2540 - val_loss: 22.0364 - val_acc: 0.0161\n",
            "Epoch 421/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6514 - acc: 0.2903 - val_loss: 22.5254 - val_acc: 0.0323\n",
            "Epoch 422/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.5877 - acc: 0.2782 - val_loss: 23.8478 - val_acc: 0.0161\n",
            "Epoch 423/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.7504 - acc: 0.2460 - val_loss: 19.8847 - val_acc: 0.0000e+00\n",
            "Epoch 424/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.8877 - acc: 0.2379 - val_loss: 21.7967 - val_acc: 0.0000e+00\n",
            "Epoch 425/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6050 - acc: 0.2782 - val_loss: 21.2103 - val_acc: 0.0323\n",
            "Epoch 426/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.6044 - acc: 0.2500 - val_loss: 23.1027 - val_acc: 0.0000e+00\n",
            "Epoch 427/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 2.7997 - acc: 0.1855 - val_loss: 23.0363 - val_acc: 0.0484\n",
            "Epoch 428/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.7670 - acc: 0.2782 - val_loss: 24.2370 - val_acc: 0.0323\n",
            "Epoch 429/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.6023 - acc: 0.2379 - val_loss: 22.3169 - val_acc: 0.0484\n",
            "Epoch 430/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.6849 - acc: 0.2581 - val_loss: 24.0740 - val_acc: 0.0645\n",
            "Epoch 431/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.6982 - acc: 0.2177 - val_loss: 20.5794 - val_acc: 0.0484\n",
            "Epoch 432/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.8996 - acc: 0.2661 - val_loss: 23.3314 - val_acc: 0.0484\n",
            "Epoch 433/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6757 - acc: 0.2339 - val_loss: 22.4708 - val_acc: 0.0161\n",
            "Epoch 434/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6973 - acc: 0.2823 - val_loss: 22.6002 - val_acc: 0.0645\n",
            "Epoch 435/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.2072 - acc: 0.2661 - val_loss: 22.6673 - val_acc: 0.0000e+00\n",
            "Epoch 436/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.6843 - acc: 0.2540 - val_loss: 22.1049 - val_acc: 0.0323\n",
            "Epoch 437/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6262 - acc: 0.2500 - val_loss: 24.9788 - val_acc: 0.0161\n",
            "Epoch 438/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.7420 - acc: 0.2702 - val_loss: 23.0718 - val_acc: 0.0484\n",
            "Epoch 439/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.8459 - acc: 0.2258 - val_loss: 21.7281 - val_acc: 0.0161\n",
            "Epoch 440/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.6783 - acc: 0.2984 - val_loss: 22.3564 - val_acc: 0.0323\n",
            "Epoch 441/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.6226 - acc: 0.2702 - val_loss: 22.7994 - val_acc: 0.0000e+00\n",
            "Epoch 442/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.7508 - acc: 0.2419 - val_loss: 21.6413 - val_acc: 0.0161\n",
            "Epoch 443/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.8291 - acc: 0.2460 - val_loss: 27.5993 - val_acc: 0.0323\n",
            "Epoch 444/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.6609 - acc: 0.2177 - val_loss: 24.8576 - val_acc: 0.0484\n",
            "Epoch 445/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.9324 - acc: 0.2500 - val_loss: 21.1589 - val_acc: 0.0161\n",
            "Epoch 446/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.7029 - acc: 0.2782 - val_loss: 22.8308 - val_acc: 0.0484\n",
            "Epoch 447/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.8196 - acc: 0.2702 - val_loss: 21.5997 - val_acc: 0.0323\n",
            "Epoch 448/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 1.4592 - acc: 0.3065 - val_loss: 21.3267 - val_acc: 0.0323\n",
            "Epoch 449/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.5705 - acc: 0.2742 - val_loss: 21.5385 - val_acc: 0.0161\n",
            "Epoch 450/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6367 - acc: 0.2661 - val_loss: 21.1489 - val_acc: 0.0323\n",
            "Epoch 451/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.7012 - acc: 0.2419 - val_loss: 21.7189 - val_acc: 0.0000e+00\n",
            "Epoch 452/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.4379 - acc: 0.2500 - val_loss: 22.9749 - val_acc: 0.0484\n",
            "Epoch 453/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.0400 - acc: 0.2460 - val_loss: 25.2096 - val_acc: 0.0645\n",
            "Epoch 454/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.0704 - acc: 0.2137 - val_loss: 23.3784 - val_acc: 0.0323\n",
            "Epoch 455/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.8171 - acc: 0.2500 - val_loss: 23.1423 - val_acc: 0.0161\n",
            "Epoch 456/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.0978 - acc: 0.1855 - val_loss: 24.0523 - val_acc: 0.0484\n",
            "Epoch 457/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.8382 - acc: 0.2661 - val_loss: 19.6663 - val_acc: 0.0323\n",
            "Epoch 458/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4650 - acc: 0.3024 - val_loss: 24.6019 - val_acc: 0.0484\n",
            "Epoch 459/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.9231 - acc: 0.2016 - val_loss: 21.5663 - val_acc: 0.0323\n",
            "Epoch 460/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.7825 - acc: 0.2581 - val_loss: 22.6477 - val_acc: 0.0323\n",
            "Epoch 461/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.5510 - acc: 0.2460 - val_loss: 24.3345 - val_acc: 0.0484\n",
            "Epoch 462/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.1398 - acc: 0.2621 - val_loss: 23.2106 - val_acc: 0.0323\n",
            "Epoch 463/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.6037 - acc: 0.2460 - val_loss: 29.0888 - val_acc: 0.0323\n",
            "Epoch 464/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.0444 - acc: 0.2419 - val_loss: 23.3276 - val_acc: 0.0484\n",
            "Epoch 465/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.7849 - acc: 0.2540 - val_loss: 23.1053 - val_acc: 0.0161\n",
            "Epoch 466/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.4901 - acc: 0.3105 - val_loss: 20.6975 - val_acc: 0.0484\n",
            "Epoch 467/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.7421 - acc: 0.2137 - val_loss: 22.7393 - val_acc: 0.0323\n",
            "Epoch 468/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.6061 - acc: 0.2218 - val_loss: 22.0595 - val_acc: 0.0323\n",
            "Epoch 469/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.2373 - acc: 0.2097 - val_loss: 22.8561 - val_acc: 0.0484\n",
            "Epoch 470/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.6252 - acc: 0.2016 - val_loss: 23.1643 - val_acc: 0.0323\n",
            "Epoch 471/1000\n",
            "248/248 [==============================] - 0s 89us/step - loss: 1.8461 - acc: 0.2581 - val_loss: 24.2224 - val_acc: 0.0806\n",
            "Epoch 472/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.9992 - acc: 0.1734 - val_loss: 22.2838 - val_acc: 0.0161\n",
            "Epoch 473/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.5939 - acc: 0.2419 - val_loss: 25.3035 - val_acc: 0.0645\n",
            "Epoch 474/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.9638 - acc: 0.2298 - val_loss: 23.3231 - val_acc: 0.0161\n",
            "Epoch 475/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.1232 - acc: 0.2056 - val_loss: 21.2588 - val_acc: 0.0161\n",
            "Epoch 476/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.2463 - acc: 0.1935 - val_loss: 21.3201 - val_acc: 0.0484\n",
            "Epoch 477/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.5326 - acc: 0.2863 - val_loss: 23.4919 - val_acc: 0.0323\n",
            "Epoch 478/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.6557 - acc: 0.2742 - val_loss: 22.7507 - val_acc: 0.0161\n",
            "Epoch 479/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.8747 - acc: 0.2782 - val_loss: 22.3911 - val_acc: 0.0323\n",
            "Epoch 480/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.7289 - acc: 0.2702 - val_loss: 23.2440 - val_acc: 0.0000e+00\n",
            "Epoch 481/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4142 - acc: 0.2823 - val_loss: 22.8965 - val_acc: 0.0323\n",
            "Epoch 482/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.2311 - acc: 0.1855 - val_loss: 23.0425 - val_acc: 0.0484\n",
            "Epoch 483/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.4601 - acc: 0.2581 - val_loss: 23.5967 - val_acc: 0.0161\n",
            "Epoch 484/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.7606 - acc: 0.2460 - val_loss: 23.3738 - val_acc: 0.0161\n",
            "Epoch 485/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.6135 - acc: 0.2339 - val_loss: 26.7072 - val_acc: 0.0161\n",
            "Epoch 486/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.8515 - acc: 0.2298 - val_loss: 22.2840 - val_acc: 0.0484\n",
            "Epoch 487/1000\n",
            "248/248 [==============================] - 0s 83us/step - loss: 1.5258 - acc: 0.2540 - val_loss: 21.5331 - val_acc: 0.0484\n",
            "Epoch 488/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4596 - acc: 0.2661 - val_loss: 21.0340 - val_acc: 0.0161\n",
            "Epoch 489/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.8529 - acc: 0.2460 - val_loss: 21.9215 - val_acc: 0.0161\n",
            "Epoch 490/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.4723 - acc: 0.2823 - val_loss: 23.3283 - val_acc: 0.0323\n",
            "Epoch 491/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3954 - acc: 0.2742 - val_loss: 24.3660 - val_acc: 0.0323\n",
            "Epoch 492/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.6206 - acc: 0.2581 - val_loss: 20.2302 - val_acc: 0.0323\n",
            "Epoch 493/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.6653 - acc: 0.2298 - val_loss: 25.9432 - val_acc: 0.0645\n",
            "Epoch 494/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6779 - acc: 0.2540 - val_loss: 21.4057 - val_acc: 0.0806\n",
            "Epoch 495/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.1108 - acc: 0.2177 - val_loss: 22.2128 - val_acc: 0.0161\n",
            "Epoch 496/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.4150 - acc: 0.2984 - val_loss: 22.6240 - val_acc: 0.0323\n",
            "Epoch 497/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.4052 - acc: 0.2782 - val_loss: 22.8765 - val_acc: 0.0323\n",
            "Epoch 498/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.7836 - acc: 0.2177 - val_loss: 25.2195 - val_acc: 0.0323\n",
            "Epoch 499/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.0935 - acc: 0.2097 - val_loss: 23.9376 - val_acc: 0.0161\n",
            "Epoch 500/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.7614 - acc: 0.2258 - val_loss: 22.4129 - val_acc: 0.0484\n",
            "Epoch 501/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3956 - acc: 0.2944 - val_loss: 22.1097 - val_acc: 0.0323\n",
            "Epoch 502/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4100 - acc: 0.2581 - val_loss: 23.8635 - val_acc: 0.0161\n",
            "Epoch 503/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3374 - acc: 0.2742 - val_loss: 23.2313 - val_acc: 0.0484\n",
            "Epoch 504/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.4676 - acc: 0.2823 - val_loss: 24.6734 - val_acc: 0.0484\n",
            "Epoch 505/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.6342 - acc: 0.2339 - val_loss: 22.0888 - val_acc: 0.0161\n",
            "Epoch 506/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3527 - acc: 0.2903 - val_loss: 24.4208 - val_acc: 0.0484\n",
            "Epoch 507/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.5778 - acc: 0.2218 - val_loss: 22.0946 - val_acc: 0.0323\n",
            "Epoch 508/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6331 - acc: 0.2863 - val_loss: 24.8329 - val_acc: 0.0484\n",
            "Epoch 509/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.6982 - acc: 0.2702 - val_loss: 23.6384 - val_acc: 0.0161\n",
            "Epoch 510/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.8445 - acc: 0.2137 - val_loss: 22.7234 - val_acc: 0.0484\n",
            "Epoch 511/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.8889 - acc: 0.2258 - val_loss: 23.6879 - val_acc: 0.0484\n",
            "Epoch 512/1000\n",
            "248/248 [==============================] - 0s 111us/step - loss: 1.6019 - acc: 0.2339 - val_loss: 20.7346 - val_acc: 0.0323\n",
            "Epoch 513/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4992 - acc: 0.2379 - val_loss: 23.5534 - val_acc: 0.0806\n",
            "Epoch 514/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.5924 - acc: 0.2500 - val_loss: 22.2902 - val_acc: 0.0645\n",
            "Epoch 515/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.5359 - acc: 0.2782 - val_loss: 24.8946 - val_acc: 0.0645\n",
            "Epoch 516/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.7198 - acc: 0.2298 - val_loss: 23.1576 - val_acc: 0.0806\n",
            "Epoch 517/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.4754 - acc: 0.2823 - val_loss: 22.7706 - val_acc: 0.0645\n",
            "Epoch 518/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3887 - acc: 0.2742 - val_loss: 21.6252 - val_acc: 0.0161\n",
            "Epoch 519/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.5777 - acc: 0.2621 - val_loss: 22.3899 - val_acc: 0.0323\n",
            "Epoch 520/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.5004 - acc: 0.2500 - val_loss: 22.8921 - val_acc: 0.0323\n",
            "Epoch 521/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.5684 - acc: 0.2581 - val_loss: 21.7674 - val_acc: 0.0323\n",
            "Epoch 522/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.5236 - acc: 0.2702 - val_loss: 22.7639 - val_acc: 0.0000e+00\n",
            "Epoch 523/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.7261 - acc: 0.2581 - val_loss: 21.8138 - val_acc: 0.0323\n",
            "Epoch 524/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.4343 - acc: 0.2621 - val_loss: 23.2838 - val_acc: 0.0323\n",
            "Epoch 525/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.5116 - acc: 0.2661 - val_loss: 23.2834 - val_acc: 0.0806\n",
            "Epoch 526/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2869 - acc: 0.1855 - val_loss: 21.5025 - val_acc: 0.0806\n",
            "Epoch 527/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.4157 - acc: 0.2823 - val_loss: 23.8604 - val_acc: 0.0484\n",
            "Epoch 528/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6200 - acc: 0.2540 - val_loss: 22.8672 - val_acc: 0.0323\n",
            "Epoch 529/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.3944 - acc: 0.2984 - val_loss: 22.7064 - val_acc: 0.0323\n",
            "Epoch 530/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.4719 - acc: 0.2581 - val_loss: 21.9318 - val_acc: 0.0323\n",
            "Epoch 531/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.4562 - acc: 0.2661 - val_loss: 21.6018 - val_acc: 0.0484\n",
            "Epoch 532/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3583 - acc: 0.2621 - val_loss: 22.6703 - val_acc: 0.0323\n",
            "Epoch 533/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4789 - acc: 0.2823 - val_loss: 22.3878 - val_acc: 0.0323\n",
            "Epoch 534/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2543 - acc: 0.3226 - val_loss: 22.4536 - val_acc: 0.0323\n",
            "Epoch 535/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.4332 - acc: 0.2419 - val_loss: 26.8492 - val_acc: 0.0484\n",
            "Epoch 536/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.7143 - acc: 0.2500 - val_loss: 23.2222 - val_acc: 0.0323\n",
            "Epoch 537/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4941 - acc: 0.2298 - val_loss: 23.7425 - val_acc: 0.0323\n",
            "Epoch 538/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3138 - acc: 0.3105 - val_loss: 21.9409 - val_acc: 0.0323\n",
            "Epoch 539/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2656 - acc: 0.2823 - val_loss: 21.7906 - val_acc: 0.0161\n",
            "Epoch 540/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3949 - acc: 0.2944 - val_loss: 22.4660 - val_acc: 0.0806\n",
            "Epoch 541/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4571 - acc: 0.2298 - val_loss: 22.1755 - val_acc: 0.0323\n",
            "Epoch 542/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3156 - acc: 0.2742 - val_loss: 23.2993 - val_acc: 0.0806\n",
            "Epoch 543/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.7345 - acc: 0.2137 - val_loss: 22.9240 - val_acc: 0.0484\n",
            "Epoch 544/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.3380 - acc: 0.3105 - val_loss: 22.1998 - val_acc: 0.0484\n",
            "Epoch 545/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.5861 - acc: 0.2379 - val_loss: 23.4247 - val_acc: 0.0161\n",
            "Epoch 546/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3946 - acc: 0.2903 - val_loss: 24.5463 - val_acc: 0.0645\n",
            "Epoch 547/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.2763 - acc: 0.3185 - val_loss: 24.0434 - val_acc: 0.0645\n",
            "Epoch 548/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.4095 - acc: 0.2742 - val_loss: 24.5174 - val_acc: 0.0323\n",
            "Epoch 549/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.0312 - acc: 0.2016 - val_loss: 23.8862 - val_acc: 0.0161\n",
            "Epoch 550/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.3522 - acc: 0.2621 - val_loss: 22.1886 - val_acc: 0.0161\n",
            "Epoch 551/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.7827 - acc: 0.2218 - val_loss: 26.3327 - val_acc: 0.0484\n",
            "Epoch 552/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.6982 - acc: 0.2460 - val_loss: 22.4817 - val_acc: 0.0323\n",
            "Epoch 553/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3548 - acc: 0.2702 - val_loss: 21.8907 - val_acc: 0.0323\n",
            "Epoch 554/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4081 - acc: 0.2863 - val_loss: 22.9009 - val_acc: 0.0323\n",
            "Epoch 555/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2937 - acc: 0.2782 - val_loss: 23.4756 - val_acc: 0.0323\n",
            "Epoch 556/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.6441 - acc: 0.2742 - val_loss: 25.6030 - val_acc: 0.0161\n",
            "Epoch 557/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3542 - acc: 0.2500 - val_loss: 22.5861 - val_acc: 0.0323\n",
            "Epoch 558/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.6863 - acc: 0.2621 - val_loss: 20.5919 - val_acc: 0.0323\n",
            "Epoch 559/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6098 - acc: 0.2258 - val_loss: 20.7455 - val_acc: 0.0484\n",
            "Epoch 560/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.3102 - acc: 0.2581 - val_loss: 22.5402 - val_acc: 0.0484\n",
            "Epoch 561/1000\n",
            "248/248 [==============================] - 0s 89us/step - loss: 1.3254 - acc: 0.2742 - val_loss: 24.1150 - val_acc: 0.0484\n",
            "Epoch 562/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4024 - acc: 0.3065 - val_loss: 21.1501 - val_acc: 0.0161\n",
            "Epoch 563/1000\n",
            "248/248 [==============================] - 0s 96us/step - loss: 1.3824 - acc: 0.2379 - val_loss: 21.6193 - val_acc: 0.0323\n",
            "Epoch 564/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.3128 - acc: 0.2702 - val_loss: 22.7320 - val_acc: 0.0000e+00\n",
            "Epoch 565/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.5208 - acc: 0.2702 - val_loss: 22.0168 - val_acc: 0.0484\n",
            "Epoch 566/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4081 - acc: 0.2782 - val_loss: 21.9530 - val_acc: 0.0484\n",
            "Epoch 567/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4347 - acc: 0.2379 - val_loss: 22.9582 - val_acc: 0.0000e+00\n",
            "Epoch 568/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.9483 - acc: 0.2218 - val_loss: 23.1148 - val_acc: 0.0323\n",
            "Epoch 569/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.4730 - acc: 0.2661 - val_loss: 24.8251 - val_acc: 0.0484\n",
            "Epoch 570/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.5916 - acc: 0.2823 - val_loss: 23.9596 - val_acc: 0.0161\n",
            "Epoch 571/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4848 - acc: 0.2581 - val_loss: 27.6740 - val_acc: 0.0000e+00\n",
            "Epoch 572/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.4471 - acc: 0.2661 - val_loss: 24.4434 - val_acc: 0.0645\n",
            "Epoch 573/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6258 - acc: 0.2581 - val_loss: 23.3459 - val_acc: 0.0484\n",
            "Epoch 574/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.5418 - acc: 0.2742 - val_loss: 24.6324 - val_acc: 0.0806\n",
            "Epoch 575/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.0575 - acc: 0.1653 - val_loss: 22.4121 - val_acc: 0.0484\n",
            "Epoch 576/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.3220 - acc: 0.2823 - val_loss: 22.2034 - val_acc: 0.0484\n",
            "Epoch 577/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.5536 - acc: 0.2702 - val_loss: 23.9020 - val_acc: 0.0484\n",
            "Epoch 578/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2733 - acc: 0.3065 - val_loss: 23.2979 - val_acc: 0.0000e+00\n",
            "Epoch 579/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.3646 - acc: 0.2782 - val_loss: 23.2066 - val_acc: 0.0161\n",
            "Epoch 580/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3830 - acc: 0.2581 - val_loss: 24.3195 - val_acc: 0.0161\n",
            "Epoch 581/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.2434 - acc: 0.2702 - val_loss: 23.4745 - val_acc: 0.0323\n",
            "Epoch 582/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4416 - acc: 0.3185 - val_loss: 23.6058 - val_acc: 0.0161\n",
            "Epoch 583/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3134 - acc: 0.2540 - val_loss: 24.6427 - val_acc: 0.0161\n",
            "Epoch 584/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.4622 - acc: 0.2823 - val_loss: 23.3050 - val_acc: 0.0484\n",
            "Epoch 585/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4311 - acc: 0.2540 - val_loss: 21.6759 - val_acc: 0.0323\n",
            "Epoch 586/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 1.5802 - acc: 0.2661 - val_loss: 21.3168 - val_acc: 0.0161\n",
            "Epoch 587/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2387 - acc: 0.2823 - val_loss: 23.9680 - val_acc: 0.0161\n",
            "Epoch 588/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.2447 - acc: 0.2500 - val_loss: 23.1316 - val_acc: 0.0484\n",
            "Epoch 589/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.2800 - acc: 0.3105 - val_loss: 24.0549 - val_acc: 0.0645\n",
            "Epoch 590/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2844 - acc: 0.2782 - val_loss: 22.5621 - val_acc: 0.0323\n",
            "Epoch 591/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.2126 - acc: 0.2903 - val_loss: 23.1324 - val_acc: 0.0323\n",
            "Epoch 592/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.3716 - acc: 0.2863 - val_loss: 23.5283 - val_acc: 0.0161\n",
            "Epoch 593/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.9304 - acc: 0.2258 - val_loss: 23.7185 - val_acc: 0.0484\n",
            "Epoch 594/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.3027 - acc: 0.2581 - val_loss: 21.0807 - val_acc: 0.0484\n",
            "Epoch 595/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4296 - acc: 0.2621 - val_loss: 22.5402 - val_acc: 0.0161\n",
            "Epoch 596/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3071 - acc: 0.2863 - val_loss: 21.6018 - val_acc: 0.0484\n",
            "Epoch 597/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.5019 - acc: 0.3024 - val_loss: 23.4664 - val_acc: 0.0806\n",
            "Epoch 598/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.3468 - acc: 0.2702 - val_loss: 22.7698 - val_acc: 0.0645\n",
            "Epoch 599/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.7629 - acc: 0.2339 - val_loss: 22.8393 - val_acc: 0.0323\n",
            "Epoch 600/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.7663 - acc: 0.2419 - val_loss: 24.5799 - val_acc: 0.0161\n",
            "Epoch 601/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.6121 - acc: 0.2379 - val_loss: 21.5307 - val_acc: 0.0484\n",
            "Epoch 602/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3212 - acc: 0.2903 - val_loss: 23.1223 - val_acc: 0.0000e+00\n",
            "Epoch 603/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.3179 - acc: 0.2863 - val_loss: 22.8799 - val_acc: 0.0645\n",
            "Epoch 604/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.5067 - acc: 0.2177 - val_loss: 23.4482 - val_acc: 0.0161\n",
            "Epoch 605/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3149 - acc: 0.2661 - val_loss: 22.0407 - val_acc: 0.0161\n",
            "Epoch 606/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4713 - acc: 0.2218 - val_loss: 23.1521 - val_acc: 0.0161\n",
            "Epoch 607/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4147 - acc: 0.2540 - val_loss: 21.9525 - val_acc: 0.0323\n",
            "Epoch 608/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2059 - acc: 0.2944 - val_loss: 23.1803 - val_acc: 0.0161\n",
            "Epoch 609/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.2667 - acc: 0.2702 - val_loss: 21.6780 - val_acc: 0.0484\n",
            "Epoch 610/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1956 - acc: 0.2823 - val_loss: 26.2254 - val_acc: 0.0806\n",
            "Epoch 611/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.6375 - acc: 0.2419 - val_loss: 22.9487 - val_acc: 0.0323\n",
            "Epoch 612/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3767 - acc: 0.2258 - val_loss: 23.9068 - val_acc: 0.0161\n",
            "Epoch 613/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.2527 - acc: 0.3024 - val_loss: 20.9298 - val_acc: 0.0645\n",
            "Epoch 614/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.3976 - acc: 0.2419 - val_loss: 22.0116 - val_acc: 0.0484\n",
            "Epoch 615/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.5458 - acc: 0.2702 - val_loss: 22.6691 - val_acc: 0.0161\n",
            "Epoch 616/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2442 - acc: 0.2984 - val_loss: 22.6277 - val_acc: 0.0484\n",
            "Epoch 617/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.0520 - acc: 0.2177 - val_loss: 21.9463 - val_acc: 0.0161\n",
            "Epoch 618/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6890 - acc: 0.1855 - val_loss: 24.0568 - val_acc: 0.0484\n",
            "Epoch 619/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.3904 - acc: 0.2460 - val_loss: 22.9778 - val_acc: 0.0645\n",
            "Epoch 620/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.7324 - acc: 0.2540 - val_loss: 23.2449 - val_acc: 0.0161\n",
            "Epoch 621/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1786 - acc: 0.2661 - val_loss: 24.5630 - val_acc: 0.0161\n",
            "Epoch 622/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1657 - acc: 0.2661 - val_loss: 22.5025 - val_acc: 0.0323\n",
            "Epoch 623/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.2463 - acc: 0.2984 - val_loss: 25.0510 - val_acc: 0.0161\n",
            "Epoch 624/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3475 - acc: 0.2621 - val_loss: 22.7489 - val_acc: 0.0000e+00\n",
            "Epoch 625/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.9076 - acc: 0.1895 - val_loss: 24.4041 - val_acc: 0.0645\n",
            "Epoch 626/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.4036 - acc: 0.2379 - val_loss: 23.6027 - val_acc: 0.0484\n",
            "Epoch 627/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.5086 - acc: 0.2258 - val_loss: 21.9678 - val_acc: 0.0000e+00\n",
            "Epoch 628/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4430 - acc: 0.2460 - val_loss: 22.7113 - val_acc: 0.0161\n",
            "Epoch 629/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.4444 - acc: 0.2581 - val_loss: 22.6282 - val_acc: 0.0000e+00\n",
            "Epoch 630/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3363 - acc: 0.2823 - val_loss: 24.7430 - val_acc: 0.0161\n",
            "Epoch 631/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2570 - acc: 0.2903 - val_loss: 21.6560 - val_acc: 0.0000e+00\n",
            "Epoch 632/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.6002 - acc: 0.2621 - val_loss: 21.8646 - val_acc: 0.0323\n",
            "Epoch 633/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1838 - acc: 0.2702 - val_loss: 23.8835 - val_acc: 0.0484\n",
            "Epoch 634/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.2998 - acc: 0.3105 - val_loss: 22.7834 - val_acc: 0.0000e+00\n",
            "Epoch 635/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.2424 - acc: 0.2863 - val_loss: 24.1475 - val_acc: 0.0161\n",
            "Epoch 636/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3669 - acc: 0.2823 - val_loss: 24.1223 - val_acc: 0.0645\n",
            "Epoch 637/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.3982 - acc: 0.2782 - val_loss: 23.4652 - val_acc: 0.0161\n",
            "Epoch 638/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.2139 - acc: 0.2944 - val_loss: 23.9446 - val_acc: 0.0161\n",
            "Epoch 639/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2064 - acc: 0.2661 - val_loss: 24.0275 - val_acc: 0.0323\n",
            "Epoch 640/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 1.3543 - acc: 0.2702 - val_loss: 24.1241 - val_acc: 0.0161\n",
            "Epoch 641/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3660 - acc: 0.2984 - val_loss: 21.9271 - val_acc: 0.0161\n",
            "Epoch 642/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.5430 - acc: 0.2218 - val_loss: 22.9158 - val_acc: 0.0484\n",
            "Epoch 643/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.5471 - acc: 0.2863 - val_loss: 25.3042 - val_acc: 0.0645\n",
            "Epoch 644/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6139 - acc: 0.2339 - val_loss: 22.6204 - val_acc: 0.0161\n",
            "Epoch 645/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.3163 - acc: 0.2460 - val_loss: 22.3040 - val_acc: 0.0000e+00\n",
            "Epoch 646/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2445 - acc: 0.2984 - val_loss: 24.2897 - val_acc: 0.0645\n",
            "Epoch 647/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3194 - acc: 0.2621 - val_loss: 23.4187 - val_acc: 0.0000e+00\n",
            "Epoch 648/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.3205 - acc: 0.2742 - val_loss: 22.7927 - val_acc: 0.0161\n",
            "Epoch 649/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.2599 - acc: 0.2702 - val_loss: 24.3588 - val_acc: 0.0161\n",
            "Epoch 650/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.4568 - acc: 0.2419 - val_loss: 23.2844 - val_acc: 0.0000e+00\n",
            "Epoch 651/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.5873 - acc: 0.2540 - val_loss: 22.5764 - val_acc: 0.0161\n",
            "Epoch 652/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.5233 - acc: 0.2863 - val_loss: 23.2249 - val_acc: 0.0323\n",
            "Epoch 653/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.1295 - acc: 0.2984 - val_loss: 23.3831 - val_acc: 0.0161\n",
            "Epoch 654/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.1947 - acc: 0.2944 - val_loss: 24.9999 - val_acc: 0.0645\n",
            "Epoch 655/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3681 - acc: 0.2661 - val_loss: 23.2026 - val_acc: 0.0323\n",
            "Epoch 656/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.2150 - acc: 0.2581 - val_loss: 22.0603 - val_acc: 0.0000e+00\n",
            "Epoch 657/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3078 - acc: 0.2742 - val_loss: 22.8379 - val_acc: 0.0161\n",
            "Epoch 658/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2635 - acc: 0.2702 - val_loss: 22.0649 - val_acc: 0.0161\n",
            "Epoch 659/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4198 - acc: 0.2581 - val_loss: 22.3611 - val_acc: 0.0161\n",
            "Epoch 660/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.4512 - acc: 0.2661 - val_loss: 22.6977 - val_acc: 0.0161\n",
            "Epoch 661/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1012 - acc: 0.3065 - val_loss: 23.7199 - val_acc: 0.0161\n",
            "Epoch 662/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.2323 - acc: 0.2903 - val_loss: 22.0667 - val_acc: 0.0000e+00\n",
            "Epoch 663/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.2522 - acc: 0.2621 - val_loss: 22.0219 - val_acc: 0.0161\n",
            "Epoch 664/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1852 - acc: 0.2944 - val_loss: 23.8885 - val_acc: 0.0323\n",
            "Epoch 665/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1925 - acc: 0.2782 - val_loss: 22.5889 - val_acc: 0.0323\n",
            "Epoch 666/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.3121 - acc: 0.2621 - val_loss: 23.1621 - val_acc: 0.0161\n",
            "Epoch 667/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3022 - acc: 0.2702 - val_loss: 23.9133 - val_acc: 0.0161\n",
            "Epoch 668/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.4250 - acc: 0.2298 - val_loss: 25.6369 - val_acc: 0.0645\n",
            "Epoch 669/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3212 - acc: 0.2823 - val_loss: 23.1999 - val_acc: 0.0323\n",
            "Epoch 670/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2133 - acc: 0.2460 - val_loss: 22.6732 - val_acc: 0.0484\n",
            "Epoch 671/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2711 - acc: 0.2460 - val_loss: 21.6626 - val_acc: 0.0323\n",
            "Epoch 672/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3755 - acc: 0.2863 - val_loss: 24.7834 - val_acc: 0.0484\n",
            "Epoch 673/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2378 - acc: 0.2742 - val_loss: 22.7408 - val_acc: 0.0161\n",
            "Epoch 674/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.2354 - acc: 0.2661 - val_loss: 24.7418 - val_acc: 0.0161\n",
            "Epoch 675/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2743 - acc: 0.2298 - val_loss: 23.2423 - val_acc: 0.0161\n",
            "Epoch 676/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.4556 - acc: 0.2823 - val_loss: 23.1676 - val_acc: 0.0161\n",
            "Epoch 677/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1920 - acc: 0.3024 - val_loss: 23.7677 - val_acc: 0.0484\n",
            "Epoch 678/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1697 - acc: 0.2984 - val_loss: 24.0147 - val_acc: 0.0323\n",
            "Epoch 679/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1328 - acc: 0.2903 - val_loss: 22.9613 - val_acc: 0.0161\n",
            "Epoch 680/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.1663 - acc: 0.2661 - val_loss: 24.0565 - val_acc: 0.0000e+00\n",
            "Epoch 681/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1107 - acc: 0.2702 - val_loss: 24.2110 - val_acc: 0.0323\n",
            "Epoch 682/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3053 - acc: 0.2742 - val_loss: 24.1225 - val_acc: 0.0323\n",
            "Epoch 683/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2473 - acc: 0.3024 - val_loss: 24.8769 - val_acc: 0.0806\n",
            "Epoch 684/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.5148 - acc: 0.2823 - val_loss: 23.4052 - val_acc: 0.0161\n",
            "Epoch 685/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2233 - acc: 0.2782 - val_loss: 23.4991 - val_acc: 0.0484\n",
            "Epoch 686/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4788 - acc: 0.2661 - val_loss: 25.4033 - val_acc: 0.0161\n",
            "Epoch 687/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6394 - acc: 0.2137 - val_loss: 24.7279 - val_acc: 0.0645\n",
            "Epoch 688/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3825 - acc: 0.2621 - val_loss: 22.5128 - val_acc: 0.0161\n",
            "Epoch 689/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2354 - acc: 0.2823 - val_loss: 22.7867 - val_acc: 0.0323\n",
            "Epoch 690/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2190 - acc: 0.2702 - val_loss: 21.9461 - val_acc: 0.0323\n",
            "Epoch 691/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2808 - acc: 0.2903 - val_loss: 23.9456 - val_acc: 0.0323\n",
            "Epoch 692/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2502 - acc: 0.2621 - val_loss: 23.2668 - val_acc: 0.0323\n",
            "Epoch 693/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.3557 - acc: 0.2661 - val_loss: 24.7696 - val_acc: 0.0323\n",
            "Epoch 694/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.5669 - acc: 0.2460 - val_loss: 22.7715 - val_acc: 0.0000e+00\n",
            "Epoch 695/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3329 - acc: 0.2540 - val_loss: 22.7015 - val_acc: 0.0161\n",
            "Epoch 696/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1898 - acc: 0.2661 - val_loss: 22.8826 - val_acc: 0.0161\n",
            "Epoch 697/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1810 - acc: 0.3024 - val_loss: 22.4507 - val_acc: 0.0000e+00\n",
            "Epoch 698/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3499 - acc: 0.2379 - val_loss: 22.8800 - val_acc: 0.0484\n",
            "Epoch 699/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 1.1490 - acc: 0.3105 - val_loss: 24.6530 - val_acc: 0.0161\n",
            "Epoch 700/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0796 - acc: 0.3024 - val_loss: 24.3296 - val_acc: 0.0484\n",
            "Epoch 701/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.2438 - acc: 0.2863 - val_loss: 23.6149 - val_acc: 0.0484\n",
            "Epoch 702/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1152 - acc: 0.3145 - val_loss: 23.4282 - val_acc: 0.0161\n",
            "Epoch 703/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.2877 - acc: 0.3024 - val_loss: 23.3132 - val_acc: 0.0323\n",
            "Epoch 704/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2910 - acc: 0.2782 - val_loss: 22.4140 - val_acc: 0.0323\n",
            "Epoch 705/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.1397 - acc: 0.3065 - val_loss: 25.1721 - val_acc: 0.0484\n",
            "Epoch 706/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1950 - acc: 0.2984 - val_loss: 24.7196 - val_acc: 0.0484\n",
            "Epoch 707/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1627 - acc: 0.3105 - val_loss: 23.4794 - val_acc: 0.0000e+00\n",
            "Epoch 708/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1213 - acc: 0.2984 - val_loss: 24.9940 - val_acc: 0.0323\n",
            "Epoch 709/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1226 - acc: 0.3105 - val_loss: 23.6495 - val_acc: 0.0323\n",
            "Epoch 710/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.8435 - acc: 0.2258 - val_loss: 22.4432 - val_acc: 0.0645\n",
            "Epoch 711/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3745 - acc: 0.2621 - val_loss: 25.2669 - val_acc: 0.0323\n",
            "Epoch 712/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1393 - acc: 0.3185 - val_loss: 26.7979 - val_acc: 0.0323\n",
            "Epoch 713/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.8874 - acc: 0.2460 - val_loss: 24.8791 - val_acc: 0.0000e+00\n",
            "Epoch 714/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.7541 - acc: 0.2339 - val_loss: 24.9171 - val_acc: 0.0000e+00\n",
            "Epoch 715/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2730 - acc: 0.2782 - val_loss: 23.3493 - val_acc: 0.0484\n",
            "Epoch 716/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2996 - acc: 0.2903 - val_loss: 24.5386 - val_acc: 0.0161\n",
            "Epoch 717/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2494 - acc: 0.2500 - val_loss: 24.2132 - val_acc: 0.0323\n",
            "Epoch 718/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4635 - acc: 0.2379 - val_loss: 22.4548 - val_acc: 0.0161\n",
            "Epoch 719/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0879 - acc: 0.3105 - val_loss: 23.2899 - val_acc: 0.0323\n",
            "Epoch 720/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0147 - acc: 0.3065 - val_loss: 22.9375 - val_acc: 0.0323\n",
            "Epoch 721/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.7720 - acc: 0.2298 - val_loss: 24.1937 - val_acc: 0.0323\n",
            "Epoch 722/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.4021 - acc: 0.2379 - val_loss: 22.8668 - val_acc: 0.0161\n",
            "Epoch 723/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3377 - acc: 0.2903 - val_loss: 22.4867 - val_acc: 0.0323\n",
            "Epoch 724/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2572 - acc: 0.2742 - val_loss: 23.9994 - val_acc: 0.0484\n",
            "Epoch 725/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1885 - acc: 0.3266 - val_loss: 26.0284 - val_acc: 0.0645\n",
            "Epoch 726/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1984 - acc: 0.2823 - val_loss: 23.7914 - val_acc: 0.0484\n",
            "Epoch 727/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.3246 - acc: 0.2500 - val_loss: 21.9274 - val_acc: 0.0323\n",
            "Epoch 728/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.3572 - acc: 0.2540 - val_loss: 24.7600 - val_acc: 0.0484\n",
            "Epoch 729/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1640 - acc: 0.2661 - val_loss: 24.6588 - val_acc: 0.0484\n",
            "Epoch 730/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.6878 - acc: 0.2540 - val_loss: 22.9872 - val_acc: 0.0323\n",
            "Epoch 731/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4761 - acc: 0.2621 - val_loss: 23.1465 - val_acc: 0.0323\n",
            "Epoch 732/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4322 - acc: 0.2782 - val_loss: 23.1280 - val_acc: 0.0484\n",
            "Epoch 733/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.2160 - acc: 0.2823 - val_loss: 23.4421 - val_acc: 0.0323\n",
            "Epoch 734/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.0992 - acc: 0.2742 - val_loss: 23.8562 - val_acc: 0.0161\n",
            "Epoch 735/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0240 - acc: 0.3306 - val_loss: 21.8743 - val_acc: 0.0484\n",
            "Epoch 736/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.4136 - acc: 0.2540 - val_loss: 22.6144 - val_acc: 0.0323\n",
            "Epoch 737/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3164 - acc: 0.2823 - val_loss: 24.8270 - val_acc: 0.0000e+00\n",
            "Epoch 738/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.4859 - acc: 0.2782 - val_loss: 22.1949 - val_acc: 0.0484\n",
            "Epoch 739/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0398 - acc: 0.3387 - val_loss: 22.3715 - val_acc: 0.0484\n",
            "Epoch 740/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.5359 - acc: 0.2258 - val_loss: 24.0362 - val_acc: 0.0161\n",
            "Epoch 741/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1556 - acc: 0.2782 - val_loss: 24.2875 - val_acc: 0.0161\n",
            "Epoch 742/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1116 - acc: 0.2863 - val_loss: 23.9710 - val_acc: 0.0484\n",
            "Epoch 743/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0721 - acc: 0.2823 - val_loss: 23.7564 - val_acc: 0.0161\n",
            "Epoch 744/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1649 - acc: 0.2944 - val_loss: 24.4069 - val_acc: 0.0645\n",
            "Epoch 745/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.3212 - acc: 0.2661 - val_loss: 23.5712 - val_acc: 0.0161\n",
            "Epoch 746/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0684 - acc: 0.2621 - val_loss: 23.8838 - val_acc: 0.0161\n",
            "Epoch 747/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2253 - acc: 0.2944 - val_loss: 28.1588 - val_acc: 0.0323\n",
            "Epoch 748/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3598 - acc: 0.3266 - val_loss: 24.4670 - val_acc: 0.0484\n",
            "Epoch 749/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.0447 - acc: 0.2742 - val_loss: 23.4762 - val_acc: 0.0323\n",
            "Epoch 750/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0593 - acc: 0.3387 - val_loss: 23.6886 - val_acc: 0.0484\n",
            "Epoch 751/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1974 - acc: 0.3185 - val_loss: 24.4503 - val_acc: 0.0161\n",
            "Epoch 752/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0654 - acc: 0.2863 - val_loss: 25.1280 - val_acc: 0.0484\n",
            "Epoch 753/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1895 - acc: 0.2621 - val_loss: 24.7317 - val_acc: 0.0323\n",
            "Epoch 754/1000\n",
            "248/248 [==============================] - 0s 79us/step - loss: 1.0357 - acc: 0.3185 - val_loss: 23.4479 - val_acc: 0.0484\n",
            "Epoch 755/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1506 - acc: 0.2661 - val_loss: 23.7343 - val_acc: 0.0323\n",
            "Epoch 756/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.0699 - acc: 0.3105 - val_loss: 23.7376 - val_acc: 0.0161\n",
            "Epoch 757/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0560 - acc: 0.3105 - val_loss: 25.1042 - val_acc: 0.0323\n",
            "Epoch 758/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2797 - acc: 0.2944 - val_loss: 25.3053 - val_acc: 0.0161\n",
            "Epoch 759/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4918 - acc: 0.2419 - val_loss: 23.0060 - val_acc: 0.0161\n",
            "Epoch 760/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 1.3726 - acc: 0.2742 - val_loss: 21.8671 - val_acc: 0.0161\n",
            "Epoch 761/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9975 - acc: 0.3065 - val_loss: 23.0579 - val_acc: 0.0484\n",
            "Epoch 762/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0930 - acc: 0.3145 - val_loss: 24.3268 - val_acc: 0.0645\n",
            "Epoch 763/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.2266 - acc: 0.2823 - val_loss: 23.8174 - val_acc: 0.0323\n",
            "Epoch 764/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3637 - acc: 0.2863 - val_loss: 25.1986 - val_acc: 0.0645\n",
            "Epoch 765/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1597 - acc: 0.2863 - val_loss: 24.2190 - val_acc: 0.0323\n",
            "Epoch 766/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0519 - acc: 0.2702 - val_loss: 24.0124 - val_acc: 0.0323\n",
            "Epoch 767/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1234 - acc: 0.3468 - val_loss: 24.5666 - val_acc: 0.0323\n",
            "Epoch 768/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.9648 - acc: 0.3065 - val_loss: 24.6988 - val_acc: 0.0161\n",
            "Epoch 769/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0132 - acc: 0.2984 - val_loss: 24.5824 - val_acc: 0.0161\n",
            "Epoch 770/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1141 - acc: 0.2460 - val_loss: 22.6532 - val_acc: 0.0161\n",
            "Epoch 771/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2610 - acc: 0.2984 - val_loss: 22.3476 - val_acc: 0.0484\n",
            "Epoch 772/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1066 - acc: 0.2621 - val_loss: 23.1672 - val_acc: 0.0323\n",
            "Epoch 773/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.7263 - acc: 0.3065 - val_loss: 23.9240 - val_acc: 0.0323\n",
            "Epoch 774/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0527 - acc: 0.2782 - val_loss: 23.5879 - val_acc: 0.0161\n",
            "Epoch 775/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0024 - acc: 0.3387 - val_loss: 25.8530 - val_acc: 0.0000e+00\n",
            "Epoch 776/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 0.9640 - acc: 0.3185 - val_loss: 24.0201 - val_acc: 0.0323\n",
            "Epoch 777/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0931 - acc: 0.2621 - val_loss: 26.6852 - val_acc: 0.0484\n",
            "Epoch 778/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0898 - acc: 0.2581 - val_loss: 23.8229 - val_acc: 0.0161\n",
            "Epoch 779/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2975 - acc: 0.2903 - val_loss: 23.9114 - val_acc: 0.0484\n",
            "Epoch 780/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0416 - acc: 0.3185 - val_loss: 25.8389 - val_acc: 0.0806\n",
            "Epoch 781/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0320 - acc: 0.3145 - val_loss: 23.6975 - val_acc: 0.0161\n",
            "Epoch 782/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9938 - acc: 0.2944 - val_loss: 24.3369 - val_acc: 0.0323\n",
            "Epoch 783/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0963 - acc: 0.3024 - val_loss: 27.7428 - val_acc: 0.0161\n",
            "Epoch 784/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 8.3485 - acc: 0.1774 - val_loss: 37.8560 - val_acc: 0.0645\n",
            "Epoch 785/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.4137 - acc: 0.2540 - val_loss: 26.3728 - val_acc: 0.0161\n",
            "Epoch 786/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4122 - acc: 0.2581 - val_loss: 26.0367 - val_acc: 0.0161\n",
            "Epoch 787/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.2806 - acc: 0.2823 - val_loss: 28.8987 - val_acc: 0.0645\n",
            "Epoch 788/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.8060 - acc: 0.2702 - val_loss: 28.2912 - val_acc: 0.0000e+00\n",
            "Epoch 789/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.3442 - acc: 0.2661 - val_loss: 22.9925 - val_acc: 0.0161\n",
            "Epoch 790/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1107 - acc: 0.2782 - val_loss: 24.0311 - val_acc: 0.0161\n",
            "Epoch 791/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1662 - acc: 0.2339 - val_loss: 25.7009 - val_acc: 0.0161\n",
            "Epoch 792/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1699 - acc: 0.2661 - val_loss: 26.6190 - val_acc: 0.0161\n",
            "Epoch 793/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3043 - acc: 0.2016 - val_loss: 24.4893 - val_acc: 0.0645\n",
            "Epoch 794/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1613 - acc: 0.3065 - val_loss: 24.1986 - val_acc: 0.0000e+00\n",
            "Epoch 795/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0693 - acc: 0.2581 - val_loss: 24.9772 - val_acc: 0.0000e+00\n",
            "Epoch 796/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2515 - acc: 0.2903 - val_loss: 23.0280 - val_acc: 0.0000e+00\n",
            "Epoch 797/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2933 - acc: 0.2702 - val_loss: 23.5701 - val_acc: 0.0161\n",
            "Epoch 798/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1016 - acc: 0.3387 - val_loss: 24.5140 - val_acc: 0.0161\n",
            "Epoch 799/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0592 - acc: 0.2823 - val_loss: 23.7447 - val_acc: 0.0323\n",
            "Epoch 800/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.0805 - acc: 0.2984 - val_loss: 25.4182 - val_acc: 0.0645\n",
            "Epoch 801/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.0943 - acc: 0.2984 - val_loss: 23.3197 - val_acc: 0.0000e+00\n",
            "Epoch 802/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9471 - acc: 0.2863 - val_loss: 24.4294 - val_acc: 0.0000e+00\n",
            "Epoch 803/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 1.1247 - acc: 0.2984 - val_loss: 25.1538 - val_acc: 0.0000e+00\n",
            "Epoch 804/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0131 - acc: 0.2984 - val_loss: 27.1968 - val_acc: 0.0484\n",
            "Epoch 805/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1709 - acc: 0.3024 - val_loss: 23.9734 - val_acc: 0.0161\n",
            "Epoch 806/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1756 - acc: 0.2984 - val_loss: 25.4194 - val_acc: 0.0000e+00\n",
            "Epoch 807/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9912 - acc: 0.2984 - val_loss: 24.9946 - val_acc: 0.0161\n",
            "Epoch 808/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9127 - acc: 0.3145 - val_loss: 24.4287 - val_acc: 0.0161\n",
            "Epoch 809/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9397 - acc: 0.2863 - val_loss: 25.4837 - val_acc: 0.0323\n",
            "Epoch 810/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0883 - acc: 0.2782 - val_loss: 25.0423 - val_acc: 0.0323\n",
            "Epoch 811/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.2706 - acc: 0.3226 - val_loss: 23.6376 - val_acc: 0.0161\n",
            "Epoch 812/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0408 - acc: 0.2863 - val_loss: 26.7274 - val_acc: 0.0484\n",
            "Epoch 813/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2294 - acc: 0.2984 - val_loss: 24.4504 - val_acc: 0.0000e+00\n",
            "Epoch 814/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0694 - acc: 0.3024 - val_loss: 24.5096 - val_acc: 0.0323\n",
            "Epoch 815/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0424 - acc: 0.3427 - val_loss: 24.5579 - val_acc: 0.0484\n",
            "Epoch 816/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.4262 - acc: 0.2540 - val_loss: 22.5735 - val_acc: 0.0323\n",
            "Epoch 817/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3218 - acc: 0.2702 - val_loss: 26.1559 - val_acc: 0.0806\n",
            "Epoch 818/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9430 - acc: 0.3185 - val_loss: 26.2793 - val_acc: 0.0484\n",
            "Epoch 819/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9931 - acc: 0.3105 - val_loss: 24.7177 - val_acc: 0.0323\n",
            "Epoch 820/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9696 - acc: 0.3145 - val_loss: 25.1499 - val_acc: 0.0161\n",
            "Epoch 821/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.5975 - acc: 0.2460 - val_loss: 24.5135 - val_acc: 0.0161\n",
            "Epoch 822/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9133 - acc: 0.3427 - val_loss: 24.4746 - val_acc: 0.0323\n",
            "Epoch 823/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9767 - acc: 0.2944 - val_loss: 24.5194 - val_acc: 0.0161\n",
            "Epoch 824/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0863 - acc: 0.2742 - val_loss: 23.9503 - val_acc: 0.0484\n",
            "Epoch 825/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9843 - acc: 0.2823 - val_loss: 25.6327 - val_acc: 0.0323\n",
            "Epoch 826/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1783 - acc: 0.2823 - val_loss: 26.8030 - val_acc: 0.0000e+00\n",
            "Epoch 827/1000\n",
            "248/248 [==============================] - 0s 46us/step - loss: 1.3288 - acc: 0.3145 - val_loss: 26.9962 - val_acc: 0.0484\n",
            "Epoch 828/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.3851 - acc: 0.2782 - val_loss: 24.8497 - val_acc: 0.0323\n",
            "Epoch 829/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9107 - acc: 0.3226 - val_loss: 26.2291 - val_acc: 0.0161\n",
            "Epoch 830/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 0.8999 - acc: 0.2863 - val_loss: 25.2049 - val_acc: 0.0161\n",
            "Epoch 831/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0654 - acc: 0.2621 - val_loss: 25.3493 - val_acc: 0.0323\n",
            "Epoch 832/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9533 - acc: 0.3226 - val_loss: 25.4328 - val_acc: 0.0484\n",
            "Epoch 833/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.0226 - acc: 0.3306 - val_loss: 25.9948 - val_acc: 0.0645\n",
            "Epoch 834/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3043 - acc: 0.2742 - val_loss: 27.3108 - val_acc: 0.0645\n",
            "Epoch 835/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 2.2045 - acc: 0.2137 - val_loss: 22.4088 - val_acc: 0.0806\n",
            "Epoch 836/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.5970 - acc: 0.2581 - val_loss: 20.8892 - val_acc: 0.0484\n",
            "Epoch 837/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9052 - acc: 0.3306 - val_loss: 23.6286 - val_acc: 0.0161\n",
            "Epoch 838/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 0.9302 - acc: 0.3145 - val_loss: 24.4428 - val_acc: 0.0161\n",
            "Epoch 839/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9241 - acc: 0.3266 - val_loss: 24.2563 - val_acc: 0.0645\n",
            "Epoch 840/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9735 - acc: 0.3347 - val_loss: 22.7922 - val_acc: 0.0161\n",
            "Epoch 841/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1993 - acc: 0.2661 - val_loss: 22.8955 - val_acc: 0.0000e+00\n",
            "Epoch 842/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1316 - acc: 0.3024 - val_loss: 23.4076 - val_acc: 0.0323\n",
            "Epoch 843/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0036 - acc: 0.3306 - val_loss: 23.8277 - val_acc: 0.0323\n",
            "Epoch 844/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9305 - acc: 0.3508 - val_loss: 24.7415 - val_acc: 0.0484\n",
            "Epoch 845/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9941 - acc: 0.3145 - val_loss: 23.4568 - val_acc: 0.0484\n",
            "Epoch 846/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6579 - acc: 0.2621 - val_loss: 25.1680 - val_acc: 0.0000e+00\n",
            "Epoch 847/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0779 - acc: 0.2782 - val_loss: 25.5577 - val_acc: 0.0161\n",
            "Epoch 848/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0978 - acc: 0.3065 - val_loss: 25.1624 - val_acc: 0.0161\n",
            "Epoch 849/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9747 - acc: 0.2903 - val_loss: 25.1878 - val_acc: 0.0161\n",
            "Epoch 850/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0367 - acc: 0.3024 - val_loss: 26.5536 - val_acc: 0.0161\n",
            "Epoch 851/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.2733 - acc: 0.2460 - val_loss: 22.2978 - val_acc: 0.0323\n",
            "Epoch 852/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9586 - acc: 0.3387 - val_loss: 25.0084 - val_acc: 0.0484\n",
            "Epoch 853/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0756 - acc: 0.2903 - val_loss: 24.8713 - val_acc: 0.0323\n",
            "Epoch 854/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9588 - acc: 0.3105 - val_loss: 24.5825 - val_acc: 0.0806\n",
            "Epoch 855/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.8990 - acc: 0.3347 - val_loss: 24.5442 - val_acc: 0.0000e+00\n",
            "Epoch 856/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.2949 - acc: 0.2702 - val_loss: 24.2116 - val_acc: 0.0806\n",
            "Epoch 857/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0193 - acc: 0.3145 - val_loss: 24.3378 - val_acc: 0.0645\n",
            "Epoch 858/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9748 - acc: 0.3024 - val_loss: 24.7630 - val_acc: 0.0484\n",
            "Epoch 859/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9581 - acc: 0.3589 - val_loss: 22.8142 - val_acc: 0.0161\n",
            "Epoch 860/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.9363 - acc: 0.3266 - val_loss: 23.6087 - val_acc: 0.0323\n",
            "Epoch 861/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8501 - acc: 0.3185 - val_loss: 23.7134 - val_acc: 0.0645\n",
            "Epoch 862/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.8741 - acc: 0.2984 - val_loss: 25.1080 - val_acc: 0.0484\n",
            "Epoch 863/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.9729 - acc: 0.3347 - val_loss: 24.9503 - val_acc: 0.0161\n",
            "Epoch 864/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9965 - acc: 0.3266 - val_loss: 22.9952 - val_acc: 0.0323\n",
            "Epoch 865/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.0580 - acc: 0.3508 - val_loss: 23.7501 - val_acc: 0.0645\n",
            "Epoch 866/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2371 - acc: 0.2581 - val_loss: 25.8628 - val_acc: 0.0323\n",
            "Epoch 867/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0386 - acc: 0.3185 - val_loss: 26.0943 - val_acc: 0.0484\n",
            "Epoch 868/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 0.9404 - acc: 0.3306 - val_loss: 23.8454 - val_acc: 0.0161\n",
            "Epoch 869/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.9852 - acc: 0.2863 - val_loss: 24.4530 - val_acc: 0.0645\n",
            "Epoch 870/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0924 - acc: 0.3145 - val_loss: 24.6900 - val_acc: 0.0484\n",
            "Epoch 871/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0579 - acc: 0.3065 - val_loss: 25.0509 - val_acc: 0.0806\n",
            "Epoch 872/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9722 - acc: 0.3024 - val_loss: 23.5844 - val_acc: 0.0484\n",
            "Epoch 873/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0578 - acc: 0.3145 - val_loss: 27.2014 - val_acc: 0.0161\n",
            "Epoch 874/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.9705 - acc: 0.2782 - val_loss: 26.6960 - val_acc: 0.0161\n",
            "Epoch 875/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0982 - acc: 0.3226 - val_loss: 22.5801 - val_acc: 0.0000e+00\n",
            "Epoch 876/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 1.0626 - acc: 0.2863 - val_loss: 24.2311 - val_acc: 0.0484\n",
            "Epoch 877/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.8893 - acc: 0.3347 - val_loss: 23.2512 - val_acc: 0.0645\n",
            "Epoch 878/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.8869 - acc: 0.3226 - val_loss: 24.7165 - val_acc: 0.0000e+00\n",
            "Epoch 879/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.0045 - acc: 0.3185 - val_loss: 26.7623 - val_acc: 0.0645\n",
            "Epoch 880/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9333 - acc: 0.3266 - val_loss: 24.8575 - val_acc: 0.0323\n",
            "Epoch 881/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0699 - acc: 0.2863 - val_loss: 26.9164 - val_acc: 0.0161\n",
            "Epoch 882/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1840 - acc: 0.2984 - val_loss: 25.2695 - val_acc: 0.0000e+00\n",
            "Epoch 883/1000\n",
            "248/248 [==============================] - 0s 106us/step - loss: 1.5831 - acc: 0.2540 - val_loss: 26.0957 - val_acc: 0.0806\n",
            "Epoch 884/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1441 - acc: 0.3145 - val_loss: 25.8501 - val_acc: 0.0000e+00\n",
            "Epoch 885/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.8990 - acc: 0.3226 - val_loss: 27.8151 - val_acc: 0.0000e+00\n",
            "Epoch 886/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4373 - acc: 0.2621 - val_loss: 24.4594 - val_acc: 0.0323\n",
            "Epoch 887/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9266 - acc: 0.3185 - val_loss: 26.0317 - val_acc: 0.0484\n",
            "Epoch 888/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1662 - acc: 0.2823 - val_loss: 24.6976 - val_acc: 0.0323\n",
            "Epoch 889/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0068 - acc: 0.3185 - val_loss: 27.0387 - val_acc: 0.0323\n",
            "Epoch 890/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1997 - acc: 0.2742 - val_loss: 22.7328 - val_acc: 0.0645\n",
            "Epoch 891/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1782 - acc: 0.2742 - val_loss: 25.3499 - val_acc: 0.0484\n",
            "Epoch 892/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 0.8912 - acc: 0.3226 - val_loss: 24.9531 - val_acc: 0.0323\n",
            "Epoch 893/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8359 - acc: 0.3387 - val_loss: 24.6418 - val_acc: 0.0323\n",
            "Epoch 894/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1859 - acc: 0.3065 - val_loss: 26.3090 - val_acc: 0.0645\n",
            "Epoch 895/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0126 - acc: 0.3105 - val_loss: 24.5644 - val_acc: 0.0484\n",
            "Epoch 896/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9290 - acc: 0.3105 - val_loss: 26.7929 - val_acc: 0.0645\n",
            "Epoch 897/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 1.3324 - acc: 0.2863 - val_loss: 23.6830 - val_acc: 0.0484\n",
            "Epoch 898/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9089 - acc: 0.3387 - val_loss: 24.7759 - val_acc: 0.0161\n",
            "Epoch 899/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.8937 - acc: 0.3589 - val_loss: 23.9877 - val_acc: 0.0484\n",
            "Epoch 900/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8310 - acc: 0.3387 - val_loss: 27.0208 - val_acc: 0.0000e+00\n",
            "Epoch 901/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.3964 - acc: 0.2823 - val_loss: 23.0233 - val_acc: 0.0645\n",
            "Epoch 902/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.0223 - acc: 0.3024 - val_loss: 26.1123 - val_acc: 0.0484\n",
            "Epoch 903/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.4063 - acc: 0.2984 - val_loss: 23.0069 - val_acc: 0.0645\n",
            "Epoch 904/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0633 - acc: 0.2621 - val_loss: 24.7471 - val_acc: 0.0323\n",
            "Epoch 905/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.0788 - acc: 0.2944 - val_loss: 25.2895 - val_acc: 0.0806\n",
            "Epoch 906/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0225 - acc: 0.3427 - val_loss: 25.1539 - val_acc: 0.0000e+00\n",
            "Epoch 907/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0632 - acc: 0.2984 - val_loss: 24.4101 - val_acc: 0.0806\n",
            "Epoch 908/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 0.9479 - acc: 0.3387 - val_loss: 25.5846 - val_acc: 0.0806\n",
            "Epoch 909/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0668 - acc: 0.2984 - val_loss: 24.0753 - val_acc: 0.0645\n",
            "Epoch 910/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0824 - acc: 0.2823 - val_loss: 24.8990 - val_acc: 0.0484\n",
            "Epoch 911/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9515 - acc: 0.2984 - val_loss: 25.5261 - val_acc: 0.0645\n",
            "Epoch 912/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9586 - acc: 0.3145 - val_loss: 22.8686 - val_acc: 0.0323\n",
            "Epoch 913/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8016 - acc: 0.3790 - val_loss: 23.8217 - val_acc: 0.0484\n",
            "Epoch 914/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 0.9694 - acc: 0.3145 - val_loss: 25.5317 - val_acc: 0.0645\n",
            "Epoch 915/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.8900 - acc: 0.3427 - val_loss: 24.5592 - val_acc: 0.0806\n",
            "Epoch 916/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9368 - acc: 0.2823 - val_loss: 25.4989 - val_acc: 0.0161\n",
            "Epoch 917/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9538 - acc: 0.3266 - val_loss: 24.9592 - val_acc: 0.0484\n",
            "Epoch 918/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0880 - acc: 0.2782 - val_loss: 22.7292 - val_acc: 0.0323\n",
            "Epoch 919/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0376 - acc: 0.2984 - val_loss: 25.5255 - val_acc: 0.0645\n",
            "Epoch 920/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.8552 - acc: 0.3145 - val_loss: 24.5091 - val_acc: 0.0323\n",
            "Epoch 921/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.8954 - acc: 0.3427 - val_loss: 24.5626 - val_acc: 0.0484\n",
            "Epoch 922/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8920 - acc: 0.3024 - val_loss: 24.4208 - val_acc: 0.0161\n",
            "Epoch 923/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8784 - acc: 0.3065 - val_loss: 24.4274 - val_acc: 0.0484\n",
            "Epoch 924/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0786 - acc: 0.3387 - val_loss: 24.3435 - val_acc: 0.0323\n",
            "Epoch 925/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9657 - acc: 0.2944 - val_loss: 23.8877 - val_acc: 0.0806\n",
            "Epoch 926/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0969 - acc: 0.2782 - val_loss: 26.7439 - val_acc: 0.0484\n",
            "Epoch 927/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1158 - acc: 0.2742 - val_loss: 23.3008 - val_acc: 0.0484\n",
            "Epoch 928/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.8838 - acc: 0.3427 - val_loss: 25.7688 - val_acc: 0.0000e+00\n",
            "Epoch 929/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 0.8737 - acc: 0.3185 - val_loss: 24.2148 - val_acc: 0.0645\n",
            "Epoch 930/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1015 - acc: 0.3306 - val_loss: 25.5087 - val_acc: 0.0323\n",
            "Epoch 931/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.9433 - acc: 0.3185 - val_loss: 24.6914 - val_acc: 0.0484\n",
            "Epoch 932/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1161 - acc: 0.2984 - val_loss: 27.5140 - val_acc: 0.0806\n",
            "Epoch 933/1000\n",
            "248/248 [==============================] - 0s 86us/step - loss: 0.9838 - acc: 0.2903 - val_loss: 26.0744 - val_acc: 0.0323\n",
            "Epoch 934/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.8580 - acc: 0.3750 - val_loss: 24.0988 - val_acc: 0.0806\n",
            "Epoch 935/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0998 - acc: 0.3347 - val_loss: 25.9057 - val_acc: 0.0161\n",
            "Epoch 936/1000\n",
            "248/248 [==============================] - 0s 83us/step - loss: 1.1323 - acc: 0.3306 - val_loss: 26.3845 - val_acc: 0.0323\n",
            "Epoch 937/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0347 - acc: 0.3548 - val_loss: 26.6328 - val_acc: 0.0484\n",
            "Epoch 938/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 0.8585 - acc: 0.3710 - val_loss: 26.1646 - val_acc: 0.0323\n",
            "Epoch 939/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9847 - acc: 0.3347 - val_loss: 23.7619 - val_acc: 0.0323\n",
            "Epoch 940/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.8573 - acc: 0.3589 - val_loss: 23.5892 - val_acc: 0.0323\n",
            "Epoch 941/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0641 - acc: 0.2863 - val_loss: 24.5066 - val_acc: 0.0806\n",
            "Epoch 942/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.8743 - acc: 0.3105 - val_loss: 24.3917 - val_acc: 0.0645\n",
            "Epoch 943/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 0.8388 - acc: 0.3669 - val_loss: 24.7839 - val_acc: 0.0323\n",
            "Epoch 944/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.8706 - acc: 0.3226 - val_loss: 26.0660 - val_acc: 0.0161\n",
            "Epoch 945/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 0.8147 - acc: 0.3226 - val_loss: 24.3624 - val_acc: 0.0323\n",
            "Epoch 946/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0936 - acc: 0.3185 - val_loss: 24.2019 - val_acc: 0.0323\n",
            "Epoch 947/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 0.9335 - acc: 0.3065 - val_loss: 26.0337 - val_acc: 0.0161\n",
            "Epoch 948/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 0.7642 - acc: 0.3226 - val_loss: 26.2565 - val_acc: 0.0161\n",
            "Epoch 949/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.7993 - acc: 0.3508 - val_loss: 25.8869 - val_acc: 0.0645\n",
            "Epoch 950/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 0.8842 - acc: 0.3145 - val_loss: 24.5727 - val_acc: 0.0484\n",
            "Epoch 951/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0687 - acc: 0.3226 - val_loss: 24.6619 - val_acc: 0.0484\n",
            "Epoch 952/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.8582 - acc: 0.3427 - val_loss: 27.2310 - val_acc: 0.0484\n",
            "Epoch 953/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0730 - acc: 0.3226 - val_loss: 25.7139 - val_acc: 0.0161\n",
            "Epoch 954/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8794 - acc: 0.3105 - val_loss: 26.4586 - val_acc: 0.0645\n",
            "Epoch 955/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.8619 - acc: 0.3226 - val_loss: 24.6230 - val_acc: 0.0323\n",
            "Epoch 956/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9993 - acc: 0.3145 - val_loss: 25.7106 - val_acc: 0.0323\n",
            "Epoch 957/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.8519 - acc: 0.3508 - val_loss: 24.5485 - val_acc: 0.0161\n",
            "Epoch 958/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.8747 - acc: 0.3589 - val_loss: 24.9477 - val_acc: 0.0645\n",
            "Epoch 959/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9364 - acc: 0.2984 - val_loss: 24.5543 - val_acc: 0.0484\n",
            "Epoch 960/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.8428 - acc: 0.3185 - val_loss: 24.9186 - val_acc: 0.0645\n",
            "Epoch 961/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.8691 - acc: 0.3427 - val_loss: 25.8394 - val_acc: 0.0484\n",
            "Epoch 962/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.8590 - acc: 0.2944 - val_loss: 25.8408 - val_acc: 0.0484\n",
            "Epoch 963/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1839 - acc: 0.2661 - val_loss: 24.8842 - val_acc: 0.0161\n",
            "Epoch 964/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 0.9374 - acc: 0.3508 - val_loss: 23.5511 - val_acc: 0.0645\n",
            "Epoch 965/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1496 - acc: 0.3065 - val_loss: 26.4399 - val_acc: 0.0323\n",
            "Epoch 966/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9924 - acc: 0.3347 - val_loss: 24.5844 - val_acc: 0.0645\n",
            "Epoch 967/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.8372 - acc: 0.3185 - val_loss: 23.1779 - val_acc: 0.0806\n",
            "Epoch 968/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9088 - acc: 0.3589 - val_loss: 24.3406 - val_acc: 0.0484\n",
            "Epoch 969/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3368 - acc: 0.2379 - val_loss: 25.1868 - val_acc: 0.0484\n",
            "Epoch 970/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9056 - acc: 0.3226 - val_loss: 25.0772 - val_acc: 0.0161\n",
            "Epoch 971/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2955 - acc: 0.2581 - val_loss: 26.1557 - val_acc: 0.0484\n",
            "Epoch 972/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9772 - acc: 0.3508 - val_loss: 24.5106 - val_acc: 0.0645\n",
            "Epoch 973/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0845 - acc: 0.2661 - val_loss: 24.1512 - val_acc: 0.0323\n",
            "Epoch 974/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9965 - acc: 0.2903 - val_loss: 25.3334 - val_acc: 0.0161\n",
            "Epoch 975/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8201 - acc: 0.3468 - val_loss: 26.3263 - val_acc: 0.0645\n",
            "Epoch 976/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9932 - acc: 0.3185 - val_loss: 25.1151 - val_acc: 0.0000e+00\n",
            "Epoch 977/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1965 - acc: 0.2903 - val_loss: 25.5885 - val_acc: 0.0484\n",
            "Epoch 978/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9572 - acc: 0.3226 - val_loss: 25.2392 - val_acc: 0.0484\n",
            "Epoch 979/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9206 - acc: 0.3306 - val_loss: 24.3976 - val_acc: 0.0645\n",
            "Epoch 980/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 0.8856 - acc: 0.3145 - val_loss: 26.3316 - val_acc: 0.0645\n",
            "Epoch 981/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0495 - acc: 0.2540 - val_loss: 24.0787 - val_acc: 0.0484\n",
            "Epoch 982/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.7869 - acc: 0.3710 - val_loss: 25.2550 - val_acc: 0.0484\n",
            "Epoch 983/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 0.8504 - acc: 0.3427 - val_loss: 24.1098 - val_acc: 0.0645\n",
            "Epoch 984/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0532 - acc: 0.2984 - val_loss: 23.3076 - val_acc: 0.0645\n",
            "Epoch 985/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0244 - acc: 0.3548 - val_loss: 25.0271 - val_acc: 0.0645\n",
            "Epoch 986/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.7520 - acc: 0.3508 - val_loss: 24.3142 - val_acc: 0.0323\n",
            "Epoch 987/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0242 - acc: 0.2903 - val_loss: 26.0908 - val_acc: 0.0323\n",
            "Epoch 988/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0423 - acc: 0.2702 - val_loss: 26.8921 - val_acc: 0.0000e+00\n",
            "Epoch 989/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2569 - acc: 0.3266 - val_loss: 25.7327 - val_acc: 0.0645\n",
            "Epoch 990/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0262 - acc: 0.3024 - val_loss: 24.8669 - val_acc: 0.0161\n",
            "Epoch 991/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 0.8647 - acc: 0.3427 - val_loss: 26.3379 - val_acc: 0.0806\n",
            "Epoch 992/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9625 - acc: 0.2903 - val_loss: 25.3053 - val_acc: 0.0484\n",
            "Epoch 993/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2546 - acc: 0.2621 - val_loss: 24.8305 - val_acc: 0.0484\n",
            "Epoch 994/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.8600 - acc: 0.3266 - val_loss: 29.3296 - val_acc: 0.0000e+00\n",
            "Epoch 995/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0386 - acc: 0.3065 - val_loss: 23.6489 - val_acc: 0.0484\n",
            "Epoch 996/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8740 - acc: 0.3589 - val_loss: 25.2266 - val_acc: 0.0645\n",
            "Epoch 997/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.8348 - acc: 0.3347 - val_loss: 23.8700 - val_acc: 0.0645\n",
            "Epoch 998/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 0.8464 - acc: 0.3589 - val_loss: 24.1887 - val_acc: 0.0484\n",
            "Epoch 999/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.8968 - acc: 0.2823 - val_loss: 25.4982 - val_acc: 0.0323\n",
            "Epoch 1000/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0811 - acc: 0.3105 - val_loss: 24.0427 - val_acc: 0.0645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVfqtwxL26ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "466da091-38d5-4ffc-c1c8-fe0a22167d6e"
      },
      "source": [
        "model.evaluate(testX, testY)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82/82 [==============================] - 0s 56us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.038924984815644, 0.10975609756097561]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-QuBleZ8ChG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_graph(history):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('No. of epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('No. of epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSDwxxDD8Zxc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "76eba77d-c1c5-4af4-ca42-19cd9b94479a"
      },
      "source": [
        "show_graph(history)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD8iYN2q9yS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "model = Sequential()\n",
        "model.add(Dense(20,input_dim = input_dims))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqII6nFd92BR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "a45ba315-d8c7-4449-b442-dce1c8fd3d2c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 20)                200       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 6         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 581\n",
            "Trainable params: 581\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah4DDpjD9580",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36fcea1a-2505-46cf-c298-5ab098a3c79e"
      },
      "source": [
        "model.compile(optimizer='sgd',loss='mean_absolute_error',metrics=['accuracy'])\n",
        "history = model.fit(trainX,trainY,validation_split=train_validation_split,epochs=epochs,verbose=verbose,shuffle=True)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 248 samples, validate on 62 samples\n",
            "Epoch 1/1000\n",
            "248/248 [==============================] - 0s 844us/step - loss: 21.0359 - acc: 0.0000e+00 - val_loss: 30.6215 - val_acc: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 20.8007 - acc: 0.0000e+00 - val_loss: 30.2664 - val_acc: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 20.3081 - acc: 0.0000e+00 - val_loss: 29.8505 - val_acc: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 19.7651 - acc: 0.0000e+00 - val_loss: 29.4508 - val_acc: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 19.2947 - acc: 0.0000e+00 - val_loss: 29.0617 - val_acc: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 18.8650 - acc: 0.0000e+00 - val_loss: 28.6679 - val_acc: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 18.4428 - acc: 0.0000e+00 - val_loss: 28.2631 - val_acc: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 18.0134 - acc: 0.0000e+00 - val_loss: 27.8416 - val_acc: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 17.5715 - acc: 0.0000e+00 - val_loss: 27.4040 - val_acc: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 17.1183 - acc: 0.0000e+00 - val_loss: 26.9524 - val_acc: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 16.6563 - acc: 0.0000e+00 - val_loss: 26.4912 - val_acc: 0.0000e+00\n",
            "Epoch 12/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 16.1892 - acc: 0.0000e+00 - val_loss: 26.0228 - val_acc: 0.0000e+00\n",
            "Epoch 13/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 15.7179 - acc: 0.0000e+00 - val_loss: 25.5497 - val_acc: 0.0000e+00\n",
            "Epoch 14/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 15.2440 - acc: 0.0000e+00 - val_loss: 25.0736 - val_acc: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 14.7679 - acc: 0.0000e+00 - val_loss: 24.5956 - val_acc: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 14.2907 - acc: 0.0000e+00 - val_loss: 24.1163 - val_acc: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 13.8127 - acc: 0.0000e+00 - val_loss: 23.6365 - val_acc: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 13.3341 - acc: 0.0000e+00 - val_loss: 23.1564 - val_acc: 0.0000e+00\n",
            "Epoch 19/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 12.8552 - acc: 0.0000e+00 - val_loss: 22.6760 - val_acc: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 12.3760 - acc: 0.0000e+00 - val_loss: 22.1954 - val_acc: 0.0000e+00\n",
            "Epoch 21/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 11.8966 - acc: 0.0000e+00 - val_loss: 21.7149 - val_acc: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 11.4171 - acc: 0.0081 - val_loss: 21.2344 - val_acc: 0.0000e+00\n",
            "Epoch 23/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 10.9449 - acc: 0.0081 - val_loss: 20.7615 - val_acc: 0.0000e+00\n",
            "Epoch 24/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 10.4803 - acc: 0.0161 - val_loss: 20.2885 - val_acc: 0.0000e+00\n",
            "Epoch 25/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 10.0252 - acc: 0.0161 - val_loss: 19.8281 - val_acc: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 9.5907 - acc: 0.0161 - val_loss: 19.3713 - val_acc: 0.0000e+00\n",
            "Epoch 27/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 9.1660 - acc: 0.0161 - val_loss: 18.9296 - val_acc: 0.0000e+00\n",
            "Epoch 28/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 8.7603 - acc: 0.0565 - val_loss: 18.4867 - val_acc: 0.0000e+00\n",
            "Epoch 29/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 8.3613 - acc: 0.0726 - val_loss: 18.0713 - val_acc: 0.0000e+00\n",
            "Epoch 30/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 8.0518 - acc: 0.0605 - val_loss: 17.7008 - val_acc: 0.0000e+00\n",
            "Epoch 31/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 7.7646 - acc: 0.0645 - val_loss: 17.3267 - val_acc: 0.0000e+00\n",
            "Epoch 32/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 7.4982 - acc: 0.0645 - val_loss: 17.0000 - val_acc: 0.0000e+00\n",
            "Epoch 33/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 7.2952 - acc: 0.0645 - val_loss: 16.6883 - val_acc: 0.0000e+00\n",
            "Epoch 34/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 7.0944 - acc: 0.0524 - val_loss: 16.3767 - val_acc: 0.0000e+00\n",
            "Epoch 35/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 6.9008 - acc: 0.0524 - val_loss: 16.0889 - val_acc: 0.0000e+00\n",
            "Epoch 36/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 6.7507 - acc: 0.0524 - val_loss: 15.8285 - val_acc: 0.0000e+00\n",
            "Epoch 37/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 6.6153 - acc: 0.0323 - val_loss: 15.5782 - val_acc: 0.0000e+00\n",
            "Epoch 38/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 6.4881 - acc: 0.0363 - val_loss: 15.3341 - val_acc: 0.0000e+00\n",
            "Epoch 39/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 6.3677 - acc: 0.0363 - val_loss: 15.1063 - val_acc: 0.0000e+00\n",
            "Epoch 40/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 6.2734 - acc: 0.0363 - val_loss: 14.8985 - val_acc: 0.0000e+00\n",
            "Epoch 41/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 6.1858 - acc: 0.0323 - val_loss: 14.6983 - val_acc: 0.0000e+00\n",
            "Epoch 42/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 6.1028 - acc: 0.0282 - val_loss: 14.4993 - val_acc: 0.0000e+00\n",
            "Epoch 43/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 6.0206 - acc: 0.0282 - val_loss: 14.3041 - val_acc: 0.0000e+00\n",
            "Epoch 44/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 5.9463 - acc: 0.0282 - val_loss: 14.1301 - val_acc: 0.0000e+00\n",
            "Epoch 45/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 5.8873 - acc: 0.0282 - val_loss: 13.9674 - val_acc: 0.0000e+00\n",
            "Epoch 46/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.8308 - acc: 0.0282 - val_loss: 13.7985 - val_acc: 0.0000e+00\n",
            "Epoch 47/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.7749 - acc: 0.0484 - val_loss: 13.6496 - val_acc: 0.0000e+00\n",
            "Epoch 48/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 5.7311 - acc: 0.0484 - val_loss: 13.5140 - val_acc: 0.0000e+00\n",
            "Epoch 49/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 5.6886 - acc: 0.0484 - val_loss: 13.3747 - val_acc: 0.0000e+00\n",
            "Epoch 50/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.6453 - acc: 0.0484 - val_loss: 13.2379 - val_acc: 0.0000e+00\n",
            "Epoch 51/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 5.6156 - acc: 0.0484 - val_loss: 13.1434 - val_acc: 0.0000e+00\n",
            "Epoch 52/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 5.5971 - acc: 0.0484 - val_loss: 13.0562 - val_acc: 0.0000e+00\n",
            "Epoch 53/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 5.5806 - acc: 0.0484 - val_loss: 12.9738 - val_acc: 0.0000e+00\n",
            "Epoch 54/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 5.5661 - acc: 0.0484 - val_loss: 12.8939 - val_acc: 0.0000e+00\n",
            "Epoch 55/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 5.5510 - acc: 0.0484 - val_loss: 12.8091 - val_acc: 0.0000e+00\n",
            "Epoch 56/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 5.5355 - acc: 0.0605 - val_loss: 12.7195 - val_acc: 0.0000e+00\n",
            "Epoch 57/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 5.5208 - acc: 0.0403 - val_loss: 12.6457 - val_acc: 0.0000e+00\n",
            "Epoch 58/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 5.5087 - acc: 0.0403 - val_loss: 12.5706 - val_acc: 0.0000e+00\n",
            "Epoch 59/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 5.4968 - acc: 0.0403 - val_loss: 12.4955 - val_acc: 0.0000e+00\n",
            "Epoch 60/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.4849 - acc: 0.0403 - val_loss: 12.4229 - val_acc: 0.0000e+00\n",
            "Epoch 61/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 5.4729 - acc: 0.0403 - val_loss: 12.3479 - val_acc: 0.0000e+00\n",
            "Epoch 62/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 5.4609 - acc: 0.0403 - val_loss: 12.2765 - val_acc: 0.0000e+00\n",
            "Epoch 63/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 5.4536 - acc: 0.0403 - val_loss: 12.2377 - val_acc: 0.0000e+00\n",
            "Epoch 64/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.4508 - acc: 0.0403 - val_loss: 12.2027 - val_acc: 0.0000e+00\n",
            "Epoch 65/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 5.4482 - acc: 0.0403 - val_loss: 12.1734 - val_acc: 0.0000e+00\n",
            "Epoch 66/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 5.4452 - acc: 0.0403 - val_loss: 12.1418 - val_acc: 0.0000e+00\n",
            "Epoch 67/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 5.4425 - acc: 0.0403 - val_loss: 12.1091 - val_acc: 0.0000e+00\n",
            "Epoch 68/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.4403 - acc: 0.0403 - val_loss: 12.0787 - val_acc: 0.0000e+00\n",
            "Epoch 69/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 5.4382 - acc: 0.0403 - val_loss: 12.0507 - val_acc: 0.0000e+00\n",
            "Epoch 70/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 5.4366 - acc: 0.0403 - val_loss: 12.0251 - val_acc: 0.0000e+00\n",
            "Epoch 71/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 5.4352 - acc: 0.0403 - val_loss: 11.9994 - val_acc: 0.0000e+00\n",
            "Epoch 72/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 5.4327 - acc: 0.0403 - val_loss: 11.9702 - val_acc: 0.0000e+00\n",
            "Epoch 73/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 5.4306 - acc: 0.0403 - val_loss: 11.9411 - val_acc: 0.0000e+00\n",
            "Epoch 74/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 5.4281 - acc: 0.0403 - val_loss: 11.9143 - val_acc: 0.0000e+00\n",
            "Epoch 75/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 5.4255 - acc: 0.0403 - val_loss: 11.8852 - val_acc: 0.0000e+00\n",
            "Epoch 76/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 5.4240 - acc: 0.0403 - val_loss: 11.8643 - val_acc: 0.0000e+00\n",
            "Epoch 77/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 5.4226 - acc: 0.0403 - val_loss: 11.8376 - val_acc: 0.0000e+00\n",
            "Epoch 78/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 5.4206 - acc: 0.0282 - val_loss: 11.8168 - val_acc: 0.0000e+00\n",
            "Epoch 79/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 5.4182 - acc: 0.0323 - val_loss: 11.7960 - val_acc: 0.0000e+00\n",
            "Epoch 80/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.4163 - acc: 0.0242 - val_loss: 11.7740 - val_acc: 0.0000e+00\n",
            "Epoch 81/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 5.4144 - acc: 0.0242 - val_loss: 11.7521 - val_acc: 0.0000e+00\n",
            "Epoch 82/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 5.4126 - acc: 0.0282 - val_loss: 11.7373 - val_acc: 0.0000e+00\n",
            "Epoch 83/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 5.4106 - acc: 0.0282 - val_loss: 11.7131 - val_acc: 0.0000e+00\n",
            "Epoch 84/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 5.4081 - acc: 0.0282 - val_loss: 11.6972 - val_acc: 0.0000e+00\n",
            "Epoch 85/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 5.4050 - acc: 0.0282 - val_loss: 11.6802 - val_acc: 0.0000e+00\n",
            "Epoch 86/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 5.4021 - acc: 0.0282 - val_loss: 11.6610 - val_acc: 0.0000e+00\n",
            "Epoch 87/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 5.3984 - acc: 0.0282 - val_loss: 11.6420 - val_acc: 0.0000e+00\n",
            "Epoch 88/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 5.3925 - acc: 0.0282 - val_loss: 11.6208 - val_acc: 0.0000e+00\n",
            "Epoch 89/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 5.3855 - acc: 0.0282 - val_loss: 11.6036 - val_acc: 0.0000e+00\n",
            "Epoch 90/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 5.3735 - acc: 0.0282 - val_loss: 11.5834 - val_acc: 0.0000e+00\n",
            "Epoch 91/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 5.3461 - acc: 0.0282 - val_loss: 11.5673 - val_acc: 0.0000e+00\n",
            "Epoch 92/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 5.2395 - acc: 0.0242 - val_loss: 11.5518 - val_acc: 0.0000e+00\n",
            "Epoch 93/1000\n",
            "248/248 [==============================] - 0s 104us/step - loss: 4.6887 - acc: 0.0323 - val_loss: 11.4894 - val_acc: 0.0000e+00\n",
            "Epoch 94/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 4.2435 - acc: 0.0565 - val_loss: 11.3652 - val_acc: 0.0000e+00\n",
            "Epoch 95/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 4.1351 - acc: 0.0524 - val_loss: 11.2071 - val_acc: 0.0000e+00\n",
            "Epoch 96/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 4.0593 - acc: 0.0847 - val_loss: 11.0837 - val_acc: 0.0000e+00\n",
            "Epoch 97/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 3.9763 - acc: 0.0887 - val_loss: 10.9492 - val_acc: 0.0000e+00\n",
            "Epoch 98/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.9222 - acc: 0.0806 - val_loss: 10.8284 - val_acc: 0.0000e+00\n",
            "Epoch 99/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 3.8456 - acc: 0.1048 - val_loss: 10.7189 - val_acc: 0.0000e+00\n",
            "Epoch 100/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 3.7813 - acc: 0.1129 - val_loss: 10.6035 - val_acc: 0.0000e+00\n",
            "Epoch 101/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.7345 - acc: 0.0927 - val_loss: 10.4825 - val_acc: 0.0000e+00\n",
            "Epoch 102/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 3.6952 - acc: 0.1008 - val_loss: 10.3752 - val_acc: 0.0000e+00\n",
            "Epoch 103/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 3.6398 - acc: 0.0968 - val_loss: 10.2987 - val_acc: 0.0000e+00\n",
            "Epoch 104/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 3.6004 - acc: 0.1008 - val_loss: 10.1837 - val_acc: 0.0000e+00\n",
            "Epoch 105/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.5720 - acc: 0.0927 - val_loss: 10.1032 - val_acc: 0.0000e+00\n",
            "Epoch 106/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 3.5067 - acc: 0.1089 - val_loss: 9.9918 - val_acc: 0.0000e+00\n",
            "Epoch 107/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 3.4577 - acc: 0.1169 - val_loss: 9.9191 - val_acc: 0.0000e+00\n",
            "Epoch 108/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 3.3236 - acc: 0.1532 - val_loss: 9.8121 - val_acc: 0.0000e+00\n",
            "Epoch 109/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 3.3074 - acc: 0.1210 - val_loss: 9.6980 - val_acc: 0.0161\n",
            "Epoch 110/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 3.2249 - acc: 0.1290 - val_loss: 9.5795 - val_acc: 0.0161\n",
            "Epoch 111/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 3.1623 - acc: 0.1573 - val_loss: 9.4771 - val_acc: 0.0161\n",
            "Epoch 112/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 3.1458 - acc: 0.1532 - val_loss: 9.3974 - val_acc: 0.0161\n",
            "Epoch 113/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 3.0870 - acc: 0.1653 - val_loss: 9.3447 - val_acc: 0.0161\n",
            "Epoch 114/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 3.0824 - acc: 0.1331 - val_loss: 9.2298 - val_acc: 0.0161\n",
            "Epoch 115/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 3.0234 - acc: 0.1411 - val_loss: 9.1347 - val_acc: 0.0161\n",
            "Epoch 116/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 2.9999 - acc: 0.1653 - val_loss: 9.0218 - val_acc: 0.0161\n",
            "Epoch 117/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 3.0214 - acc: 0.1331 - val_loss: 8.9380 - val_acc: 0.0161\n",
            "Epoch 118/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.9771 - acc: 0.1532 - val_loss: 8.8566 - val_acc: 0.0161\n",
            "Epoch 119/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.9203 - acc: 0.1452 - val_loss: 8.7786 - val_acc: 0.0161\n",
            "Epoch 120/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.9275 - acc: 0.1411 - val_loss: 8.6859 - val_acc: 0.0000e+00\n",
            "Epoch 121/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.9074 - acc: 0.1532 - val_loss: 8.6315 - val_acc: 0.0000e+00\n",
            "Epoch 122/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.8349 - acc: 0.1492 - val_loss: 8.5835 - val_acc: 0.0000e+00\n",
            "Epoch 123/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.8399 - acc: 0.1371 - val_loss: 8.5420 - val_acc: 0.0161\n",
            "Epoch 124/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.8331 - acc: 0.1573 - val_loss: 8.4480 - val_acc: 0.0000e+00\n",
            "Epoch 125/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.7811 - acc: 0.1411 - val_loss: 8.3839 - val_acc: 0.0000e+00\n",
            "Epoch 126/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.7712 - acc: 0.1532 - val_loss: 8.3296 - val_acc: 0.0000e+00\n",
            "Epoch 127/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 2.7481 - acc: 0.1532 - val_loss: 8.2541 - val_acc: 0.0000e+00\n",
            "Epoch 128/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.7816 - acc: 0.1290 - val_loss: 8.2138 - val_acc: 0.0000e+00\n",
            "Epoch 129/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.7641 - acc: 0.1008 - val_loss: 8.1368 - val_acc: 0.0161\n",
            "Epoch 130/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.7188 - acc: 0.1331 - val_loss: 8.0552 - val_acc: 0.0161\n",
            "Epoch 131/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.6938 - acc: 0.1371 - val_loss: 8.0162 - val_acc: 0.0161\n",
            "Epoch 132/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.6817 - acc: 0.1492 - val_loss: 7.9332 - val_acc: 0.0161\n",
            "Epoch 133/1000\n",
            "248/248 [==============================] - 0s 85us/step - loss: 2.6472 - acc: 0.1532 - val_loss: 7.9009 - val_acc: 0.0161\n",
            "Epoch 134/1000\n",
            "248/248 [==============================] - 0s 98us/step - loss: 2.6482 - acc: 0.1250 - val_loss: 7.8447 - val_acc: 0.0161\n",
            "Epoch 135/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.6180 - acc: 0.1573 - val_loss: 7.8470 - val_acc: 0.0161\n",
            "Epoch 136/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.5909 - acc: 0.1452 - val_loss: 7.7975 - val_acc: 0.0161\n",
            "Epoch 137/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 2.5756 - acc: 0.1734 - val_loss: 7.7767 - val_acc: 0.0161\n",
            "Epoch 138/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.5385 - acc: 0.1976 - val_loss: 7.6919 - val_acc: 0.0161\n",
            "Epoch 139/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.5841 - acc: 0.1613 - val_loss: 7.7668 - val_acc: 0.0323\n",
            "Epoch 140/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 2.5560 - acc: 0.1573 - val_loss: 7.6746 - val_acc: 0.0161\n",
            "Epoch 141/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 2.5148 - acc: 0.2137 - val_loss: 7.5894 - val_acc: 0.0161\n",
            "Epoch 142/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.5450 - acc: 0.1452 - val_loss: 7.5233 - val_acc: 0.0161\n",
            "Epoch 143/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.4928 - acc: 0.1734 - val_loss: 7.5506 - val_acc: 0.0161\n",
            "Epoch 144/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.4973 - acc: 0.1976 - val_loss: 7.5582 - val_acc: 0.0161\n",
            "Epoch 145/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.4536 - acc: 0.1935 - val_loss: 7.4877 - val_acc: 0.0161\n",
            "Epoch 146/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 2.4356 - acc: 0.1976 - val_loss: 7.4258 - val_acc: 0.0161\n",
            "Epoch 147/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 2.4786 - acc: 0.1734 - val_loss: 7.3822 - val_acc: 0.0161\n",
            "Epoch 148/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.4434 - acc: 0.1653 - val_loss: 7.4476 - val_acc: 0.0161\n",
            "Epoch 149/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.4683 - acc: 0.1532 - val_loss: 7.3983 - val_acc: 0.0161\n",
            "Epoch 150/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.4207 - acc: 0.1815 - val_loss: 7.3500 - val_acc: 0.0161\n",
            "Epoch 151/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.4337 - acc: 0.1653 - val_loss: 7.2392 - val_acc: 0.0161\n",
            "Epoch 152/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.3798 - acc: 0.1774 - val_loss: 7.2029 - val_acc: 0.0161\n",
            "Epoch 153/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.4063 - acc: 0.1815 - val_loss: 7.1846 - val_acc: 0.0161\n",
            "Epoch 154/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.3501 - acc: 0.1895 - val_loss: 7.1868 - val_acc: 0.0161\n",
            "Epoch 155/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.4025 - acc: 0.1774 - val_loss: 7.1808 - val_acc: 0.0161\n",
            "Epoch 156/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.3440 - acc: 0.1855 - val_loss: 7.1485 - val_acc: 0.0161\n",
            "Epoch 157/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 2.3917 - acc: 0.1734 - val_loss: 7.0613 - val_acc: 0.0000e+00\n",
            "Epoch 158/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.3085 - acc: 0.1976 - val_loss: 7.0899 - val_acc: 0.0161\n",
            "Epoch 159/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.3141 - acc: 0.1653 - val_loss: 7.1171 - val_acc: 0.0161\n",
            "Epoch 160/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.3892 - acc: 0.1411 - val_loss: 7.0302 - val_acc: 0.0000e+00\n",
            "Epoch 161/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.3147 - acc: 0.1371 - val_loss: 6.9868 - val_acc: 0.0000e+00\n",
            "Epoch 162/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.3082 - acc: 0.1613 - val_loss: 7.0634 - val_acc: 0.0161\n",
            "Epoch 163/1000\n",
            "248/248 [==============================] - 0s 79us/step - loss: 2.2949 - acc: 0.1573 - val_loss: 6.9854 - val_acc: 0.0000e+00\n",
            "Epoch 164/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 2.3011 - acc: 0.1573 - val_loss: 7.0449 - val_acc: 0.0161\n",
            "Epoch 165/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 2.2566 - acc: 0.1734 - val_loss: 6.9328 - val_acc: 0.0000e+00\n",
            "Epoch 166/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.2876 - acc: 0.1573 - val_loss: 6.8226 - val_acc: 0.0000e+00\n",
            "Epoch 167/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 2.2237 - acc: 0.1774 - val_loss: 6.9610 - val_acc: 0.0000e+00\n",
            "Epoch 168/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 2.2239 - acc: 0.1734 - val_loss: 6.6944 - val_acc: 0.0161\n",
            "Epoch 169/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.2214 - acc: 0.1734 - val_loss: 6.6883 - val_acc: 0.0000e+00\n",
            "Epoch 170/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 2.1753 - acc: 0.1694 - val_loss: 6.7827 - val_acc: 0.0000e+00\n",
            "Epoch 171/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.1723 - acc: 0.1774 - val_loss: 6.6300 - val_acc: 0.0161\n",
            "Epoch 172/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 2.1684 - acc: 0.1573 - val_loss: 6.6159 - val_acc: 0.0161\n",
            "Epoch 173/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 2.1464 - acc: 0.1895 - val_loss: 6.5333 - val_acc: 0.0161\n",
            "Epoch 174/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 2.1484 - acc: 0.1855 - val_loss: 6.4463 - val_acc: 0.0161\n",
            "Epoch 175/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 2.0981 - acc: 0.1895 - val_loss: 6.3646 - val_acc: 0.0323\n",
            "Epoch 176/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 2.0821 - acc: 0.2177 - val_loss: 6.3930 - val_acc: 0.0161\n",
            "Epoch 177/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 2.1619 - acc: 0.1331 - val_loss: 6.2547 - val_acc: 0.0323\n",
            "Epoch 178/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 2.0961 - acc: 0.1976 - val_loss: 6.2142 - val_acc: 0.0323\n",
            "Epoch 179/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 2.0990 - acc: 0.1169 - val_loss: 6.1505 - val_acc: 0.0323\n",
            "Epoch 180/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 2.0784 - acc: 0.1613 - val_loss: 6.1442 - val_acc: 0.0323\n",
            "Epoch 181/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.9928 - acc: 0.2460 - val_loss: 6.1691 - val_acc: 0.0323\n",
            "Epoch 182/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 2.0531 - acc: 0.1573 - val_loss: 6.0354 - val_acc: 0.0323\n",
            "Epoch 183/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0156 - acc: 0.1734 - val_loss: 6.0116 - val_acc: 0.0323\n",
            "Epoch 184/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 2.0040 - acc: 0.1935 - val_loss: 5.9425 - val_acc: 0.0323\n",
            "Epoch 185/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 2.0347 - acc: 0.1895 - val_loss: 5.9477 - val_acc: 0.0323\n",
            "Epoch 186/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.9753 - acc: 0.2177 - val_loss: 5.8774 - val_acc: 0.0161\n",
            "Epoch 187/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.9382 - acc: 0.2339 - val_loss: 5.7932 - val_acc: 0.0323\n",
            "Epoch 188/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.9503 - acc: 0.1774 - val_loss: 5.8088 - val_acc: 0.0161\n",
            "Epoch 189/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.9501 - acc: 0.1734 - val_loss: 5.7144 - val_acc: 0.0323\n",
            "Epoch 190/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.8939 - acc: 0.2218 - val_loss: 5.6582 - val_acc: 0.0323\n",
            "Epoch 191/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.8987 - acc: 0.2218 - val_loss: 5.6377 - val_acc: 0.0323\n",
            "Epoch 192/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.9143 - acc: 0.1976 - val_loss: 5.6225 - val_acc: 0.0323\n",
            "Epoch 193/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.8949 - acc: 0.2016 - val_loss: 5.6091 - val_acc: 0.0323\n",
            "Epoch 194/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.9753 - acc: 0.1694 - val_loss: 5.5318 - val_acc: 0.0645\n",
            "Epoch 195/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.8789 - acc: 0.2056 - val_loss: 5.5129 - val_acc: 0.0645\n",
            "Epoch 196/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.9197 - acc: 0.1452 - val_loss: 5.4625 - val_acc: 0.0645\n",
            "Epoch 197/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.8505 - acc: 0.2177 - val_loss: 5.4413 - val_acc: 0.0645\n",
            "Epoch 198/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.8345 - acc: 0.1694 - val_loss: 5.3848 - val_acc: 0.0645\n",
            "Epoch 199/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.8242 - acc: 0.1895 - val_loss: 5.3651 - val_acc: 0.0645\n",
            "Epoch 200/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.8334 - acc: 0.1855 - val_loss: 5.3459 - val_acc: 0.0484\n",
            "Epoch 201/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.8657 - acc: 0.1734 - val_loss: 5.2886 - val_acc: 0.0484\n",
            "Epoch 202/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.8010 - acc: 0.2218 - val_loss: 5.2941 - val_acc: 0.0484\n",
            "Epoch 203/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.7993 - acc: 0.1976 - val_loss: 5.2938 - val_acc: 0.0645\n",
            "Epoch 204/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.7813 - acc: 0.2339 - val_loss: 5.2203 - val_acc: 0.0484\n",
            "Epoch 205/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.7573 - acc: 0.2097 - val_loss: 5.1959 - val_acc: 0.0484\n",
            "Epoch 206/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.7551 - acc: 0.2097 - val_loss: 5.1592 - val_acc: 0.0323\n",
            "Epoch 207/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.8113 - acc: 0.1935 - val_loss: 5.1536 - val_acc: 0.0484\n",
            "Epoch 208/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.7602 - acc: 0.1774 - val_loss: 5.1354 - val_acc: 0.0484\n",
            "Epoch 209/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.7648 - acc: 0.2137 - val_loss: 5.1240 - val_acc: 0.0161\n",
            "Epoch 210/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.7561 - acc: 0.1895 - val_loss: 5.0997 - val_acc: 0.0161\n",
            "Epoch 211/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.7437 - acc: 0.1976 - val_loss: 5.0557 - val_acc: 0.0161\n",
            "Epoch 212/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.7700 - acc: 0.1895 - val_loss: 5.0605 - val_acc: 0.0161\n",
            "Epoch 213/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.7242 - acc: 0.2097 - val_loss: 5.0014 - val_acc: 0.0161\n",
            "Epoch 214/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.7781 - acc: 0.1855 - val_loss: 4.9917 - val_acc: 0.0161\n",
            "Epoch 215/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6908 - acc: 0.2056 - val_loss: 4.9692 - val_acc: 0.0161\n",
            "Epoch 216/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6492 - acc: 0.2258 - val_loss: 4.9305 - val_acc: 0.0161\n",
            "Epoch 217/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.7146 - acc: 0.1935 - val_loss: 4.8989 - val_acc: 0.0161\n",
            "Epoch 218/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.6520 - acc: 0.2339 - val_loss: 4.8906 - val_acc: 0.0323\n",
            "Epoch 219/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.6922 - acc: 0.2016 - val_loss: 4.8912 - val_acc: 0.0323\n",
            "Epoch 220/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6609 - acc: 0.2097 - val_loss: 4.8913 - val_acc: 0.0161\n",
            "Epoch 221/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6912 - acc: 0.2056 - val_loss: 4.8472 - val_acc: 0.0323\n",
            "Epoch 222/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6468 - acc: 0.1935 - val_loss: 4.8330 - val_acc: 0.0161\n",
            "Epoch 223/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 1.6811 - acc: 0.2097 - val_loss: 4.8035 - val_acc: 0.0161\n",
            "Epoch 224/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.6620 - acc: 0.2056 - val_loss: 4.8277 - val_acc: 0.0161\n",
            "Epoch 225/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.6725 - acc: 0.2016 - val_loss: 4.7746 - val_acc: 0.0161\n",
            "Epoch 226/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.6796 - acc: 0.1895 - val_loss: 4.7844 - val_acc: 0.0161\n",
            "Epoch 227/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 1.5736 - acc: 0.2460 - val_loss: 4.7693 - val_acc: 0.0161\n",
            "Epoch 228/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.6155 - acc: 0.2379 - val_loss: 4.7462 - val_acc: 0.0161\n",
            "Epoch 229/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.6119 - acc: 0.2298 - val_loss: 4.7524 - val_acc: 0.0161\n",
            "Epoch 230/1000\n",
            "248/248 [==============================] - 0s 79us/step - loss: 1.6415 - acc: 0.2218 - val_loss: 4.7323 - val_acc: 0.0161\n",
            "Epoch 231/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.6663 - acc: 0.1774 - val_loss: 4.6845 - val_acc: 0.0161\n",
            "Epoch 232/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 1.6233 - acc: 0.1935 - val_loss: 4.7057 - val_acc: 0.0161\n",
            "Epoch 233/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.5732 - acc: 0.2460 - val_loss: 4.6767 - val_acc: 0.0161\n",
            "Epoch 234/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.6227 - acc: 0.1895 - val_loss: 4.6572 - val_acc: 0.0161\n",
            "Epoch 235/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.5736 - acc: 0.2177 - val_loss: 4.6482 - val_acc: 0.0161\n",
            "Epoch 236/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.5858 - acc: 0.2097 - val_loss: 4.6501 - val_acc: 0.0323\n",
            "Epoch 237/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.6227 - acc: 0.2056 - val_loss: 4.5981 - val_acc: 0.0323\n",
            "Epoch 238/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.5738 - acc: 0.2258 - val_loss: 4.6109 - val_acc: 0.0323\n",
            "Epoch 239/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.5520 - acc: 0.1935 - val_loss: 4.5950 - val_acc: 0.0323\n",
            "Epoch 240/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.5455 - acc: 0.1976 - val_loss: 4.5858 - val_acc: 0.0323\n",
            "Epoch 241/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.5389 - acc: 0.2218 - val_loss: 4.5286 - val_acc: 0.0323\n",
            "Epoch 242/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.5854 - acc: 0.2218 - val_loss: 4.5339 - val_acc: 0.0323\n",
            "Epoch 243/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.5885 - acc: 0.1935 - val_loss: 4.5204 - val_acc: 0.0323\n",
            "Epoch 244/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.5303 - acc: 0.2298 - val_loss: 4.5217 - val_acc: 0.0323\n",
            "Epoch 245/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.5309 - acc: 0.2177 - val_loss: 4.4973 - val_acc: 0.0323\n",
            "Epoch 246/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.5668 - acc: 0.2177 - val_loss: 4.4984 - val_acc: 0.0161\n",
            "Epoch 247/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.5374 - acc: 0.2419 - val_loss: 4.4896 - val_acc: 0.0161\n",
            "Epoch 248/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.4820 - acc: 0.2540 - val_loss: 4.4521 - val_acc: 0.0161\n",
            "Epoch 249/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.4794 - acc: 0.2177 - val_loss: 4.4447 - val_acc: 0.0161\n",
            "Epoch 250/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.5651 - acc: 0.1815 - val_loss: 4.4818 - val_acc: 0.0161\n",
            "Epoch 251/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.5277 - acc: 0.2379 - val_loss: 4.4365 - val_acc: 0.0161\n",
            "Epoch 252/1000\n",
            "248/248 [==============================] - 0s 86us/step - loss: 1.4870 - acc: 0.2258 - val_loss: 4.4386 - val_acc: 0.0161\n",
            "Epoch 253/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.5241 - acc: 0.1935 - val_loss: 4.4462 - val_acc: 0.0161\n",
            "Epoch 254/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4815 - acc: 0.2339 - val_loss: 4.4245 - val_acc: 0.0161\n",
            "Epoch 255/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.5245 - acc: 0.2056 - val_loss: 4.4428 - val_acc: 0.0161\n",
            "Epoch 256/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.5026 - acc: 0.2137 - val_loss: 4.4307 - val_acc: 0.0161\n",
            "Epoch 257/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.4744 - acc: 0.2258 - val_loss: 4.4411 - val_acc: 0.0161\n",
            "Epoch 258/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.5875 - acc: 0.1895 - val_loss: 4.3745 - val_acc: 0.0161\n",
            "Epoch 259/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.5946 - acc: 0.1734 - val_loss: 4.3399 - val_acc: 0.0161\n",
            "Epoch 260/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4587 - acc: 0.2137 - val_loss: 4.3571 - val_acc: 0.0161\n",
            "Epoch 261/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.5045 - acc: 0.2177 - val_loss: 4.3161 - val_acc: 0.0161\n",
            "Epoch 262/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4182 - acc: 0.2581 - val_loss: 4.3498 - val_acc: 0.0161\n",
            "Epoch 263/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.4224 - acc: 0.2661 - val_loss: 4.3318 - val_acc: 0.0323\n",
            "Epoch 264/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.4492 - acc: 0.2137 - val_loss: 4.3055 - val_acc: 0.0323\n",
            "Epoch 265/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.4639 - acc: 0.2339 - val_loss: 4.3148 - val_acc: 0.0323\n",
            "Epoch 266/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.5062 - acc: 0.2258 - val_loss: 4.2885 - val_acc: 0.0323\n",
            "Epoch 267/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.4770 - acc: 0.2056 - val_loss: 4.3447 - val_acc: 0.0323\n",
            "Epoch 268/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.4322 - acc: 0.2379 - val_loss: 4.2978 - val_acc: 0.0323\n",
            "Epoch 269/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4746 - acc: 0.2379 - val_loss: 4.2945 - val_acc: 0.0161\n",
            "Epoch 270/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.4279 - acc: 0.2218 - val_loss: 4.2823 - val_acc: 0.0161\n",
            "Epoch 271/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.4950 - acc: 0.1976 - val_loss: 4.2591 - val_acc: 0.0161\n",
            "Epoch 272/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.4933 - acc: 0.2177 - val_loss: 4.2944 - val_acc: 0.0323\n",
            "Epoch 273/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.4174 - acc: 0.2419 - val_loss: 4.2497 - val_acc: 0.0323\n",
            "Epoch 274/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 1.4370 - acc: 0.2137 - val_loss: 4.2202 - val_acc: 0.0323\n",
            "Epoch 275/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4591 - acc: 0.1895 - val_loss: 4.1972 - val_acc: 0.0323\n",
            "Epoch 276/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4177 - acc: 0.2540 - val_loss: 4.2349 - val_acc: 0.0323\n",
            "Epoch 277/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3866 - acc: 0.2339 - val_loss: 4.2634 - val_acc: 0.0323\n",
            "Epoch 278/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 1.4353 - acc: 0.2056 - val_loss: 4.2383 - val_acc: 0.0323\n",
            "Epoch 279/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.4209 - acc: 0.2016 - val_loss: 4.1822 - val_acc: 0.0323\n",
            "Epoch 280/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.4940 - acc: 0.1935 - val_loss: 4.1545 - val_acc: 0.0323\n",
            "Epoch 281/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4441 - acc: 0.1976 - val_loss: 4.1296 - val_acc: 0.0323\n",
            "Epoch 282/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3938 - acc: 0.1935 - val_loss: 4.1844 - val_acc: 0.0323\n",
            "Epoch 283/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3917 - acc: 0.2581 - val_loss: 4.2285 - val_acc: 0.0323\n",
            "Epoch 284/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.4713 - acc: 0.1895 - val_loss: 4.1971 - val_acc: 0.0323\n",
            "Epoch 285/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.4302 - acc: 0.2137 - val_loss: 4.1166 - val_acc: 0.0323\n",
            "Epoch 286/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.4559 - acc: 0.2298 - val_loss: 4.1416 - val_acc: 0.0323\n",
            "Epoch 287/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.4264 - acc: 0.2097 - val_loss: 4.2181 - val_acc: 0.0323\n",
            "Epoch 288/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4244 - acc: 0.2137 - val_loss: 4.1365 - val_acc: 0.0323\n",
            "Epoch 289/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.4139 - acc: 0.2218 - val_loss: 4.1445 - val_acc: 0.0323\n",
            "Epoch 290/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.4260 - acc: 0.2258 - val_loss: 4.1128 - val_acc: 0.0323\n",
            "Epoch 291/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.3720 - acc: 0.2298 - val_loss: 4.0929 - val_acc: 0.0323\n",
            "Epoch 292/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3634 - acc: 0.2581 - val_loss: 4.1901 - val_acc: 0.0323\n",
            "Epoch 293/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.4824 - acc: 0.1855 - val_loss: 4.0919 - val_acc: 0.0323\n",
            "Epoch 294/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.4167 - acc: 0.2258 - val_loss: 4.0963 - val_acc: 0.0323\n",
            "Epoch 295/1000\n",
            "248/248 [==============================] - 0s 94us/step - loss: 1.4335 - acc: 0.2177 - val_loss: 4.1094 - val_acc: 0.0323\n",
            "Epoch 296/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3832 - acc: 0.2056 - val_loss: 4.0726 - val_acc: 0.0323\n",
            "Epoch 297/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.3329 - acc: 0.2540 - val_loss: 4.0555 - val_acc: 0.0323\n",
            "Epoch 298/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3509 - acc: 0.2540 - val_loss: 4.0718 - val_acc: 0.0323\n",
            "Epoch 299/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 1.3855 - acc: 0.2379 - val_loss: 4.0504 - val_acc: 0.0323\n",
            "Epoch 300/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.4229 - acc: 0.2581 - val_loss: 4.0116 - val_acc: 0.0323\n",
            "Epoch 301/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.3679 - acc: 0.1895 - val_loss: 4.0381 - val_acc: 0.0323\n",
            "Epoch 302/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.4361 - acc: 0.2258 - val_loss: 4.0250 - val_acc: 0.0484\n",
            "Epoch 303/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4016 - acc: 0.2661 - val_loss: 3.9819 - val_acc: 0.0323\n",
            "Epoch 304/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.3910 - acc: 0.2137 - val_loss: 3.9970 - val_acc: 0.0484\n",
            "Epoch 305/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3311 - acc: 0.2218 - val_loss: 3.9978 - val_acc: 0.0484\n",
            "Epoch 306/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3730 - acc: 0.2298 - val_loss: 3.9792 - val_acc: 0.0484\n",
            "Epoch 307/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.3783 - acc: 0.2177 - val_loss: 3.9256 - val_acc: 0.0484\n",
            "Epoch 308/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.3327 - acc: 0.2661 - val_loss: 3.9590 - val_acc: 0.0484\n",
            "Epoch 309/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3185 - acc: 0.2460 - val_loss: 4.0095 - val_acc: 0.0484\n",
            "Epoch 310/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.3650 - acc: 0.2097 - val_loss: 4.0021 - val_acc: 0.0484\n",
            "Epoch 311/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3359 - acc: 0.2379 - val_loss: 3.9278 - val_acc: 0.0484\n",
            "Epoch 312/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.4029 - acc: 0.1774 - val_loss: 3.9710 - val_acc: 0.0323\n",
            "Epoch 313/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.4062 - acc: 0.2137 - val_loss: 4.0433 - val_acc: 0.0484\n",
            "Epoch 314/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3630 - acc: 0.2419 - val_loss: 3.9197 - val_acc: 0.0484\n",
            "Epoch 315/1000\n",
            "248/248 [==============================] - 0s 84us/step - loss: 1.3324 - acc: 0.2298 - val_loss: 3.9077 - val_acc: 0.0323\n",
            "Epoch 316/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3222 - acc: 0.2460 - val_loss: 3.8461 - val_acc: 0.0484\n",
            "Epoch 317/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.3847 - acc: 0.2137 - val_loss: 3.8898 - val_acc: 0.0323\n",
            "Epoch 318/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.3581 - acc: 0.2339 - val_loss: 3.9076 - val_acc: 0.0484\n",
            "Epoch 319/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3119 - acc: 0.2298 - val_loss: 3.8531 - val_acc: 0.0484\n",
            "Epoch 320/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.3611 - acc: 0.2218 - val_loss: 3.9569 - val_acc: 0.0323\n",
            "Epoch 321/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.2995 - acc: 0.2460 - val_loss: 3.8829 - val_acc: 0.0323\n",
            "Epoch 322/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3272 - acc: 0.2218 - val_loss: 3.8622 - val_acc: 0.0323\n",
            "Epoch 323/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.3503 - acc: 0.2460 - val_loss: 3.8917 - val_acc: 0.0323\n",
            "Epoch 324/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3290 - acc: 0.2218 - val_loss: 3.9185 - val_acc: 0.0323\n",
            "Epoch 325/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3388 - acc: 0.2298 - val_loss: 3.8396 - val_acc: 0.0484\n",
            "Epoch 326/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3407 - acc: 0.2218 - val_loss: 3.8359 - val_acc: 0.0323\n",
            "Epoch 327/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.3329 - acc: 0.2339 - val_loss: 3.8280 - val_acc: 0.0484\n",
            "Epoch 328/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.4014 - acc: 0.1734 - val_loss: 3.8947 - val_acc: 0.0323\n",
            "Epoch 329/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.3728 - acc: 0.1976 - val_loss: 3.7920 - val_acc: 0.0323\n",
            "Epoch 330/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.3666 - acc: 0.2177 - val_loss: 3.8151 - val_acc: 0.0484\n",
            "Epoch 331/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.3408 - acc: 0.2379 - val_loss: 3.8639 - val_acc: 0.0323\n",
            "Epoch 332/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.3645 - acc: 0.2097 - val_loss: 3.8116 - val_acc: 0.0323\n",
            "Epoch 333/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.3650 - acc: 0.2379 - val_loss: 3.8889 - val_acc: 0.0323\n",
            "Epoch 334/1000\n",
            "248/248 [==============================] - 0s 84us/step - loss: 1.3188 - acc: 0.2621 - val_loss: 3.8829 - val_acc: 0.0323\n",
            "Epoch 335/1000\n",
            "248/248 [==============================] - 0s 80us/step - loss: 1.3793 - acc: 0.2097 - val_loss: 3.8892 - val_acc: 0.0323\n",
            "Epoch 336/1000\n",
            "248/248 [==============================] - 0s 87us/step - loss: 1.3857 - acc: 0.2016 - val_loss: 3.7700 - val_acc: 0.0323\n",
            "Epoch 337/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.2896 - acc: 0.2419 - val_loss: 3.8323 - val_acc: 0.0323\n",
            "Epoch 338/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2807 - acc: 0.2419 - val_loss: 3.8146 - val_acc: 0.0323\n",
            "Epoch 339/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.3126 - acc: 0.2379 - val_loss: 3.8084 - val_acc: 0.0323\n",
            "Epoch 340/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3280 - acc: 0.2339 - val_loss: 3.7846 - val_acc: 0.0484\n",
            "Epoch 341/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3350 - acc: 0.1935 - val_loss: 3.7854 - val_acc: 0.0323\n",
            "Epoch 342/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.2957 - acc: 0.2379 - val_loss: 3.8089 - val_acc: 0.0323\n",
            "Epoch 343/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.3357 - acc: 0.2137 - val_loss: 3.7839 - val_acc: 0.0323\n",
            "Epoch 344/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3446 - acc: 0.2177 - val_loss: 3.7702 - val_acc: 0.0323\n",
            "Epoch 345/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.2970 - acc: 0.2379 - val_loss: 3.7959 - val_acc: 0.0323\n",
            "Epoch 346/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2961 - acc: 0.2258 - val_loss: 3.8087 - val_acc: 0.0323\n",
            "Epoch 347/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.2588 - acc: 0.2621 - val_loss: 3.7698 - val_acc: 0.0323\n",
            "Epoch 348/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3098 - acc: 0.2097 - val_loss: 3.7812 - val_acc: 0.0323\n",
            "Epoch 349/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3129 - acc: 0.2419 - val_loss: 3.7520 - val_acc: 0.0323\n",
            "Epoch 350/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2675 - acc: 0.2661 - val_loss: 3.8097 - val_acc: 0.0484\n",
            "Epoch 351/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2782 - acc: 0.2379 - val_loss: 3.7140 - val_acc: 0.0323\n",
            "Epoch 352/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3403 - acc: 0.2056 - val_loss: 3.7768 - val_acc: 0.0484\n",
            "Epoch 353/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2908 - acc: 0.2339 - val_loss: 3.7468 - val_acc: 0.0323\n",
            "Epoch 354/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.3398 - acc: 0.2298 - val_loss: 3.7119 - val_acc: 0.0323\n",
            "Epoch 355/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.2541 - acc: 0.2581 - val_loss: 3.6928 - val_acc: 0.0323\n",
            "Epoch 356/1000\n",
            "248/248 [==============================] - 0s 83us/step - loss: 1.3145 - acc: 0.2460 - val_loss: 3.8437 - val_acc: 0.0484\n",
            "Epoch 357/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2813 - acc: 0.2218 - val_loss: 3.7863 - val_acc: 0.0323\n",
            "Epoch 358/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.2893 - acc: 0.2298 - val_loss: 3.7895 - val_acc: 0.0323\n",
            "Epoch 359/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2456 - acc: 0.2500 - val_loss: 3.7410 - val_acc: 0.0484\n",
            "Epoch 360/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3093 - acc: 0.2137 - val_loss: 3.7564 - val_acc: 0.0323\n",
            "Epoch 361/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2842 - acc: 0.2621 - val_loss: 3.7367 - val_acc: 0.0323\n",
            "Epoch 362/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2252 - acc: 0.2540 - val_loss: 3.7661 - val_acc: 0.0323\n",
            "Epoch 363/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3032 - acc: 0.2339 - val_loss: 3.6699 - val_acc: 0.0484\n",
            "Epoch 364/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.2545 - acc: 0.2419 - val_loss: 3.7064 - val_acc: 0.0645\n",
            "Epoch 365/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2859 - acc: 0.2056 - val_loss: 3.8044 - val_acc: 0.0323\n",
            "Epoch 366/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2627 - acc: 0.2621 - val_loss: 3.7957 - val_acc: 0.0323\n",
            "Epoch 367/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.3240 - acc: 0.2621 - val_loss: 3.7962 - val_acc: 0.0161\n",
            "Epoch 368/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.2575 - acc: 0.2500 - val_loss: 3.7716 - val_acc: 0.0000e+00\n",
            "Epoch 369/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.2719 - acc: 0.2702 - val_loss: 3.7674 - val_acc: 0.0161\n",
            "Epoch 370/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3150 - acc: 0.2218 - val_loss: 3.7747 - val_acc: 0.0000e+00\n",
            "Epoch 371/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3557 - acc: 0.2581 - val_loss: 3.7991 - val_acc: 0.0161\n",
            "Epoch 372/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.2847 - acc: 0.2218 - val_loss: 3.7121 - val_acc: 0.0323\n",
            "Epoch 373/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 1.2719 - acc: 0.2298 - val_loss: 3.8029 - val_acc: 0.0161\n",
            "Epoch 374/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2578 - acc: 0.2460 - val_loss: 3.7312 - val_acc: 0.0161\n",
            "Epoch 375/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2190 - acc: 0.2944 - val_loss: 3.7287 - val_acc: 0.0000e+00\n",
            "Epoch 376/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.2157 - acc: 0.2742 - val_loss: 3.7635 - val_acc: 0.0161\n",
            "Epoch 377/1000\n",
            "248/248 [==============================] - 0s 81us/step - loss: 1.2599 - acc: 0.2742 - val_loss: 3.7646 - val_acc: 0.0000e+00\n",
            "Epoch 378/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2530 - acc: 0.2661 - val_loss: 3.7271 - val_acc: 0.0000e+00\n",
            "Epoch 379/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2622 - acc: 0.2702 - val_loss: 3.8029 - val_acc: 0.0161\n",
            "Epoch 380/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2333 - acc: 0.2621 - val_loss: 3.6663 - val_acc: 0.0323\n",
            "Epoch 381/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2355 - acc: 0.2661 - val_loss: 3.8241 - val_acc: 0.0323\n",
            "Epoch 382/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2222 - acc: 0.2903 - val_loss: 3.7495 - val_acc: 0.0323\n",
            "Epoch 383/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.3014 - acc: 0.2298 - val_loss: 3.7666 - val_acc: 0.0484\n",
            "Epoch 384/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.3143 - acc: 0.2702 - val_loss: 3.7004 - val_acc: 0.0484\n",
            "Epoch 385/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.2957 - acc: 0.2782 - val_loss: 3.6509 - val_acc: 0.0323\n",
            "Epoch 386/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 1.2191 - acc: 0.2823 - val_loss: 3.7210 - val_acc: 0.0323\n",
            "Epoch 387/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2751 - acc: 0.2258 - val_loss: 3.6734 - val_acc: 0.0323\n",
            "Epoch 388/1000\n",
            "248/248 [==============================] - 0s 80us/step - loss: 1.2067 - acc: 0.2782 - val_loss: 3.7822 - val_acc: 0.0484\n",
            "Epoch 389/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2097 - acc: 0.2702 - val_loss: 3.6830 - val_acc: 0.0323\n",
            "Epoch 390/1000\n",
            "248/248 [==============================] - 0s 84us/step - loss: 1.2087 - acc: 0.2823 - val_loss: 3.8450 - val_acc: 0.0484\n",
            "Epoch 391/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.3369 - acc: 0.2339 - val_loss: 3.6909 - val_acc: 0.0645\n",
            "Epoch 392/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2158 - acc: 0.2903 - val_loss: 3.7521 - val_acc: 0.0161\n",
            "Epoch 393/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2787 - acc: 0.2258 - val_loss: 3.7210 - val_acc: 0.0323\n",
            "Epoch 394/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2863 - acc: 0.2379 - val_loss: 3.6366 - val_acc: 0.0484\n",
            "Epoch 395/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2789 - acc: 0.2016 - val_loss: 3.6659 - val_acc: 0.0323\n",
            "Epoch 396/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.2230 - acc: 0.2742 - val_loss: 3.6548 - val_acc: 0.0484\n",
            "Epoch 397/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.2305 - acc: 0.2581 - val_loss: 3.7516 - val_acc: 0.0323\n",
            "Epoch 398/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2293 - acc: 0.2702 - val_loss: 3.6750 - val_acc: 0.0484\n",
            "Epoch 399/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1790 - acc: 0.2984 - val_loss: 3.7295 - val_acc: 0.0484\n",
            "Epoch 400/1000\n",
            "248/248 [==============================] - 0s 83us/step - loss: 1.2427 - acc: 0.2863 - val_loss: 3.7377 - val_acc: 0.0323\n",
            "Epoch 401/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.2485 - acc: 0.2823 - val_loss: 3.7239 - val_acc: 0.0484\n",
            "Epoch 402/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.3322 - acc: 0.2379 - val_loss: 3.7051 - val_acc: 0.0161\n",
            "Epoch 403/1000\n",
            "248/248 [==============================] - 0s 84us/step - loss: 1.2195 - acc: 0.2944 - val_loss: 3.7019 - val_acc: 0.0484\n",
            "Epoch 404/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2692 - acc: 0.2581 - val_loss: 3.6749 - val_acc: 0.0484\n",
            "Epoch 405/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.2489 - acc: 0.2460 - val_loss: 3.7025 - val_acc: 0.0645\n",
            "Epoch 406/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.2460 - acc: 0.2661 - val_loss: 3.6601 - val_acc: 0.0484\n",
            "Epoch 407/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2017 - acc: 0.2823 - val_loss: 3.6679 - val_acc: 0.0161\n",
            "Epoch 408/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2783 - acc: 0.2661 - val_loss: 3.6932 - val_acc: 0.0484\n",
            "Epoch 409/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.1793 - acc: 0.2903 - val_loss: 3.6590 - val_acc: 0.0484\n",
            "Epoch 410/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2056 - acc: 0.2298 - val_loss: 3.8376 - val_acc: 0.0484\n",
            "Epoch 411/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.2769 - acc: 0.2782 - val_loss: 3.6851 - val_acc: 0.0484\n",
            "Epoch 412/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 1.1944 - acc: 0.2903 - val_loss: 3.7435 - val_acc: 0.0484\n",
            "Epoch 413/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.1808 - acc: 0.2823 - val_loss: 3.6873 - val_acc: 0.0323\n",
            "Epoch 414/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1580 - acc: 0.2742 - val_loss: 3.6912 - val_acc: 0.0484\n",
            "Epoch 415/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.2050 - acc: 0.2419 - val_loss: 3.7108 - val_acc: 0.0484\n",
            "Epoch 416/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2211 - acc: 0.2823 - val_loss: 3.6915 - val_acc: 0.0484\n",
            "Epoch 417/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1930 - acc: 0.2702 - val_loss: 3.7384 - val_acc: 0.0323\n",
            "Epoch 418/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 1.2272 - acc: 0.2823 - val_loss: 3.6693 - val_acc: 0.0484\n",
            "Epoch 419/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.2677 - acc: 0.2540 - val_loss: 3.7048 - val_acc: 0.0484\n",
            "Epoch 420/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2238 - acc: 0.2661 - val_loss: 3.6653 - val_acc: 0.0484\n",
            "Epoch 421/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.1984 - acc: 0.2702 - val_loss: 3.7115 - val_acc: 0.0323\n",
            "Epoch 422/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1849 - acc: 0.2702 - val_loss: 3.7196 - val_acc: 0.0484\n",
            "Epoch 423/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.1644 - acc: 0.3024 - val_loss: 3.7065 - val_acc: 0.0161\n",
            "Epoch 424/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2211 - acc: 0.2702 - val_loss: 3.6970 - val_acc: 0.0161\n",
            "Epoch 425/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.1512 - acc: 0.3024 - val_loss: 3.7180 - val_acc: 0.0323\n",
            "Epoch 426/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2370 - acc: 0.2460 - val_loss: 3.7097 - val_acc: 0.0484\n",
            "Epoch 427/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1992 - acc: 0.2823 - val_loss: 3.7104 - val_acc: 0.0484\n",
            "Epoch 428/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1774 - acc: 0.2702 - val_loss: 3.7175 - val_acc: 0.0323\n",
            "Epoch 429/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.2426 - acc: 0.2702 - val_loss: 3.7440 - val_acc: 0.0484\n",
            "Epoch 430/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.3070 - acc: 0.2218 - val_loss: 3.6778 - val_acc: 0.0484\n",
            "Epoch 431/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.2131 - acc: 0.3145 - val_loss: 3.7470 - val_acc: 0.0484\n",
            "Epoch 432/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1447 - acc: 0.3185 - val_loss: 3.6740 - val_acc: 0.0645\n",
            "Epoch 433/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.2002 - acc: 0.3065 - val_loss: 3.6862 - val_acc: 0.0484\n",
            "Epoch 434/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1747 - acc: 0.2984 - val_loss: 3.6734 - val_acc: 0.0484\n",
            "Epoch 435/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1872 - acc: 0.3024 - val_loss: 3.6575 - val_acc: 0.0484\n",
            "Epoch 436/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.2019 - acc: 0.3145 - val_loss: 3.6306 - val_acc: 0.0484\n",
            "Epoch 437/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1988 - acc: 0.2984 - val_loss: 3.6679 - val_acc: 0.0484\n",
            "Epoch 438/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.1824 - acc: 0.2863 - val_loss: 3.6964 - val_acc: 0.0484\n",
            "Epoch 439/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1420 - acc: 0.2984 - val_loss: 3.6432 - val_acc: 0.0484\n",
            "Epoch 440/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.2392 - acc: 0.2782 - val_loss: 3.6561 - val_acc: 0.0484\n",
            "Epoch 441/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.2142 - acc: 0.2540 - val_loss: 3.7007 - val_acc: 0.0323\n",
            "Epoch 442/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1534 - acc: 0.3024 - val_loss: 3.7001 - val_acc: 0.0645\n",
            "Epoch 443/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1742 - acc: 0.3065 - val_loss: 3.6940 - val_acc: 0.0323\n",
            "Epoch 444/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2115 - acc: 0.2944 - val_loss: 3.7844 - val_acc: 0.0484\n",
            "Epoch 445/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1626 - acc: 0.3024 - val_loss: 3.7351 - val_acc: 0.0161\n",
            "Epoch 446/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.2340 - acc: 0.2500 - val_loss: 3.6694 - val_acc: 0.0645\n",
            "Epoch 447/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1457 - acc: 0.2863 - val_loss: 3.7061 - val_acc: 0.0161\n",
            "Epoch 448/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1447 - acc: 0.2823 - val_loss: 3.7241 - val_acc: 0.0645\n",
            "Epoch 449/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.2179 - acc: 0.2581 - val_loss: 3.6995 - val_acc: 0.0484\n",
            "Epoch 450/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2042 - acc: 0.2742 - val_loss: 3.6478 - val_acc: 0.0484\n",
            "Epoch 451/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.2135 - acc: 0.2702 - val_loss: 3.7317 - val_acc: 0.0323\n",
            "Epoch 452/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1862 - acc: 0.2702 - val_loss: 3.7319 - val_acc: 0.0323\n",
            "Epoch 453/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.2346 - acc: 0.2540 - val_loss: 3.7281 - val_acc: 0.0484\n",
            "Epoch 454/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2396 - acc: 0.2621 - val_loss: 3.7677 - val_acc: 0.0323\n",
            "Epoch 455/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2488 - acc: 0.2460 - val_loss: 3.7034 - val_acc: 0.0484\n",
            "Epoch 456/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.1818 - acc: 0.3024 - val_loss: 3.9051 - val_acc: 0.0161\n",
            "Epoch 457/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.2644 - acc: 0.2581 - val_loss: 3.7411 - val_acc: 0.0323\n",
            "Epoch 458/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1243 - acc: 0.3266 - val_loss: 3.7804 - val_acc: 0.0484\n",
            "Epoch 459/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1802 - acc: 0.2661 - val_loss: 3.6810 - val_acc: 0.0484\n",
            "Epoch 460/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.1658 - acc: 0.2823 - val_loss: 3.7610 - val_acc: 0.0484\n",
            "Epoch 461/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1550 - acc: 0.3185 - val_loss: 3.6815 - val_acc: 0.0161\n",
            "Epoch 462/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1898 - acc: 0.2742 - val_loss: 3.6769 - val_acc: 0.0323\n",
            "Epoch 463/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1806 - acc: 0.2863 - val_loss: 3.6258 - val_acc: 0.0484\n",
            "Epoch 464/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2894 - acc: 0.2177 - val_loss: 3.6535 - val_acc: 0.0484\n",
            "Epoch 465/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 1.1767 - acc: 0.2702 - val_loss: 3.6567 - val_acc: 0.0645\n",
            "Epoch 466/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1416 - acc: 0.3145 - val_loss: 3.6934 - val_acc: 0.0484\n",
            "Epoch 467/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.2109 - acc: 0.2903 - val_loss: 3.6514 - val_acc: 0.0323\n",
            "Epoch 468/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.1644 - acc: 0.2903 - val_loss: 3.7723 - val_acc: 0.0161\n",
            "Epoch 469/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.2122 - acc: 0.2540 - val_loss: 3.6969 - val_acc: 0.0323\n",
            "Epoch 470/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1679 - acc: 0.3065 - val_loss: 3.8074 - val_acc: 0.0323\n",
            "Epoch 471/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1385 - acc: 0.3024 - val_loss: 3.6767 - val_acc: 0.0161\n",
            "Epoch 472/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1736 - acc: 0.2621 - val_loss: 3.7012 - val_acc: 0.0323\n",
            "Epoch 473/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1561 - acc: 0.2984 - val_loss: 3.7590 - val_acc: 0.0484\n",
            "Epoch 474/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2697 - acc: 0.2460 - val_loss: 3.7605 - val_acc: 0.0323\n",
            "Epoch 475/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2117 - acc: 0.2379 - val_loss: 3.6865 - val_acc: 0.0484\n",
            "Epoch 476/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.2065 - acc: 0.2661 - val_loss: 3.6582 - val_acc: 0.0323\n",
            "Epoch 477/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1482 - acc: 0.2823 - val_loss: 3.7629 - val_acc: 0.0323\n",
            "Epoch 478/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1294 - acc: 0.3024 - val_loss: 3.6632 - val_acc: 0.0161\n",
            "Epoch 479/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1779 - acc: 0.2782 - val_loss: 3.7276 - val_acc: 0.0645\n",
            "Epoch 480/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1357 - acc: 0.3185 - val_loss: 3.7918 - val_acc: 0.0645\n",
            "Epoch 481/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1973 - acc: 0.2863 - val_loss: 3.7396 - val_acc: 0.0484\n",
            "Epoch 482/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1130 - acc: 0.2984 - val_loss: 3.7034 - val_acc: 0.0484\n",
            "Epoch 483/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1428 - acc: 0.3266 - val_loss: 3.6471 - val_acc: 0.0484\n",
            "Epoch 484/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1522 - acc: 0.3065 - val_loss: 3.6890 - val_acc: 0.0161\n",
            "Epoch 485/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.2034 - acc: 0.2702 - val_loss: 3.7025 - val_acc: 0.0484\n",
            "Epoch 486/1000\n",
            "248/248 [==============================] - 0s 97us/step - loss: 1.1453 - acc: 0.2823 - val_loss: 3.6556 - val_acc: 0.0161\n",
            "Epoch 487/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1111 - acc: 0.3065 - val_loss: 3.6868 - val_acc: 0.0484\n",
            "Epoch 488/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.2126 - acc: 0.2581 - val_loss: 3.6768 - val_acc: 0.0161\n",
            "Epoch 489/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1170 - acc: 0.3185 - val_loss: 3.6948 - val_acc: 0.0161\n",
            "Epoch 490/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.2013 - acc: 0.2661 - val_loss: 3.6957 - val_acc: 0.0484\n",
            "Epoch 491/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1373 - acc: 0.2984 - val_loss: 3.7116 - val_acc: 0.0161\n",
            "Epoch 492/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.2109 - acc: 0.2702 - val_loss: 3.7316 - val_acc: 0.0484\n",
            "Epoch 493/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1563 - acc: 0.2944 - val_loss: 3.6603 - val_acc: 0.0645\n",
            "Epoch 494/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.1697 - acc: 0.2984 - val_loss: 3.6517 - val_acc: 0.0484\n",
            "Epoch 495/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1773 - acc: 0.2742 - val_loss: 3.6759 - val_acc: 0.0484\n",
            "Epoch 496/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1978 - acc: 0.2702 - val_loss: 3.7553 - val_acc: 0.0161\n",
            "Epoch 497/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.2451 - acc: 0.2258 - val_loss: 3.7170 - val_acc: 0.0323\n",
            "Epoch 498/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1252 - acc: 0.2984 - val_loss: 3.6735 - val_acc: 0.0484\n",
            "Epoch 499/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1785 - acc: 0.2742 - val_loss: 3.7139 - val_acc: 0.0484\n",
            "Epoch 500/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1554 - acc: 0.3065 - val_loss: 3.7060 - val_acc: 0.0484\n",
            "Epoch 501/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.2116 - acc: 0.2419 - val_loss: 3.7051 - val_acc: 0.0484\n",
            "Epoch 502/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.1435 - acc: 0.2944 - val_loss: 3.6596 - val_acc: 0.0161\n",
            "Epoch 503/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1245 - acc: 0.3387 - val_loss: 3.6575 - val_acc: 0.0645\n",
            "Epoch 504/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1080 - acc: 0.3306 - val_loss: 3.7299 - val_acc: 0.0323\n",
            "Epoch 505/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1188 - acc: 0.2984 - val_loss: 3.6863 - val_acc: 0.0161\n",
            "Epoch 506/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1592 - acc: 0.2661 - val_loss: 3.6752 - val_acc: 0.0484\n",
            "Epoch 507/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1671 - acc: 0.2782 - val_loss: 3.6749 - val_acc: 0.0484\n",
            "Epoch 508/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.1297 - acc: 0.3226 - val_loss: 3.7308 - val_acc: 0.0323\n",
            "Epoch 509/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1574 - acc: 0.3065 - val_loss: 3.5927 - val_acc: 0.0484\n",
            "Epoch 510/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1590 - acc: 0.3145 - val_loss: 3.7286 - val_acc: 0.0484\n",
            "Epoch 511/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1594 - acc: 0.3065 - val_loss: 3.7089 - val_acc: 0.0323\n",
            "Epoch 512/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.2129 - acc: 0.2500 - val_loss: 3.6735 - val_acc: 0.0323\n",
            "Epoch 513/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.1451 - acc: 0.2621 - val_loss: 3.8117 - val_acc: 0.0161\n",
            "Epoch 514/1000\n",
            "248/248 [==============================] - 0s 83us/step - loss: 1.2016 - acc: 0.2661 - val_loss: 3.6324 - val_acc: 0.0323\n",
            "Epoch 515/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.2137 - acc: 0.2460 - val_loss: 3.6803 - val_acc: 0.0161\n",
            "Epoch 516/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0974 - acc: 0.3548 - val_loss: 3.7650 - val_acc: 0.0161\n",
            "Epoch 517/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.1577 - acc: 0.3065 - val_loss: 3.7116 - val_acc: 0.0484\n",
            "Epoch 518/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1974 - acc: 0.2782 - val_loss: 3.7331 - val_acc: 0.0161\n",
            "Epoch 519/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.1048 - acc: 0.3347 - val_loss: 3.7050 - val_acc: 0.0645\n",
            "Epoch 520/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.1351 - acc: 0.3065 - val_loss: 3.7144 - val_acc: 0.0323\n",
            "Epoch 521/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1510 - acc: 0.2984 - val_loss: 3.7466 - val_acc: 0.0484\n",
            "Epoch 522/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1367 - acc: 0.3145 - val_loss: 3.6835 - val_acc: 0.0484\n",
            "Epoch 523/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1442 - acc: 0.2984 - val_loss: 3.6698 - val_acc: 0.0645\n",
            "Epoch 524/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1782 - acc: 0.3065 - val_loss: 3.7234 - val_acc: 0.0323\n",
            "Epoch 525/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1802 - acc: 0.2742 - val_loss: 3.7520 - val_acc: 0.0161\n",
            "Epoch 526/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.1083 - acc: 0.3306 - val_loss: 3.7035 - val_acc: 0.0161\n",
            "Epoch 527/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1689 - acc: 0.2984 - val_loss: 3.6823 - val_acc: 0.0484\n",
            "Epoch 528/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0986 - acc: 0.3266 - val_loss: 3.6400 - val_acc: 0.0645\n",
            "Epoch 529/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.1396 - acc: 0.2742 - val_loss: 3.6672 - val_acc: 0.0645\n",
            "Epoch 530/1000\n",
            "248/248 [==============================] - 0s 87us/step - loss: 1.2053 - acc: 0.2500 - val_loss: 3.6914 - val_acc: 0.0484\n",
            "Epoch 531/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1896 - acc: 0.2661 - val_loss: 3.7230 - val_acc: 0.0484\n",
            "Epoch 532/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1234 - acc: 0.2823 - val_loss: 3.6529 - val_acc: 0.0645\n",
            "Epoch 533/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.1352 - acc: 0.3145 - val_loss: 3.6169 - val_acc: 0.0323\n",
            "Epoch 534/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1169 - acc: 0.3226 - val_loss: 3.6502 - val_acc: 0.0323\n",
            "Epoch 535/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0966 - acc: 0.3508 - val_loss: 3.6558 - val_acc: 0.0645\n",
            "Epoch 536/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.1083 - acc: 0.3226 - val_loss: 3.6770 - val_acc: 0.0323\n",
            "Epoch 537/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.1658 - acc: 0.3105 - val_loss: 3.7658 - val_acc: 0.0484\n",
            "Epoch 538/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.1257 - acc: 0.3185 - val_loss: 3.7048 - val_acc: 0.0645\n",
            "Epoch 539/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1512 - acc: 0.2823 - val_loss: 3.7131 - val_acc: 0.0484\n",
            "Epoch 540/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1776 - acc: 0.2823 - val_loss: 3.6540 - val_acc: 0.0323\n",
            "Epoch 541/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1375 - acc: 0.2863 - val_loss: 3.6151 - val_acc: 0.0645\n",
            "Epoch 542/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1286 - acc: 0.2702 - val_loss: 3.6489 - val_acc: 0.0323\n",
            "Epoch 543/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1247 - acc: 0.2782 - val_loss: 3.6939 - val_acc: 0.0323\n",
            "Epoch 544/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1660 - acc: 0.2903 - val_loss: 3.6296 - val_acc: 0.0484\n",
            "Epoch 545/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1818 - acc: 0.2540 - val_loss: 3.6533 - val_acc: 0.0323\n",
            "Epoch 546/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.2025 - acc: 0.2702 - val_loss: 3.7818 - val_acc: 0.0323\n",
            "Epoch 547/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1447 - acc: 0.3226 - val_loss: 3.6374 - val_acc: 0.0645\n",
            "Epoch 548/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1455 - acc: 0.3185 - val_loss: 3.6979 - val_acc: 0.0645\n",
            "Epoch 549/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1338 - acc: 0.2661 - val_loss: 3.7395 - val_acc: 0.0806\n",
            "Epoch 550/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.2095 - acc: 0.2540 - val_loss: 3.7886 - val_acc: 0.0323\n",
            "Epoch 551/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1374 - acc: 0.2984 - val_loss: 3.6484 - val_acc: 0.0645\n",
            "Epoch 552/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.1044 - acc: 0.3065 - val_loss: 3.7231 - val_acc: 0.0645\n",
            "Epoch 553/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.1666 - acc: 0.2782 - val_loss: 3.7108 - val_acc: 0.0484\n",
            "Epoch 554/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0879 - acc: 0.3185 - val_loss: 3.6742 - val_acc: 0.0484\n",
            "Epoch 555/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1477 - acc: 0.3226 - val_loss: 3.6572 - val_acc: 0.0323\n",
            "Epoch 556/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.0863 - acc: 0.3347 - val_loss: 3.6831 - val_acc: 0.0645\n",
            "Epoch 557/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.1247 - acc: 0.3185 - val_loss: 3.8752 - val_acc: 0.0323\n",
            "Epoch 558/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1884 - acc: 0.2661 - val_loss: 3.6995 - val_acc: 0.0323\n",
            "Epoch 559/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1135 - acc: 0.2944 - val_loss: 3.5990 - val_acc: 0.0645\n",
            "Epoch 560/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.1502 - acc: 0.3024 - val_loss: 3.7361 - val_acc: 0.0161\n",
            "Epoch 561/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1314 - acc: 0.3065 - val_loss: 3.6482 - val_acc: 0.0645\n",
            "Epoch 562/1000\n",
            "248/248 [==============================] - 0s 80us/step - loss: 1.0988 - acc: 0.3387 - val_loss: 3.6692 - val_acc: 0.0484\n",
            "Epoch 563/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0932 - acc: 0.3185 - val_loss: 3.6288 - val_acc: 0.0806\n",
            "Epoch 564/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1821 - acc: 0.2379 - val_loss: 3.6246 - val_acc: 0.0323\n",
            "Epoch 565/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1409 - acc: 0.3065 - val_loss: 3.7478 - val_acc: 0.0323\n",
            "Epoch 566/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.0931 - acc: 0.3226 - val_loss: 3.6376 - val_acc: 0.0806\n",
            "Epoch 567/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.2034 - acc: 0.2137 - val_loss: 3.5835 - val_acc: 0.0484\n",
            "Epoch 568/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1985 - acc: 0.2460 - val_loss: 3.6246 - val_acc: 0.0645\n",
            "Epoch 569/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.0949 - acc: 0.3427 - val_loss: 3.5946 - val_acc: 0.0161\n",
            "Epoch 570/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.1186 - acc: 0.3065 - val_loss: 3.5873 - val_acc: 0.0645\n",
            "Epoch 571/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0853 - acc: 0.3105 - val_loss: 3.5929 - val_acc: 0.0645\n",
            "Epoch 572/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1196 - acc: 0.2903 - val_loss: 3.5950 - val_acc: 0.0323\n",
            "Epoch 573/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.1012 - acc: 0.3266 - val_loss: 3.5857 - val_acc: 0.0645\n",
            "Epoch 574/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1375 - acc: 0.3024 - val_loss: 3.6349 - val_acc: 0.0484\n",
            "Epoch 575/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1891 - acc: 0.2460 - val_loss: 3.6599 - val_acc: 0.0323\n",
            "Epoch 576/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1483 - acc: 0.2903 - val_loss: 3.5916 - val_acc: 0.0484\n",
            "Epoch 577/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0784 - acc: 0.3347 - val_loss: 3.6186 - val_acc: 0.0645\n",
            "Epoch 578/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0965 - acc: 0.2984 - val_loss: 3.6176 - val_acc: 0.0484\n",
            "Epoch 579/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1793 - acc: 0.2621 - val_loss: 3.6449 - val_acc: 0.0645\n",
            "Epoch 580/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.1136 - acc: 0.2944 - val_loss: 3.5933 - val_acc: 0.0645\n",
            "Epoch 581/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 1.0596 - acc: 0.3387 - val_loss: 3.6046 - val_acc: 0.0161\n",
            "Epoch 582/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1728 - acc: 0.2621 - val_loss: 3.6344 - val_acc: 0.0323\n",
            "Epoch 583/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1421 - acc: 0.2742 - val_loss: 3.5702 - val_acc: 0.0323\n",
            "Epoch 584/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.1742 - acc: 0.2298 - val_loss: 3.5620 - val_acc: 0.0161\n",
            "Epoch 585/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1518 - acc: 0.2823 - val_loss: 3.6105 - val_acc: 0.0323\n",
            "Epoch 586/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.1380 - acc: 0.2903 - val_loss: 3.5736 - val_acc: 0.0484\n",
            "Epoch 587/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1099 - acc: 0.3105 - val_loss: 3.5580 - val_acc: 0.0484\n",
            "Epoch 588/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1692 - acc: 0.2944 - val_loss: 3.5929 - val_acc: 0.0484\n",
            "Epoch 589/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0830 - acc: 0.2984 - val_loss: 3.5925 - val_acc: 0.0161\n",
            "Epoch 590/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1661 - acc: 0.2984 - val_loss: 3.6837 - val_acc: 0.0161\n",
            "Epoch 591/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.2256 - acc: 0.2782 - val_loss: 3.6858 - val_acc: 0.0323\n",
            "Epoch 592/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.1837 - acc: 0.2581 - val_loss: 3.6966 - val_acc: 0.0161\n",
            "Epoch 593/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.1251 - acc: 0.3145 - val_loss: 3.6124 - val_acc: 0.0484\n",
            "Epoch 594/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0745 - acc: 0.3306 - val_loss: 3.7083 - val_acc: 0.0000e+00\n",
            "Epoch 595/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.1923 - acc: 0.2863 - val_loss: 3.6468 - val_acc: 0.0161\n",
            "Epoch 596/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.1536 - acc: 0.3024 - val_loss: 3.6513 - val_acc: 0.0161\n",
            "Epoch 597/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1292 - acc: 0.2984 - val_loss: 3.6077 - val_acc: 0.0484\n",
            "Epoch 598/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.1106 - acc: 0.3145 - val_loss: 3.6233 - val_acc: 0.0484\n",
            "Epoch 599/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1169 - acc: 0.3105 - val_loss: 3.6094 - val_acc: 0.0323\n",
            "Epoch 600/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0989 - acc: 0.3024 - val_loss: 3.5941 - val_acc: 0.0161\n",
            "Epoch 601/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0900 - acc: 0.3185 - val_loss: 3.6128 - val_acc: 0.0161\n",
            "Epoch 602/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1315 - acc: 0.2742 - val_loss: 3.6672 - val_acc: 0.0323\n",
            "Epoch 603/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0975 - acc: 0.3185 - val_loss: 3.6087 - val_acc: 0.0323\n",
            "Epoch 604/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1396 - acc: 0.2863 - val_loss: 3.6207 - val_acc: 0.0161\n",
            "Epoch 605/1000\n",
            "248/248 [==============================] - 0s 84us/step - loss: 1.0717 - acc: 0.3347 - val_loss: 3.5903 - val_acc: 0.0323\n",
            "Epoch 606/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0830 - acc: 0.3266 - val_loss: 3.7028 - val_acc: 0.0000e+00\n",
            "Epoch 607/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1364 - acc: 0.2903 - val_loss: 3.6177 - val_acc: 0.0161\n",
            "Epoch 608/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.1360 - acc: 0.3024 - val_loss: 3.5539 - val_acc: 0.0484\n",
            "Epoch 609/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0915 - acc: 0.3145 - val_loss: 3.6443 - val_acc: 0.0000e+00\n",
            "Epoch 610/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0622 - acc: 0.3347 - val_loss: 3.7691 - val_acc: 0.0000e+00\n",
            "Epoch 611/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1928 - acc: 0.2782 - val_loss: 3.6264 - val_acc: 0.0484\n",
            "Epoch 612/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.0689 - acc: 0.3105 - val_loss: 3.6965 - val_acc: 0.0000e+00\n",
            "Epoch 613/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.1046 - acc: 0.3185 - val_loss: 3.6219 - val_acc: 0.0161\n",
            "Epoch 614/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1377 - acc: 0.3065 - val_loss: 3.5424 - val_acc: 0.0484\n",
            "Epoch 615/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.1581 - acc: 0.2742 - val_loss: 3.6229 - val_acc: 0.0323\n",
            "Epoch 616/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0842 - acc: 0.3185 - val_loss: 3.5587 - val_acc: 0.0645\n",
            "Epoch 617/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0912 - acc: 0.3145 - val_loss: 3.6352 - val_acc: 0.0323\n",
            "Epoch 618/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1361 - acc: 0.3145 - val_loss: 3.6563 - val_acc: 0.0161\n",
            "Epoch 619/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0769 - acc: 0.3508 - val_loss: 3.6495 - val_acc: 0.0161\n",
            "Epoch 620/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1532 - acc: 0.2339 - val_loss: 3.6029 - val_acc: 0.0000e+00\n",
            "Epoch 621/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1173 - acc: 0.2742 - val_loss: 3.5971 - val_acc: 0.0161\n",
            "Epoch 622/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0763 - acc: 0.3347 - val_loss: 3.5933 - val_acc: 0.0645\n",
            "Epoch 623/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0715 - acc: 0.3266 - val_loss: 3.6198 - val_acc: 0.0000e+00\n",
            "Epoch 624/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1204 - acc: 0.3185 - val_loss: 3.6158 - val_acc: 0.0000e+00\n",
            "Epoch 625/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1699 - acc: 0.3145 - val_loss: 3.5587 - val_acc: 0.0323\n",
            "Epoch 626/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1301 - acc: 0.2702 - val_loss: 3.5121 - val_acc: 0.0645\n",
            "Epoch 627/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.1041 - acc: 0.3145 - val_loss: 3.5432 - val_acc: 0.0484\n",
            "Epoch 628/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1273 - acc: 0.2782 - val_loss: 3.6560 - val_acc: 0.0161\n",
            "Epoch 629/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1473 - acc: 0.2863 - val_loss: 3.6149 - val_acc: 0.0000e+00\n",
            "Epoch 630/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1189 - acc: 0.2863 - val_loss: 3.6476 - val_acc: 0.0000e+00\n",
            "Epoch 631/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0675 - acc: 0.3306 - val_loss: 3.6394 - val_acc: 0.0161\n",
            "Epoch 632/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1908 - acc: 0.2661 - val_loss: 3.5264 - val_acc: 0.0645\n",
            "Epoch 633/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0942 - acc: 0.3065 - val_loss: 3.6179 - val_acc: 0.0161\n",
            "Epoch 634/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0825 - acc: 0.3145 - val_loss: 3.5334 - val_acc: 0.0645\n",
            "Epoch 635/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.0562 - acc: 0.3387 - val_loss: 3.5681 - val_acc: 0.0323\n",
            "Epoch 636/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 1.1261 - acc: 0.2823 - val_loss: 3.5639 - val_acc: 0.0645\n",
            "Epoch 637/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0628 - acc: 0.3347 - val_loss: 3.5138 - val_acc: 0.0323\n",
            "Epoch 638/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0971 - acc: 0.3024 - val_loss: 3.5341 - val_acc: 0.0645\n",
            "Epoch 639/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 1.0637 - acc: 0.3306 - val_loss: 3.5490 - val_acc: 0.0000e+00\n",
            "Epoch 640/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1241 - acc: 0.2903 - val_loss: 3.6468 - val_acc: 0.0161\n",
            "Epoch 641/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.1165 - acc: 0.3065 - val_loss: 3.5829 - val_acc: 0.0484\n",
            "Epoch 642/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0911 - acc: 0.3387 - val_loss: 3.6022 - val_acc: 0.0645\n",
            "Epoch 643/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1226 - acc: 0.3024 - val_loss: 3.5856 - val_acc: 0.0323\n",
            "Epoch 644/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.0558 - acc: 0.3468 - val_loss: 3.5590 - val_acc: 0.0323\n",
            "Epoch 645/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.0732 - acc: 0.2984 - val_loss: 3.5667 - val_acc: 0.0645\n",
            "Epoch 646/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0970 - acc: 0.2742 - val_loss: 3.6233 - val_acc: 0.0161\n",
            "Epoch 647/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1009 - acc: 0.3105 - val_loss: 3.5859 - val_acc: 0.0161\n",
            "Epoch 648/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0941 - acc: 0.3185 - val_loss: 3.5718 - val_acc: 0.0323\n",
            "Epoch 649/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1123 - acc: 0.3226 - val_loss: 3.5350 - val_acc: 0.0645\n",
            "Epoch 650/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.0516 - acc: 0.3266 - val_loss: 3.6067 - val_acc: 0.0161\n",
            "Epoch 651/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0575 - acc: 0.3226 - val_loss: 3.5619 - val_acc: 0.0484\n",
            "Epoch 652/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0669 - acc: 0.3185 - val_loss: 3.5093 - val_acc: 0.0645\n",
            "Epoch 653/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1414 - acc: 0.2782 - val_loss: 3.6501 - val_acc: 0.0161\n",
            "Epoch 654/1000\n",
            "248/248 [==============================] - 0s 78us/step - loss: 1.0781 - acc: 0.3306 - val_loss: 3.6055 - val_acc: 0.0161\n",
            "Epoch 655/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0962 - acc: 0.3024 - val_loss: 3.5797 - val_acc: 0.0161\n",
            "Epoch 656/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0939 - acc: 0.3226 - val_loss: 3.5596 - val_acc: 0.0484\n",
            "Epoch 657/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.1119 - acc: 0.3024 - val_loss: 3.6645 - val_acc: 0.0161\n",
            "Epoch 658/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0816 - acc: 0.3065 - val_loss: 3.7087 - val_acc: 0.0000e+00\n",
            "Epoch 659/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0862 - acc: 0.3105 - val_loss: 3.6781 - val_acc: 0.0161\n",
            "Epoch 660/1000\n",
            "248/248 [==============================] - 0s 74us/step - loss: 1.1442 - acc: 0.2661 - val_loss: 3.5630 - val_acc: 0.0161\n",
            "Epoch 661/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0690 - acc: 0.3065 - val_loss: 3.5817 - val_acc: 0.0000e+00\n",
            "Epoch 662/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1069 - acc: 0.2903 - val_loss: 3.5791 - val_acc: 0.0645\n",
            "Epoch 663/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.1095 - acc: 0.3185 - val_loss: 3.5980 - val_acc: 0.0000e+00\n",
            "Epoch 664/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1536 - acc: 0.3065 - val_loss: 3.5481 - val_acc: 0.0161\n",
            "Epoch 665/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1448 - acc: 0.2823 - val_loss: 3.5916 - val_acc: 0.0645\n",
            "Epoch 666/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1182 - acc: 0.2742 - val_loss: 3.5431 - val_acc: 0.0645\n",
            "Epoch 667/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.1230 - acc: 0.2823 - val_loss: 3.5846 - val_acc: 0.0000e+00\n",
            "Epoch 668/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.0711 - acc: 0.3185 - val_loss: 3.6047 - val_acc: 0.0000e+00\n",
            "Epoch 669/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1495 - acc: 0.2903 - val_loss: 3.6471 - val_acc: 0.0161\n",
            "Epoch 670/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1386 - acc: 0.3105 - val_loss: 3.5465 - val_acc: 0.0484\n",
            "Epoch 671/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0422 - acc: 0.3548 - val_loss: 3.5804 - val_acc: 0.0323\n",
            "Epoch 672/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0234 - acc: 0.3306 - val_loss: 3.5735 - val_acc: 0.0484\n",
            "Epoch 673/1000\n",
            "248/248 [==============================] - 0s 69us/step - loss: 1.1007 - acc: 0.2984 - val_loss: 3.6075 - val_acc: 0.0323\n",
            "Epoch 674/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 1.1033 - acc: 0.3306 - val_loss: 3.5754 - val_acc: 0.0161\n",
            "Epoch 675/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0895 - acc: 0.3065 - val_loss: 3.5602 - val_acc: 0.0645\n",
            "Epoch 676/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0972 - acc: 0.3065 - val_loss: 3.5804 - val_acc: 0.0323\n",
            "Epoch 677/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1069 - acc: 0.3185 - val_loss: 3.5325 - val_acc: 0.0645\n",
            "Epoch 678/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1165 - acc: 0.2984 - val_loss: 3.5461 - val_acc: 0.0645\n",
            "Epoch 679/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0743 - acc: 0.2944 - val_loss: 3.6164 - val_acc: 0.0000e+00\n",
            "Epoch 680/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.1103 - acc: 0.3145 - val_loss: 3.5358 - val_acc: 0.0484\n",
            "Epoch 681/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.1486 - acc: 0.2500 - val_loss: 3.6052 - val_acc: 0.0161\n",
            "Epoch 682/1000\n",
            "248/248 [==============================] - 0s 80us/step - loss: 1.1395 - acc: 0.2984 - val_loss: 3.5665 - val_acc: 0.0484\n",
            "Epoch 683/1000\n",
            "248/248 [==============================] - 0s 85us/step - loss: 1.0393 - acc: 0.3387 - val_loss: 3.5897 - val_acc: 0.0645\n",
            "Epoch 684/1000\n",
            "248/248 [==============================] - 0s 89us/step - loss: 1.0755 - acc: 0.3387 - val_loss: 3.5368 - val_acc: 0.0161\n",
            "Epoch 685/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0667 - acc: 0.3105 - val_loss: 3.5259 - val_acc: 0.0645\n",
            "Epoch 686/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0350 - acc: 0.3548 - val_loss: 3.4895 - val_acc: 0.0645\n",
            "Epoch 687/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 1.1052 - acc: 0.2742 - val_loss: 3.5408 - val_acc: 0.0161\n",
            "Epoch 688/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1212 - acc: 0.3145 - val_loss: 3.5575 - val_acc: 0.0645\n",
            "Epoch 689/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.0966 - acc: 0.3347 - val_loss: 3.5717 - val_acc: 0.0484\n",
            "Epoch 690/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1083 - acc: 0.2621 - val_loss: 3.5354 - val_acc: 0.0484\n",
            "Epoch 691/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0802 - acc: 0.3185 - val_loss: 3.6126 - val_acc: 0.0161\n",
            "Epoch 692/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0208 - acc: 0.3266 - val_loss: 3.6056 - val_acc: 0.0161\n",
            "Epoch 693/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.0528 - acc: 0.3145 - val_loss: 3.5428 - val_acc: 0.0323\n",
            "Epoch 694/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0609 - acc: 0.3105 - val_loss: 3.5738 - val_acc: 0.0000e+00\n",
            "Epoch 695/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.0890 - acc: 0.3105 - val_loss: 3.6025 - val_acc: 0.0484\n",
            "Epoch 696/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0592 - acc: 0.3508 - val_loss: 3.5838 - val_acc: 0.0484\n",
            "Epoch 697/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0792 - acc: 0.3185 - val_loss: 3.5088 - val_acc: 0.0645\n",
            "Epoch 698/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.1421 - acc: 0.2863 - val_loss: 3.5877 - val_acc: 0.0161\n",
            "Epoch 699/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.0619 - acc: 0.3468 - val_loss: 3.5625 - val_acc: 0.0645\n",
            "Epoch 700/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 1.0475 - acc: 0.3468 - val_loss: 3.6613 - val_acc: 0.0161\n",
            "Epoch 701/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0326 - acc: 0.3266 - val_loss: 3.6354 - val_acc: 0.0161\n",
            "Epoch 702/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.1094 - acc: 0.2782 - val_loss: 3.5171 - val_acc: 0.0161\n",
            "Epoch 703/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.1398 - acc: 0.2621 - val_loss: 3.6001 - val_acc: 0.0484\n",
            "Epoch 704/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.1520 - acc: 0.2661 - val_loss: 3.6655 - val_acc: 0.0323\n",
            "Epoch 705/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.1051 - acc: 0.2903 - val_loss: 3.5952 - val_acc: 0.0161\n",
            "Epoch 706/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0718 - acc: 0.2984 - val_loss: 3.5442 - val_acc: 0.0161\n",
            "Epoch 707/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0856 - acc: 0.2984 - val_loss: 3.5786 - val_acc: 0.0161\n",
            "Epoch 708/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0720 - acc: 0.3347 - val_loss: 3.5448 - val_acc: 0.0323\n",
            "Epoch 709/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0550 - acc: 0.3226 - val_loss: 3.6104 - val_acc: 0.0000e+00\n",
            "Epoch 710/1000\n",
            "248/248 [==============================] - 0s 91us/step - loss: 1.0946 - acc: 0.3387 - val_loss: 3.5580 - val_acc: 0.0161\n",
            "Epoch 711/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0621 - acc: 0.3065 - val_loss: 3.6358 - val_acc: 0.0000e+00\n",
            "Epoch 712/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0469 - acc: 0.3145 - val_loss: 3.5821 - val_acc: 0.0323\n",
            "Epoch 713/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0407 - acc: 0.3145 - val_loss: 3.5420 - val_acc: 0.0161\n",
            "Epoch 714/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1236 - acc: 0.2903 - val_loss: 3.5429 - val_acc: 0.0161\n",
            "Epoch 715/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0650 - acc: 0.3347 - val_loss: 3.5948 - val_acc: 0.0161\n",
            "Epoch 716/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0448 - acc: 0.3347 - val_loss: 3.5435 - val_acc: 0.0645\n",
            "Epoch 717/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0622 - acc: 0.3185 - val_loss: 3.5880 - val_acc: 0.0484\n",
            "Epoch 718/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.0852 - acc: 0.2944 - val_loss: 3.5977 - val_acc: 0.0161\n",
            "Epoch 719/1000\n",
            "248/248 [==============================] - 0s 82us/step - loss: 1.0585 - acc: 0.3266 - val_loss: 3.5914 - val_acc: 0.0323\n",
            "Epoch 720/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0500 - acc: 0.3266 - val_loss: 3.5879 - val_acc: 0.0323\n",
            "Epoch 721/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0350 - acc: 0.3266 - val_loss: 3.5613 - val_acc: 0.0323\n",
            "Epoch 722/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1112 - acc: 0.2742 - val_loss: 3.5613 - val_acc: 0.0484\n",
            "Epoch 723/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0897 - acc: 0.2984 - val_loss: 3.5375 - val_acc: 0.0161\n",
            "Epoch 724/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0229 - acc: 0.3347 - val_loss: 3.6027 - val_acc: 0.0323\n",
            "Epoch 725/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1108 - acc: 0.2944 - val_loss: 3.5026 - val_acc: 0.0161\n",
            "Epoch 726/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0835 - acc: 0.3024 - val_loss: 3.5165 - val_acc: 0.0484\n",
            "Epoch 727/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1081 - acc: 0.2863 - val_loss: 3.5728 - val_acc: 0.0161\n",
            "Epoch 728/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0782 - acc: 0.3065 - val_loss: 3.5412 - val_acc: 0.0161\n",
            "Epoch 729/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.1103 - acc: 0.2903 - val_loss: 3.5388 - val_acc: 0.0161\n",
            "Epoch 730/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 1.0154 - acc: 0.3387 - val_loss: 3.5706 - val_acc: 0.0161\n",
            "Epoch 731/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0626 - acc: 0.3024 - val_loss: 3.5735 - val_acc: 0.0161\n",
            "Epoch 732/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0992 - acc: 0.2742 - val_loss: 3.5064 - val_acc: 0.0161\n",
            "Epoch 733/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0347 - acc: 0.3427 - val_loss: 3.5198 - val_acc: 0.0161\n",
            "Epoch 734/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.0822 - acc: 0.3065 - val_loss: 3.5843 - val_acc: 0.0161\n",
            "Epoch 735/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0658 - acc: 0.3105 - val_loss: 3.5516 - val_acc: 0.0161\n",
            "Epoch 736/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0219 - acc: 0.3427 - val_loss: 3.5536 - val_acc: 0.0161\n",
            "Epoch 737/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0296 - acc: 0.3468 - val_loss: 3.5452 - val_acc: 0.0161\n",
            "Epoch 738/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0300 - acc: 0.3306 - val_loss: 3.6710 - val_acc: 0.0000e+00\n",
            "Epoch 739/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0756 - acc: 0.3266 - val_loss: 3.5594 - val_acc: 0.0484\n",
            "Epoch 740/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0459 - acc: 0.3468 - val_loss: 3.5315 - val_acc: 0.0323\n",
            "Epoch 741/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0667 - acc: 0.3065 - val_loss: 3.6254 - val_acc: 0.0000e+00\n",
            "Epoch 742/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0782 - acc: 0.3266 - val_loss: 3.6164 - val_acc: 0.0000e+00\n",
            "Epoch 743/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.1071 - acc: 0.2823 - val_loss: 3.5919 - val_acc: 0.0000e+00\n",
            "Epoch 744/1000\n",
            "248/248 [==============================] - 0s 127us/step - loss: 1.0574 - acc: 0.3306 - val_loss: 3.6212 - val_acc: 0.0161\n",
            "Epoch 745/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0502 - acc: 0.2944 - val_loss: 3.5714 - val_acc: 0.0484\n",
            "Epoch 746/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0573 - acc: 0.3185 - val_loss: 3.4864 - val_acc: 0.0484\n",
            "Epoch 747/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0415 - acc: 0.3185 - val_loss: 3.5147 - val_acc: 0.0161\n",
            "Epoch 748/1000\n",
            "248/248 [==============================] - 0s 79us/step - loss: 1.0462 - acc: 0.3306 - val_loss: 3.5487 - val_acc: 0.0161\n",
            "Epoch 749/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0151 - acc: 0.3306 - val_loss: 3.5884 - val_acc: 0.0161\n",
            "Epoch 750/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0716 - acc: 0.3347 - val_loss: 3.5731 - val_acc: 0.0161\n",
            "Epoch 751/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0339 - acc: 0.3185 - val_loss: 3.5181 - val_acc: 0.0161\n",
            "Epoch 752/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0869 - acc: 0.2823 - val_loss: 3.5558 - val_acc: 0.0323\n",
            "Epoch 753/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0852 - acc: 0.3024 - val_loss: 3.5537 - val_acc: 0.0323\n",
            "Epoch 754/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.0470 - acc: 0.3105 - val_loss: 3.5720 - val_acc: 0.0323\n",
            "Epoch 755/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 0.9725 - acc: 0.3548 - val_loss: 3.5284 - val_acc: 0.0323\n",
            "Epoch 756/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0651 - acc: 0.3024 - val_loss: 3.5637 - val_acc: 0.0000e+00\n",
            "Epoch 757/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0532 - acc: 0.3347 - val_loss: 3.5091 - val_acc: 0.0323\n",
            "Epoch 758/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0690 - acc: 0.3145 - val_loss: 3.5268 - val_acc: 0.0323\n",
            "Epoch 759/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0863 - acc: 0.2823 - val_loss: 3.5617 - val_acc: 0.0000e+00\n",
            "Epoch 760/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0664 - acc: 0.3145 - val_loss: 3.6039 - val_acc: 0.0000e+00\n",
            "Epoch 761/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0664 - acc: 0.3427 - val_loss: 3.5349 - val_acc: 0.0161\n",
            "Epoch 762/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0339 - acc: 0.3024 - val_loss: 3.5455 - val_acc: 0.0161\n",
            "Epoch 763/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0519 - acc: 0.3105 - val_loss: 3.5858 - val_acc: 0.0000e+00\n",
            "Epoch 764/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.1655 - acc: 0.2500 - val_loss: 3.5027 - val_acc: 0.0161\n",
            "Epoch 765/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0458 - acc: 0.3226 - val_loss: 3.5757 - val_acc: 0.0000e+00\n",
            "Epoch 766/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0773 - acc: 0.3065 - val_loss: 3.5183 - val_acc: 0.0484\n",
            "Epoch 767/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.1439 - acc: 0.2782 - val_loss: 3.5744 - val_acc: 0.0484\n",
            "Epoch 768/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0070 - acc: 0.3347 - val_loss: 3.4836 - val_acc: 0.0484\n",
            "Epoch 769/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1074 - acc: 0.2984 - val_loss: 3.5910 - val_acc: 0.0484\n",
            "Epoch 770/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0078 - acc: 0.3589 - val_loss: 3.5693 - val_acc: 0.0161\n",
            "Epoch 771/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0765 - acc: 0.2944 - val_loss: 3.4731 - val_acc: 0.0484\n",
            "Epoch 772/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.1125 - acc: 0.2984 - val_loss: 3.5540 - val_acc: 0.0000e+00\n",
            "Epoch 773/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.2034 - acc: 0.2419 - val_loss: 3.5510 - val_acc: 0.0484\n",
            "Epoch 774/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0146 - acc: 0.3065 - val_loss: 3.5776 - val_acc: 0.0000e+00\n",
            "Epoch 775/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.0600 - acc: 0.2984 - val_loss: 3.6078 - val_acc: 0.0161\n",
            "Epoch 776/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0392 - acc: 0.3468 - val_loss: 3.6244 - val_acc: 0.0323\n",
            "Epoch 777/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1178 - acc: 0.2782 - val_loss: 3.5527 - val_acc: 0.0000e+00\n",
            "Epoch 778/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0169 - acc: 0.3347 - val_loss: 3.5681 - val_acc: 0.0161\n",
            "Epoch 779/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0388 - acc: 0.3226 - val_loss: 3.5917 - val_acc: 0.0161\n",
            "Epoch 780/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0663 - acc: 0.3065 - val_loss: 3.5566 - val_acc: 0.0161\n",
            "Epoch 781/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0429 - acc: 0.3185 - val_loss: 3.5559 - val_acc: 0.0161\n",
            "Epoch 782/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0700 - acc: 0.3185 - val_loss: 3.5004 - val_acc: 0.0323\n",
            "Epoch 783/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0073 - acc: 0.3629 - val_loss: 3.5028 - val_acc: 0.0323\n",
            "Epoch 784/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0816 - acc: 0.3306 - val_loss: 3.5723 - val_acc: 0.0000e+00\n",
            "Epoch 785/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0105 - acc: 0.3226 - val_loss: 3.5651 - val_acc: 0.0161\n",
            "Epoch 786/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.1167 - acc: 0.2419 - val_loss: 3.4817 - val_acc: 0.0645\n",
            "Epoch 787/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0393 - acc: 0.3185 - val_loss: 3.5204 - val_acc: 0.0323\n",
            "Epoch 788/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0315 - acc: 0.3024 - val_loss: 3.5448 - val_acc: 0.0000e+00\n",
            "Epoch 789/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0315 - acc: 0.3387 - val_loss: 3.5328 - val_acc: 0.0000e+00\n",
            "Epoch 790/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0748 - acc: 0.3065 - val_loss: 3.5444 - val_acc: 0.0161\n",
            "Epoch 791/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0851 - acc: 0.3145 - val_loss: 3.5091 - val_acc: 0.0161\n",
            "Epoch 792/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0579 - acc: 0.3024 - val_loss: 3.5001 - val_acc: 0.0484\n",
            "Epoch 793/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0208 - acc: 0.3347 - val_loss: 3.5556 - val_acc: 0.0161\n",
            "Epoch 794/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.1039 - acc: 0.2742 - val_loss: 3.4961 - val_acc: 0.0484\n",
            "Epoch 795/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0112 - acc: 0.3508 - val_loss: 3.4481 - val_acc: 0.0484\n",
            "Epoch 796/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0932 - acc: 0.3024 - val_loss: 3.5015 - val_acc: 0.0161\n",
            "Epoch 797/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0499 - acc: 0.3024 - val_loss: 3.5246 - val_acc: 0.0323\n",
            "Epoch 798/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0604 - acc: 0.2823 - val_loss: 3.5835 - val_acc: 0.0161\n",
            "Epoch 799/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.1067 - acc: 0.2540 - val_loss: 3.5315 - val_acc: 0.0161\n",
            "Epoch 800/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0134 - acc: 0.3387 - val_loss: 3.5367 - val_acc: 0.0323\n",
            "Epoch 801/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0236 - acc: 0.3024 - val_loss: 3.5794 - val_acc: 0.0161\n",
            "Epoch 802/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0469 - acc: 0.3185 - val_loss: 3.5065 - val_acc: 0.0484\n",
            "Epoch 803/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.1285 - acc: 0.2702 - val_loss: 3.5584 - val_acc: 0.0323\n",
            "Epoch 804/1000\n",
            "248/248 [==============================] - 0s 75us/step - loss: 1.0418 - acc: 0.3024 - val_loss: 3.4872 - val_acc: 0.0323\n",
            "Epoch 805/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0484 - acc: 0.2903 - val_loss: 3.5390 - val_acc: 0.0323\n",
            "Epoch 806/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 0.9934 - acc: 0.3306 - val_loss: 3.5795 - val_acc: 0.0323\n",
            "Epoch 807/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0121 - acc: 0.3266 - val_loss: 3.4344 - val_acc: 0.0645\n",
            "Epoch 808/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0307 - acc: 0.3508 - val_loss: 3.4910 - val_acc: 0.0484\n",
            "Epoch 809/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0196 - acc: 0.3387 - val_loss: 3.5609 - val_acc: 0.0000e+00\n",
            "Epoch 810/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 0.9960 - acc: 0.3145 - val_loss: 3.5683 - val_acc: 0.0161\n",
            "Epoch 811/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0358 - acc: 0.3185 - val_loss: 3.4291 - val_acc: 0.0484\n",
            "Epoch 812/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0905 - acc: 0.3024 - val_loss: 3.6580 - val_acc: 0.0000e+00\n",
            "Epoch 813/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0620 - acc: 0.2944 - val_loss: 3.5272 - val_acc: 0.0323\n",
            "Epoch 814/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9957 - acc: 0.3266 - val_loss: 3.5702 - val_acc: 0.0000e+00\n",
            "Epoch 815/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0379 - acc: 0.3226 - val_loss: 3.4981 - val_acc: 0.0323\n",
            "Epoch 816/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0542 - acc: 0.3024 - val_loss: 3.5776 - val_acc: 0.0000e+00\n",
            "Epoch 817/1000\n",
            "248/248 [==============================] - 0s 79us/step - loss: 1.0357 - acc: 0.3024 - val_loss: 3.5766 - val_acc: 0.0161\n",
            "Epoch 818/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0726 - acc: 0.2742 - val_loss: 3.5423 - val_acc: 0.0161\n",
            "Epoch 819/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0805 - acc: 0.3226 - val_loss: 3.5637 - val_acc: 0.0161\n",
            "Epoch 820/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0085 - acc: 0.3306 - val_loss: 3.5551 - val_acc: 0.0161\n",
            "Epoch 821/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0315 - acc: 0.3306 - val_loss: 3.5270 - val_acc: 0.0161\n",
            "Epoch 822/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0716 - acc: 0.3105 - val_loss: 3.5628 - val_acc: 0.0161\n",
            "Epoch 823/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0687 - acc: 0.2823 - val_loss: 3.5498 - val_acc: 0.0000e+00\n",
            "Epoch 824/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9893 - acc: 0.3629 - val_loss: 3.5009 - val_acc: 0.0000e+00\n",
            "Epoch 825/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0756 - acc: 0.3065 - val_loss: 3.6594 - val_acc: 0.0161\n",
            "Epoch 826/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0292 - acc: 0.3306 - val_loss: 3.4678 - val_acc: 0.0484\n",
            "Epoch 827/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0711 - acc: 0.2984 - val_loss: 3.5334 - val_acc: 0.0161\n",
            "Epoch 828/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0300 - acc: 0.3024 - val_loss: 3.5457 - val_acc: 0.0161\n",
            "Epoch 829/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0211 - acc: 0.3468 - val_loss: 3.5910 - val_acc: 0.0000e+00\n",
            "Epoch 830/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0915 - acc: 0.2661 - val_loss: 3.5146 - val_acc: 0.0323\n",
            "Epoch 831/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0256 - acc: 0.3266 - val_loss: 3.6031 - val_acc: 0.0000e+00\n",
            "Epoch 832/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0084 - acc: 0.3105 - val_loss: 3.5677 - val_acc: 0.0000e+00\n",
            "Epoch 833/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9827 - acc: 0.3226 - val_loss: 3.5564 - val_acc: 0.0161\n",
            "Epoch 834/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0512 - acc: 0.3145 - val_loss: 3.4538 - val_acc: 0.0484\n",
            "Epoch 835/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0186 - acc: 0.3145 - val_loss: 3.5035 - val_acc: 0.0000e+00\n",
            "Epoch 836/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 1.0338 - acc: 0.3065 - val_loss: 3.6305 - val_acc: 0.0161\n",
            "Epoch 837/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0347 - acc: 0.3145 - val_loss: 3.4743 - val_acc: 0.0484\n",
            "Epoch 838/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 1.0054 - acc: 0.3266 - val_loss: 3.5576 - val_acc: 0.0323\n",
            "Epoch 839/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0178 - acc: 0.3589 - val_loss: 3.5718 - val_acc: 0.0161\n",
            "Epoch 840/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0668 - acc: 0.3024 - val_loss: 3.6207 - val_acc: 0.0161\n",
            "Epoch 841/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0050 - acc: 0.3266 - val_loss: 3.5757 - val_acc: 0.0161\n",
            "Epoch 842/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0653 - acc: 0.3226 - val_loss: 3.6026 - val_acc: 0.0000e+00\n",
            "Epoch 843/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0152 - acc: 0.3508 - val_loss: 3.5869 - val_acc: 0.0161\n",
            "Epoch 844/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9928 - acc: 0.3629 - val_loss: 3.6087 - val_acc: 0.0484\n",
            "Epoch 845/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0634 - acc: 0.2984 - val_loss: 3.5633 - val_acc: 0.0161\n",
            "Epoch 846/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 0.9612 - acc: 0.3669 - val_loss: 3.7036 - val_acc: 0.0000e+00\n",
            "Epoch 847/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9948 - acc: 0.3629 - val_loss: 3.6395 - val_acc: 0.0161\n",
            "Epoch 848/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.0138 - acc: 0.3427 - val_loss: 3.5204 - val_acc: 0.0484\n",
            "Epoch 849/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0290 - acc: 0.3266 - val_loss: 3.6097 - val_acc: 0.0323\n",
            "Epoch 850/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.0211 - acc: 0.3226 - val_loss: 3.5633 - val_acc: 0.0484\n",
            "Epoch 851/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0474 - acc: 0.2661 - val_loss: 3.6354 - val_acc: 0.0161\n",
            "Epoch 852/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0335 - acc: 0.2661 - val_loss: 3.6637 - val_acc: 0.0000e+00\n",
            "Epoch 853/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.1072 - acc: 0.2419 - val_loss: 3.6324 - val_acc: 0.0000e+00\n",
            "Epoch 854/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 0.9824 - acc: 0.3226 - val_loss: 3.4590 - val_acc: 0.0161\n",
            "Epoch 855/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0049 - acc: 0.3387 - val_loss: 3.5377 - val_acc: 0.0000e+00\n",
            "Epoch 856/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0558 - acc: 0.2863 - val_loss: 3.4843 - val_acc: 0.0161\n",
            "Epoch 857/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9636 - acc: 0.3629 - val_loss: 3.6211 - val_acc: 0.0161\n",
            "Epoch 858/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0275 - acc: 0.2823 - val_loss: 3.5268 - val_acc: 0.0161\n",
            "Epoch 859/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0720 - acc: 0.3105 - val_loss: 3.4837 - val_acc: 0.0323\n",
            "Epoch 860/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0616 - acc: 0.3226 - val_loss: 3.5401 - val_acc: 0.0161\n",
            "Epoch 861/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9829 - acc: 0.3387 - val_loss: 3.5754 - val_acc: 0.0000e+00\n",
            "Epoch 862/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0037 - acc: 0.3347 - val_loss: 3.4958 - val_acc: 0.0484\n",
            "Epoch 863/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9926 - acc: 0.3347 - val_loss: 3.6412 - val_acc: 0.0161\n",
            "Epoch 864/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0654 - acc: 0.2782 - val_loss: 3.5604 - val_acc: 0.0000e+00\n",
            "Epoch 865/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1258 - acc: 0.2621 - val_loss: 3.4809 - val_acc: 0.0323\n",
            "Epoch 866/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9796 - acc: 0.3911 - val_loss: 3.4974 - val_acc: 0.0323\n",
            "Epoch 867/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.9684 - acc: 0.3669 - val_loss: 3.4975 - val_acc: 0.0161\n",
            "Epoch 868/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9964 - acc: 0.3710 - val_loss: 3.4896 - val_acc: 0.0323\n",
            "Epoch 869/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0325 - acc: 0.3347 - val_loss: 3.5928 - val_acc: 0.0323\n",
            "Epoch 870/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 0.9823 - acc: 0.3145 - val_loss: 3.5432 - val_acc: 0.0000e+00\n",
            "Epoch 871/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 0.9460 - acc: 0.3669 - val_loss: 3.5449 - val_acc: 0.0323\n",
            "Epoch 872/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9751 - acc: 0.3226 - val_loss: 3.5630 - val_acc: 0.0323\n",
            "Epoch 873/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9717 - acc: 0.3589 - val_loss: 3.5204 - val_acc: 0.0323\n",
            "Epoch 874/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0146 - acc: 0.3306 - val_loss: 3.5950 - val_acc: 0.0000e+00\n",
            "Epoch 875/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9500 - acc: 0.3589 - val_loss: 3.5028 - val_acc: 0.0161\n",
            "Epoch 876/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0388 - acc: 0.3266 - val_loss: 3.5650 - val_acc: 0.0161\n",
            "Epoch 877/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 0.9946 - acc: 0.3669 - val_loss: 3.4903 - val_acc: 0.0484\n",
            "Epoch 878/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 0.9858 - acc: 0.3427 - val_loss: 3.4760 - val_acc: 0.0484\n",
            "Epoch 879/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 1.0484 - acc: 0.2863 - val_loss: 3.5341 - val_acc: 0.0323\n",
            "Epoch 880/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9694 - acc: 0.3548 - val_loss: 3.5223 - val_acc: 0.0000e+00\n",
            "Epoch 881/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0318 - acc: 0.3105 - val_loss: 3.5934 - val_acc: 0.0323\n",
            "Epoch 882/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0380 - acc: 0.2984 - val_loss: 3.6100 - val_acc: 0.0161\n",
            "Epoch 883/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0634 - acc: 0.2984 - val_loss: 3.6650 - val_acc: 0.0000e+00\n",
            "Epoch 884/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9988 - acc: 0.3347 - val_loss: 3.6757 - val_acc: 0.0000e+00\n",
            "Epoch 885/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0045 - acc: 0.3145 - val_loss: 3.5639 - val_acc: 0.0323\n",
            "Epoch 886/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 0.9938 - acc: 0.3105 - val_loss: 3.5931 - val_acc: 0.0000e+00\n",
            "Epoch 887/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9784 - acc: 0.3226 - val_loss: 3.6063 - val_acc: 0.0000e+00\n",
            "Epoch 888/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.0211 - acc: 0.3105 - val_loss: 3.6181 - val_acc: 0.0323\n",
            "Epoch 889/1000\n",
            "248/248 [==============================] - 0s 63us/step - loss: 1.0108 - acc: 0.3185 - val_loss: 3.6012 - val_acc: 0.0323\n",
            "Epoch 890/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9644 - acc: 0.3347 - val_loss: 3.6333 - val_acc: 0.0000e+00\n",
            "Epoch 891/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9538 - acc: 0.3750 - val_loss: 3.5357 - val_acc: 0.0161\n",
            "Epoch 892/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.9923 - acc: 0.3226 - val_loss: 3.6339 - val_acc: 0.0000e+00\n",
            "Epoch 893/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9422 - acc: 0.3710 - val_loss: 3.5446 - val_acc: 0.0161\n",
            "Epoch 894/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 1.0038 - acc: 0.3347 - val_loss: 3.6049 - val_acc: 0.0323\n",
            "Epoch 895/1000\n",
            "248/248 [==============================] - 0s 64us/step - loss: 1.0171 - acc: 0.2944 - val_loss: 3.4919 - val_acc: 0.0323\n",
            "Epoch 896/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0159 - acc: 0.3185 - val_loss: 3.4836 - val_acc: 0.0323\n",
            "Epoch 897/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 1.0024 - acc: 0.3226 - val_loss: 3.5006 - val_acc: 0.0161\n",
            "Epoch 898/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9656 - acc: 0.3669 - val_loss: 3.6723 - val_acc: 0.0000e+00\n",
            "Epoch 899/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0022 - acc: 0.3185 - val_loss: 3.6486 - val_acc: 0.0000e+00\n",
            "Epoch 900/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9867 - acc: 0.3508 - val_loss: 3.5748 - val_acc: 0.0323\n",
            "Epoch 901/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0643 - acc: 0.2621 - val_loss: 3.7126 - val_acc: 0.0000e+00\n",
            "Epoch 902/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 0.9741 - acc: 0.3548 - val_loss: 3.7145 - val_acc: 0.0000e+00\n",
            "Epoch 903/1000\n",
            "248/248 [==============================] - 0s 66us/step - loss: 1.0224 - acc: 0.3347 - val_loss: 3.5544 - val_acc: 0.0323\n",
            "Epoch 904/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9442 - acc: 0.3548 - val_loss: 3.6056 - val_acc: 0.0323\n",
            "Epoch 905/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9982 - acc: 0.3427 - val_loss: 3.5208 - val_acc: 0.0323\n",
            "Epoch 906/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 0.9779 - acc: 0.3266 - val_loss: 3.5339 - val_acc: 0.0161\n",
            "Epoch 907/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 0.9819 - acc: 0.3306 - val_loss: 3.5890 - val_acc: 0.0000e+00\n",
            "Epoch 908/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9752 - acc: 0.3387 - val_loss: 3.6111 - val_acc: 0.0000e+00\n",
            "Epoch 909/1000\n",
            "248/248 [==============================] - 0s 77us/step - loss: 1.0503 - acc: 0.3427 - val_loss: 3.6882 - val_acc: 0.0161\n",
            "Epoch 910/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9820 - acc: 0.3427 - val_loss: 3.5403 - val_acc: 0.0323\n",
            "Epoch 911/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9839 - acc: 0.3347 - val_loss: 3.5994 - val_acc: 0.0323\n",
            "Epoch 912/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 1.0064 - acc: 0.3105 - val_loss: 3.6316 - val_acc: 0.0161\n",
            "Epoch 913/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9882 - acc: 0.3306 - val_loss: 3.5615 - val_acc: 0.0323\n",
            "Epoch 914/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9790 - acc: 0.3548 - val_loss: 3.6938 - val_acc: 0.0000e+00\n",
            "Epoch 915/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0355 - acc: 0.3105 - val_loss: 3.5570 - val_acc: 0.0161\n",
            "Epoch 916/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0334 - acc: 0.3105 - val_loss: 3.5875 - val_acc: 0.0000e+00\n",
            "Epoch 917/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0383 - acc: 0.3105 - val_loss: 3.5181 - val_acc: 0.0161\n",
            "Epoch 918/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9659 - acc: 0.3508 - val_loss: 3.5179 - val_acc: 0.0323\n",
            "Epoch 919/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9843 - acc: 0.3629 - val_loss: 3.5014 - val_acc: 0.0323\n",
            "Epoch 920/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0504 - acc: 0.2944 - val_loss: 3.5729 - val_acc: 0.0161\n",
            "Epoch 921/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9524 - acc: 0.3468 - val_loss: 3.5671 - val_acc: 0.0161\n",
            "Epoch 922/1000\n",
            "248/248 [==============================] - 0s 73us/step - loss: 0.9796 - acc: 0.3306 - val_loss: 3.5310 - val_acc: 0.0161\n",
            "Epoch 923/1000\n",
            "248/248 [==============================] - 0s 68us/step - loss: 1.0857 - acc: 0.2702 - val_loss: 3.5106 - val_acc: 0.0323\n",
            "Epoch 924/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 1.0198 - acc: 0.3589 - val_loss: 3.5053 - val_acc: 0.0161\n",
            "Epoch 925/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9968 - acc: 0.3266 - val_loss: 3.6504 - val_acc: 0.0161\n",
            "Epoch 926/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0008 - acc: 0.3387 - val_loss: 3.5092 - val_acc: 0.0161\n",
            "Epoch 927/1000\n",
            "248/248 [==============================] - 0s 70us/step - loss: 0.9860 - acc: 0.3589 - val_loss: 3.5704 - val_acc: 0.0161\n",
            "Epoch 928/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0400 - acc: 0.2984 - val_loss: 3.5172 - val_acc: 0.0161\n",
            "Epoch 929/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9781 - acc: 0.3024 - val_loss: 3.5096 - val_acc: 0.0323\n",
            "Epoch 930/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 0.9570 - acc: 0.3468 - val_loss: 3.6117 - val_acc: 0.0323\n",
            "Epoch 931/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0102 - acc: 0.3306 - val_loss: 3.4966 - val_acc: 0.0161\n",
            "Epoch 932/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9654 - acc: 0.3427 - val_loss: 3.5063 - val_acc: 0.0323\n",
            "Epoch 933/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9800 - acc: 0.3065 - val_loss: 3.6519 - val_acc: 0.0000e+00\n",
            "Epoch 934/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 0.9990 - acc: 0.3024 - val_loss: 3.5233 - val_acc: 0.0000e+00\n",
            "Epoch 935/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9670 - acc: 0.3589 - val_loss: 3.4864 - val_acc: 0.0161\n",
            "Epoch 936/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.9860 - acc: 0.3589 - val_loss: 3.5737 - val_acc: 0.0000e+00\n",
            "Epoch 937/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9970 - acc: 0.3266 - val_loss: 3.4673 - val_acc: 0.0484\n",
            "Epoch 938/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0096 - acc: 0.3024 - val_loss: 3.5813 - val_acc: 0.0000e+00\n",
            "Epoch 939/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9547 - acc: 0.3669 - val_loss: 3.4827 - val_acc: 0.0161\n",
            "Epoch 940/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9494 - acc: 0.3468 - val_loss: 3.5953 - val_acc: 0.0161\n",
            "Epoch 941/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.1053 - acc: 0.2782 - val_loss: 3.5073 - val_acc: 0.0000e+00\n",
            "Epoch 942/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 1.0085 - acc: 0.3185 - val_loss: 3.6165 - val_acc: 0.0161\n",
            "Epoch 943/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0157 - acc: 0.3185 - val_loss: 3.5737 - val_acc: 0.0161\n",
            "Epoch 944/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9948 - acc: 0.3185 - val_loss: 3.5764 - val_acc: 0.0161\n",
            "Epoch 945/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9858 - acc: 0.3387 - val_loss: 3.5610 - val_acc: 0.0161\n",
            "Epoch 946/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9833 - acc: 0.3387 - val_loss: 3.5761 - val_acc: 0.0000e+00\n",
            "Epoch 947/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9815 - acc: 0.3387 - val_loss: 3.6031 - val_acc: 0.0000e+00\n",
            "Epoch 948/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 1.0222 - acc: 0.3105 - val_loss: 3.5983 - val_acc: 0.0161\n",
            "Epoch 949/1000\n",
            "248/248 [==============================] - 0s 61us/step - loss: 0.9461 - acc: 0.3589 - val_loss: 3.5286 - val_acc: 0.0161\n",
            "Epoch 950/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9583 - acc: 0.3468 - val_loss: 3.5329 - val_acc: 0.0323\n",
            "Epoch 951/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 1.0377 - acc: 0.3145 - val_loss: 3.5863 - val_acc: 0.0161\n",
            "Epoch 952/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 0.9502 - acc: 0.3669 - val_loss: 3.5369 - val_acc: 0.0161\n",
            "Epoch 953/1000\n",
            "248/248 [==============================] - 0s 62us/step - loss: 0.9260 - acc: 0.3669 - val_loss: 3.5067 - val_acc: 0.0161\n",
            "Epoch 954/1000\n",
            "248/248 [==============================] - 0s 76us/step - loss: 0.9524 - acc: 0.3508 - val_loss: 3.5919 - val_acc: 0.0000e+00\n",
            "Epoch 955/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9584 - acc: 0.3629 - val_loss: 3.5210 - val_acc: 0.0323\n",
            "Epoch 956/1000\n",
            "248/248 [==============================] - 0s 60us/step - loss: 0.9892 - acc: 0.3065 - val_loss: 3.5173 - val_acc: 0.0161\n",
            "Epoch 957/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9338 - acc: 0.3669 - val_loss: 3.5850 - val_acc: 0.0000e+00\n",
            "Epoch 958/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 1.0184 - acc: 0.3185 - val_loss: 3.5891 - val_acc: 0.0161\n",
            "Epoch 959/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9881 - acc: 0.3387 - val_loss: 3.5664 - val_acc: 0.0323\n",
            "Epoch 960/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 1.0401 - acc: 0.2984 - val_loss: 3.5230 - val_acc: 0.0000e+00\n",
            "Epoch 961/1000\n",
            "248/248 [==============================] - 0s 65us/step - loss: 1.0111 - acc: 0.3347 - val_loss: 3.5774 - val_acc: 0.0161\n",
            "Epoch 962/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.9428 - acc: 0.3831 - val_loss: 3.5812 - val_acc: 0.0161\n",
            "Epoch 963/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9715 - acc: 0.3185 - val_loss: 3.6313 - val_acc: 0.0000e+00\n",
            "Epoch 964/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.0056 - acc: 0.3347 - val_loss: 3.4854 - val_acc: 0.0323\n",
            "Epoch 965/1000\n",
            "248/248 [==============================] - 0s 72us/step - loss: 1.0296 - acc: 0.3226 - val_loss: 3.5558 - val_acc: 0.0161\n",
            "Epoch 966/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9816 - acc: 0.3266 - val_loss: 3.5608 - val_acc: 0.0000e+00\n",
            "Epoch 967/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0148 - acc: 0.3387 - val_loss: 3.5352 - val_acc: 0.0323\n",
            "Epoch 968/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9814 - acc: 0.3508 - val_loss: 3.5477 - val_acc: 0.0323\n",
            "Epoch 969/1000\n",
            "248/248 [==============================] - 0s 49us/step - loss: 1.0278 - acc: 0.3024 - val_loss: 3.6325 - val_acc: 0.0161\n",
            "Epoch 970/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 0.9591 - acc: 0.3468 - val_loss: 3.5103 - val_acc: 0.0161\n",
            "Epoch 971/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9662 - acc: 0.3266 - val_loss: 3.6664 - val_acc: 0.0161\n",
            "Epoch 972/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 0.9497 - acc: 0.3669 - val_loss: 3.5984 - val_acc: 0.0323\n",
            "Epoch 973/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9699 - acc: 0.3427 - val_loss: 3.5291 - val_acc: 0.0323\n",
            "Epoch 974/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9938 - acc: 0.3185 - val_loss: 3.5744 - val_acc: 0.0323\n",
            "Epoch 975/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9778 - acc: 0.3306 - val_loss: 3.7035 - val_acc: 0.0000e+00\n",
            "Epoch 976/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 1.0385 - acc: 0.2702 - val_loss: 3.5758 - val_acc: 0.0161\n",
            "Epoch 977/1000\n",
            "248/248 [==============================] - 0s 71us/step - loss: 0.9712 - acc: 0.3306 - val_loss: 3.4987 - val_acc: 0.0161\n",
            "Epoch 978/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 1.0352 - acc: 0.3226 - val_loss: 3.4894 - val_acc: 0.0323\n",
            "Epoch 979/1000\n",
            "248/248 [==============================] - 0s 67us/step - loss: 1.0223 - acc: 0.3024 - val_loss: 3.6735 - val_acc: 0.0000e+00\n",
            "Epoch 980/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 1.0422 - acc: 0.2944 - val_loss: 3.6027 - val_acc: 0.0161\n",
            "Epoch 981/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9435 - acc: 0.3790 - val_loss: 3.6054 - val_acc: 0.0161\n",
            "Epoch 982/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9665 - acc: 0.3266 - val_loss: 3.5502 - val_acc: 0.0161\n",
            "Epoch 983/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9222 - acc: 0.3589 - val_loss: 3.4807 - val_acc: 0.0323\n",
            "Epoch 984/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9256 - acc: 0.3790 - val_loss: 3.6354 - val_acc: 0.0000e+00\n",
            "Epoch 985/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9048 - acc: 0.3790 - val_loss: 3.4770 - val_acc: 0.0161\n",
            "Epoch 986/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 0.9755 - acc: 0.3387 - val_loss: 3.5242 - val_acc: 0.0484\n",
            "Epoch 987/1000\n",
            "248/248 [==============================] - 0s 46us/step - loss: 1.0403 - acc: 0.3145 - val_loss: 3.6007 - val_acc: 0.0161\n",
            "Epoch 988/1000\n",
            "248/248 [==============================] - 0s 52us/step - loss: 0.9422 - acc: 0.3750 - val_loss: 3.5205 - val_acc: 0.0161\n",
            "Epoch 989/1000\n",
            "248/248 [==============================] - 0s 59us/step - loss: 0.9371 - acc: 0.3790 - val_loss: 3.5488 - val_acc: 0.0161\n",
            "Epoch 990/1000\n",
            "248/248 [==============================] - 0s 58us/step - loss: 0.9856 - acc: 0.3427 - val_loss: 3.7589 - val_acc: 0.0161\n",
            "Epoch 991/1000\n",
            "248/248 [==============================] - 0s 54us/step - loss: 0.9944 - acc: 0.3387 - val_loss: 3.5538 - val_acc: 0.0484\n",
            "Epoch 992/1000\n",
            "248/248 [==============================] - 0s 55us/step - loss: 0.9841 - acc: 0.3508 - val_loss: 3.4941 - val_acc: 0.0161\n",
            "Epoch 993/1000\n",
            "248/248 [==============================] - 0s 53us/step - loss: 0.9577 - acc: 0.3710 - val_loss: 3.5010 - val_acc: 0.0323\n",
            "Epoch 994/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9683 - acc: 0.3266 - val_loss: 3.5240 - val_acc: 0.0161\n",
            "Epoch 995/1000\n",
            "248/248 [==============================] - 0s 57us/step - loss: 0.9673 - acc: 0.3427 - val_loss: 3.4402 - val_acc: 0.0161\n",
            "Epoch 996/1000\n",
            "248/248 [==============================] - 0s 56us/step - loss: 0.9768 - acc: 0.3387 - val_loss: 3.5857 - val_acc: 0.0000e+00\n",
            "Epoch 997/1000\n",
            "248/248 [==============================] - 0s 48us/step - loss: 0.9716 - acc: 0.3589 - val_loss: 3.6837 - val_acc: 0.0000e+00\n",
            "Epoch 998/1000\n",
            "248/248 [==============================] - 0s 47us/step - loss: 0.9991 - acc: 0.3508 - val_loss: 3.6198 - val_acc: 0.0000e+00\n",
            "Epoch 999/1000\n",
            "248/248 [==============================] - 0s 51us/step - loss: 1.0340 - acc: 0.3065 - val_loss: 3.5507 - val_acc: 0.0000e+00\n",
            "Epoch 1000/1000\n",
            "248/248 [==============================] - 0s 50us/step - loss: 0.9403 - acc: 0.3508 - val_loss: 3.8030 - val_acc: 0.0323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypqYwJ1Q-Fuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "72d67cf1-f591-4d4f-eb5c-2e03213d1969"
      },
      "source": [
        "model.evaluate(testX, testY)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82/82 [==============================] - 0s 96us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.45501311232404, 0.08536585365853659]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbiFB2bl-Mgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "53dace6c-13c4-4649-c989-8c69d42e9726"
      },
      "source": [
        "show_graph(history)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDViWlPEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}